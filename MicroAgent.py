#!/usr/bin/env python
"""
å¢å¼ºç‰ˆåŠ¨ä½œé¢„æµ‹å™¨ - åŸºäºQwen3-VL-2B-Instructæ¨¡å‹
æ–°å¢é€šç”¨èŠå¤©åŠŸèƒ½ï¼Œæ”¯æŒçº¯æ–‡æœ¬æˆ–å›¾æ–‡å¯¹è¯
"""

import torch
import os
import json
import re
from PIL import Image
from transformers import Qwen3VLForConditionalGeneration, Qwen3VLProcessor
from peft import PeftModel


class ActionPredictor:
    def __init__(self, model_path=None, adapter_path=None, device=None):
        """
        åˆå§‹åŒ–åŠ¨ä½œé¢„æµ‹å™¨
        :param model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        :param adapter_path: LoRAé€‚é…å™¨è·¯å¾„
        :param device: è®¾å¤‡è®¾ç½® ('cuda:0', 'cpu'ç­‰)
        """
        if model_path is None:
            # é»˜è®¤ä½¿ç”¨å…¨é‡å¾®è°ƒæ¨¡å‹çš„checkpointç›®å½•
            self.model_path = '/mnt/workspace/qwen3-vl-2b-instruct-lora_llm/checkpoint-30'
        else:
            self.model_path = model_path

        self.adapter_path = adapter_path

        if device is None:
            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)

        print(f"[INFO] ä½¿ç”¨è®¾å¤‡: {self.device}")
        self._load_model()

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        print("[INFO] æ­£åœ¨åŠ è½½Qwen3-VL-2B-Instructæ¨¡å‹...")

        # æ£€æµ‹æ¨¡å‹ç±»å‹ï¼šå…¨é‡å¾®è°ƒæ¨¡å‹ æˆ– LoRAé€‚é…å™¨
        is_lora = (os.path.exists(os.path.join(self.model_path, "adapter_model.bin")) or
                   os.path.exists(os.path.join(self.model_path, "adapter_model.safetensors"))) and \
                  os.path.exists(os.path.join(self.model_path, "adapter_config.json"))

        is_full = (os.path.exists(os.path.join(self.model_path, "pytorch_model.bin")) or
                   os.path.exists(os.path.join(self.model_path, "model.safetensors"))) and \
                  os.path.exists(os.path.join(self.model_path, "config.json"))

        # ç¡®å®šå¤„ç†å™¨è·¯å¾„
        processor_path = self.model_path

        if is_lora or self.adapter_path:
            # LoRAæ¨¡å¼ï¼šéœ€æŒ‡å®šåŸºç¡€æ¨¡å‹
            if self.adapter_path:
                # å¦‚æœæä¾›äº†adapter_pathï¼Œä½¿ç”¨å®ƒä½œä¸ºLoRAè·¯å¾„
                adapter_dir = self.adapter_path
            else:
                # å¦åˆ™ä½¿ç”¨model_pathä½œä¸ºLoRAè·¯å¾„
                adapter_dir = self.model_path

            # ä»adapter_config.jsonä¸­è·å–åŸºç¡€æ¨¡å‹åç§°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            adapter_config_path = os.path.join(adapter_dir, "adapter_config.json")
            if os.path.exists(adapter_config_path):
                with open(adapter_config_path, 'r', encoding='utf-8') as f:
                    adapter_config = json.load(f)
                base_model = adapter_config.get("base_model_name_or_path", "/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct")
            else:
                base_model = "/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct"

            print(f"â†’ æ£€æµ‹åˆ°LoRAæ¨¡å‹ï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")

            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                base_model,
                torch_dtype=torch.bfloat16,
                device_map=self.device,
                trust_remote_code=True
            )

            # åŠ è½½é€‚é…å™¨
            self.model = PeftModel.from_pretrained(self.model, adapter_dir, is_trainable=False)
            print(f"âœ“ LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ: {adapter_dir}")

            # å¦‚æœæ˜¯LoRAä¸”ç›®æ ‡è·¯å¾„ç¼ºå°‘tokenizeré…ç½®ï¼Œä»åŸºç¡€æ¨¡å‹åŠ è½½å¤„ç†å™¨
            if not os.path.exists(os.path.join(self.model_path, "tokenizer_config.json")):
                processor_path = base_model

        elif is_full:
            # å…¨é‡æ¨¡å‹ï¼šç›´æ¥åŠ è½½
            print(f"â†’ åŠ è½½å…¨é‡å¾®è°ƒæ¨¡å‹: {self.model_path}")
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                device_map=self.device,
                trust_remote_code=True
            )
        else:
            # å¦‚æœéƒ½ä¸æ˜¯ï¼Œå°è¯•ä½œä¸ºåŸºç¡€æ¨¡å‹åŠ è½½
            print(f"â†’ å°è¯•ä½œä¸ºåŸºç¡€æ¨¡å‹åŠ è½½: {self.model_path}")
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                device_map=self.device,
                trust_remote_code=True
            )

        # åŠ è½½å¤„ç†å™¨
        self.processor = Qwen3VLProcessor.from_pretrained(
            processor_path,
            trust_remote_code=True
        )

        self.model.eval()
        print("[INFO] æ¨¡å‹åŠ è½½å®Œæˆ âœ“")
    
    def _process_image(self, img):
        """ç»Ÿä¸€å¤„ç†å„ç§æ ¼å¼çš„å›¾ç‰‡è¾“å…¥"""
        if isinstance(img, Image.Image):
            return img.convert('RGB')
        
        if isinstance(img, str):
            if img.startswith('data:image'):
                import base64
                from io import BytesIO
                base64_str = img.split(',', 1)[1] if ',' in img else img
                image_bytes = base64.b64decode(base64_str)
                return Image.open(BytesIO(image_bytes)).convert('RGB')
            elif os.path.exists(img):
                return Image.open(img).convert('RGB')
            else:
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img}")
        
        raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {type(img)}")
    
    def get_action(self, image1, image2):
        """
        ä»ä¸¤å¼ è¿ç»­å›¾ç‰‡é¢„æµ‹åŠ¨ä½œ
        :param image1: ç¬¬ä¸€å¼ å›¾ç‰‡ (è·¯å¾„/PIL/base64)
        :param image2: ç¬¬äºŒå¼ å›¾ç‰‡ (è·¯å¾„/PIL/base64)
        :return: (direction_bool, distance_int) 
                 direction_bool: True="+", False="-"
                 distance_int: ç§»åŠ¨è·ç¦»(æ•´æ•°)
        """
        # å¤„ç†å›¾ç‰‡
        pil_img1 = self._process_image(image1)
        pil_img2 = self._process_image(image2)
        
        # æ„å»ºä¸“ä¸šæç¤ºè¯
        prompt = """è¯·åˆ†æä¸¤å¼ è¿ç»­æ‹æ‘„çš„æ˜¾å¾®é•œå›¾åƒï¼Œåˆ¤æ–­å½“å‰èšç„¦çŠ¶æ€çš„å˜åŒ–è¶‹åŠ¿ï¼Œå¹¶æ®æ­¤æ¨æ–­ç”µæœºåº”å‘å“ªä¸ªæ–¹å‘ç§»åŠ¨å¤šå°‘æ­¥ä»¥æ¥è¿‘æœ€ä½³èšç„¦ä½ç½®ã€‚ - å¦‚æœç¬¬äºŒå¼ å›¾åƒæ¯”ç¬¬ä¸€å¼ æ›´æ¨¡ç³Šï¼Œè¯´æ˜ç„¦ç‚¹æ­£åœ¨è¿œç¦»æœ€ä½³ä½ç½®ï¼Œç”µæœºåº”å‘è´Ÿæ–¹å‘ï¼ˆ"-"ï¼‰ç§»åŠ¨ï¼› - å¦‚æœç¬¬äºŒå¼ å›¾åƒæ¯”ç¬¬ä¸€å¼ æ›´æ¸…æ™°ï¼Œè¯´æ˜ç„¦ç‚¹æ­£åœ¨æ¥è¿‘æœ€ä½³ä½ç½®ï¼Œç”µæœºåº”ç»§ç»­å‘æ­£æ–¹å‘ï¼ˆ"+"ï¼‰ç§»åŠ¨ã€‚ è¯·åŸºäºå›¾åƒæ¸…æ™°åº¦å˜åŒ–ï¼Œä¼°è®¡ç”µæœºéœ€ç§»åŠ¨çš„æ­¥æ•°ï¼ˆå–æ•´æ•°ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–è§£é‡Šï¼š {"analysis": "ç”µæœºåº”è¯¥å‘{x}æ–¹å‘ç§»åŠ¨{y}æ­¥ã€‚", "direction": "{x}", "distance": {y}} æ³¨æ„ï¼š - "direction" åªèƒ½æ˜¯ "+" æˆ– "-"ï¼› - "distance" å¿…é¡»æ˜¯éè´Ÿæ•´æ•°ï¼ˆå¦‚ 0, 1, 2, ...ï¼‰ï¼› - "analysis" ä¸­çš„æ–¹å‘å’Œæ­¥æ•°å¿…é¡»ä¸ direction å’Œ distance å­—æ®µä¸€è‡´ï¼› - è¾“å‡ºå¿…é¡»æ˜¯çº¯ JSONï¼Œæ—  Markdownã€æ— æ³¨é‡Šã€æ— å¤šä½™ç©ºæ ¼æˆ–æ¢è¡Œã€‚"""
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{
            "role": "user",
            "content": [
                {"type": "image"}, 
                {"type": "image"},
                {"type": "text", "text": prompt}
            ]
        }]
        
        return self._extract_action(messages, [pil_img1, pil_img2])
    
    def chat(self, prompt, images=None, max_new_tokens=512, temperature=0.3):
        """
        é€šç”¨å›¾æ–‡å¯¹è¯åŠŸèƒ½
        :param prompt: ç”¨æˆ·æ–‡æœ¬æç¤º
        :param images: å¯é€‰ï¼Œå›¾ç‰‡åˆ—è¡¨ï¼ˆæ”¯æŒ0ï½Nå¼ ï¼Œæ ¼å¼åŒget_actionï¼‰
        :param max_new_tokens: ç”Ÿæˆæœ€å¤§tokenæ•°
        :param temperature: é‡‡æ ·æ¸©åº¦
        :return: æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬å›å¤ï¼ˆå­—ç¬¦ä¸²ï¼‰
        """
        # å¤„ç†å›¾ç‰‡
        pil_images = []
        content = []

        if images:
            if not isinstance(images, list):
                images = [images]

            for img in images:
                # ä½¿ç”¨æ›´å¥å£®çš„å›¾ç‰‡å¤„ç†æ–¹å¼
                pil_img = self._process_image(img)
                pil_images.append(pil_img)
                content.append({"type": "image"})

        # æ·»åŠ æ–‡æœ¬æç¤º
        content.append({"type": "text", "text": prompt})

        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "user", "content": content}]

        # å¤„ç†è¾“å…¥
        text = self.processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # ä½¿ç”¨æ›´å¥å£®çš„è¾“å…¥å¤„ç†æ–¹å¼
        if pil_images:
            inputs = self.processor(
                text=[text],
                images=pil_images,
                padding=True,
                return_tensors="pt"
            )
        else:
            inputs = self.processor(
                text=[text],
                padding=True,
                return_tensors="pt"
            )

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                pad_token_id=self.processor.tokenizer.eos_token_id
            )

        # æå–ç”Ÿæˆå†…å®¹ï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
        generated_ids_trimmed = generated_ids[:, inputs['input_ids'].shape[1]:]
        response = self.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0].strip()

        return response
    
    def _extract_action(self, messages, pil_images):
        """å†…éƒ¨æ–¹æ³•ï¼šä»æ¨¡å‹è¾“å‡ºæå–åŠ¨ä½œä¿¡æ¯"""
        text = self.processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # ä½¿ç”¨æ›´å¥å£®çš„è¾“å…¥å¤„ç†æ–¹å¼
        inputs = self.processor(
            text=[text],
            images=pil_images,
            padding=True,
            return_tensors="pt"
        )

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=128,
                temperature=0.1,
                pad_token_id=self.processor.tokenizer.eos_token_id
            )

        generated_ids_trimmed = generated_ids[:, inputs['input_ids'].shape[1]:]
        output = self.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True
        )[0].strip()

        print(f"[DEBUG] æ¨¡å‹åŸå§‹è¾“å‡º: {output}")

        # å°è¯•æå–JSON
        try:
            # å¤šç§JSONæå–ç­–ç•¥
            json_str = None
            patterns = [
                r'\{[^{}]*"direction"[^{}]*"distance"[^{}]*\}',
                r'\{.*?\}',
                output.strip()
            ]

            for pattern in patterns:
                if pattern.startswith('{'):
                    json_str = pattern
                    break
                match = re.search(pattern, output, re.DOTALL)
                if match:
                    json_str = match.group()
                    break

            if json_str:
                result = json.loads(json_str)
                direction = str(result.get("direction", "-")).strip()
                distance = result.get("distance", 0)

                # è½¬æ¢æ ¼å¼
                direction_bool = direction == "+"
                distance_int = int(distance) if isinstance(distance, (int, float)) else 0

                print(f"[SUCCESS] è§£æç»“æœ: direction={direction_bool} ({direction}), distance={distance_int}")
                return direction_bool, distance_int

        except (json.JSONDecodeError, ValueError, TypeError) as e:
            print(f"[WARNING] JSONè§£æå¤±è´¥: {e}")

        # å¤±è´¥å›é€€ç­–ç•¥ï¼šæ­£åˆ™æå–
        print("[INFO] å°è¯•æ­£åˆ™å›é€€è§£æ...")
        dir_match = re.search(r'"direction"\s*:\s*"([^"]+)"', output)
        dist_match = re.search(r'"distance"\s*:\s*(\d+)', output)

        if dir_match and dist_match:
            direction_bool = dir_match.group(1).strip() == "+"
            distance_int = int(dist_match.group(1))
            print(f"[SUCCESS] æ­£åˆ™è§£ææˆåŠŸ: direction={direction_bool}, distance={distance_int}")
            return direction_bool, distance_int

        print("[ERROR] æ— æ³•è§£æåŠ¨ä½œä¿¡æ¯ï¼Œè¿”å›é»˜è®¤å€¼")
        return False, 0
    
    def close(self):
        """é‡Šæ”¾èµ„æº"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'processor'):
            del self.processor
        torch.cuda.empty_cache()
        print("[INFO] èµ„æºå·²é‡Šæ”¾")


def main():
    """å®Œæ•´åŠŸèƒ½æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸš€ å¢å¼ºç‰ˆåŠ¨ä½œé¢„æµ‹å™¨ - åŠŸèƒ½æµ‹è¯•")
    print("="*60)

    try:
        # åˆå§‹åŒ–é¢„æµ‹å™¨
        predictor = ActionPredictor()

        # # ========== æµ‹è¯•1: é€šç”¨èŠå¤©åŠŸèƒ½ï¼ˆçº¯æ–‡æœ¬ï¼‰==========
        # print("\n" + "-"*60)
        # print("ğŸ“Œ æµ‹è¯•1: é€šç”¨èŠå¤©åŠŸèƒ½ï¼ˆçº¯æ–‡æœ¬æé—®ï¼‰")
        # print("-"*60)
        # text_prompt = "æ˜¾å¾®é•œä½¿ç”¨æ“ä½œæµç¨‹ä¸æ³¨æ„äº‹é¡¹"
        # print(f"\nğŸ‘¤ ç”¨æˆ·æé—®: {text_prompt}")
        # print("\nğŸ¤– æ¨¡å‹å›å¤:")
        # try:
        #     response = predictor.chat(text_prompt, max_new_tokens=400, temperature=0.5)
        #     print(response)
        # except Exception as e:
        #     print(f"[ERROR] èŠå¤©åŠŸèƒ½å‡ºé”™: {e}")

#         # # ========== æµ‹è¯•2: é€šç”¨èŠå¤©åŠŸèƒ½ï¼ˆå¸¦å›¾ç‰‡ï¼‰==========
#         print("\n" + "-"*60)
#         print("ğŸ“Œ æµ‹è¯•2: é€šç”¨èŠå¤©åŠŸèƒ½ï¼ˆå¸¦å›¾ç‰‡æè¿°ï¼‰")
#         print("-"*60)
#         img_prompt = "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­å†™çš„æœ‰ä»€ä¹ˆå­—"
#         # æä¾›å…·ä½“çš„å›¾ç‰‡è·¯å¾„
#         test_img_path = "./test.png"    # æ›¿æ¢ä¸ºå®é™…çš„å›¾ç‰‡è·¯å¾„
#         print(f"\nğŸ–¼ï¸  ä½¿ç”¨çš„å›¾ç‰‡è·¯å¾„: {test_img_path}")
#         print(f"ğŸ‘¤ ç”¨æˆ·æé—®: {img_prompt}")
#         print("\nğŸ¤– æ¨¡å‹å›å¤:")
#         try:
#             response = predictor.chat(img_prompt, images=[test_img_path], max_new_tokens=200)
#             print(response)
#         except Exception as e:
#             print(f"[ERROR] å›¾ç‰‡èŠå¤©åŠŸèƒ½å‡ºé”™: {e}")

#         # ========== æµ‹è¯•3: åŠ¨ä½œé¢„æµ‹åŠŸèƒ½ ==========
        print("\n" + "-"*60)
        print("ğŸ“Œ æµ‹è¯•3: åŠ¨ä½œé¢„æµ‹åŠŸèƒ½ï¼ˆè¿ç»­å¸§ï¼‰")
        print("-"*60)
        # æä¾›ä¸¤å¼ æœ‰ä½ç§»å˜åŒ–çš„å›¾ç‰‡è·¯å¾„
        frame1_path = "./data/vlm_finetune_dataset_fixed/images/sample_0_25_0.png"  # æ›¿æ¢ä¸ºå®é™…çš„ç¬¬ä¸€å¼ å›¾ç‰‡è·¯å¾„
        frame2_path = "./data/vlm_finetune_dataset_fixed/images/sample_0_25_1.png"  # æ›¿æ¢ä¸ºå®é™…çš„ç¬¬äºŒå¼ å›¾ç‰‡è·¯å¾„
        print(f"ä½¿ç”¨çš„å›¾ç‰‡è·¯å¾„:\n   Frame 1: {frame1_path}\n   Frame 2: {frame2_path}")
        try:
            direction, distance = predictor.get_action(frame1_path, frame2_path)
            print(f"\nğŸ¯ é¢„æµ‹ç»“æœ:")
            print(f"   â€¢ æ–¹å‘: {'â†’ å‘å³ (+)' if direction else 'â† å‘å·¦ (-)'}")  # ä¿®æ­£æ–¹å‘æè¿°
            print(f"   â€¢ è·ç¦»: {distance} åƒç´ ")
        except Exception as e:
            print(f"[ERROR] åŠ¨ä½œé¢„æµ‹æµ‹è¯•å‡ºé”™: {e}")

        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("="*60 + "\n")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # ç¡®ä¿èµ„æºé‡Šæ”¾
        if 'predictor' in locals():
            predictor.close()

if __name__ == "__main__":
    main()
