#!/usr/bin/env python
"""
Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•å¾®è°ƒå‰åæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¯¹æ¯”
"""
import json
import torch
import datetime
from datasets import Dataset
from transformers import AutoModelForVision2Seq, AutoProcessor
from peft import PeftModel
import os
from PIL import Image
import base64
from io import BytesIO
import re
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import numpy as np
import os
import yaml

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs('./logs', exist_ok=True)

def load_config(config_path="./config/evaluation_config.yaml"):
    """åŠ è½½è¯„ä¼°é…ç½®æ–‡ä»¶"""
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
    if not os.path.exists(config_path):
        print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        return {
            'test_dataset_path': './vlm_finetune_dataset.json',
            'base_model_path': '/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct',
            'results_output_dir': './logs/',
            'max_test_samples': 60,
            'max_preview_samples': 3,
            'max_tokens': 256,
            'temperature': 0.1
        }

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def load_test_data(config):
    """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
    print(f"ä» {config['test_dataset_path']} åŠ è½½æµ‹è¯•æ•°æ®...")

    try:
        #with open('./vlm_test_dataset.json', 'r', encoding='utf-8') as f:
        with open(config['test_dataset_path'], 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    except FileNotFoundError:
        print("è­¦å‘Š: æ‰¾ä¸åˆ°vlm_test_dataset.jsonï¼Œå°è¯•ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æœ€åéƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†...")
        backup_path = config['test_dataset_path'].replace('.json', '_fixed.json')
        try:
            with open(backup_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except FileNotFoundError:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°å¤‡ç”¨æ•°æ®é›†æ–‡ä»¶ {backup_path}")
            return []
        # ä½¿ç”¨æœ€å10%ä½œä¸ºæµ‹è¯•é›†
        split_idx = int(0.9 * len(raw_data))
        raw_data = raw_data[split_idx:]

    # ç®€å•çš„æ•°æ®æå– - ä»conversationså­—æ®µä¸­æå–é—®é¢˜å’Œç­”æ¡ˆ
    test_data = []
    for item in raw_data:
        conversations = item.get('conversations', [])
        if not conversations or len(conversations) < 0:
            continue

        # æå–ç¬¬ä¸€ä¸ªç”¨æˆ·é—®é¢˜å’Œå¯¹åº”åŠ©æ‰‹å›ç­”
        user_msg = ""
        assistant_msg = ""

        for conv in conversations:
            role = conv.get('role', '')
            content = conv.get('content', [])  # contentå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨

            if role == 'user':
                if isinstance(content, str):
                    user_msg = content
                elif isinstance(content, list):
                    for content_item in content:
                        if isinstance(content_item, dict):
                            content_type = content_item.get('type', '')
                            if content_type == 'text':
                                user_msg += content_item.get('text', '')
                            elif content_type == 'image':
                                # æ·»åŠ å›¾åƒæ ‡è®°ï¼Œå®é™…å›¾åƒå°†åœ¨æ¨¡å‹æ¨ç†æ—¶å¤„ç†
                                user_msg += " [IMAGE]"
            elif role == 'assistant':
                if isinstance(content, str):
                    assistant_msg = content
                elif isinstance(content, list):
                    for content_item in content:
                        if isinstance(content_item, dict):
                            content_type = content_item.get('type', '')
                            if content_type == 'text':
                                assistant_msg += content_item.get('text', '')

        if user_msg and assistant_msg:
            test_data.append({
                'id': item.get('id', f'test_{len(test_data)}'),
                'question': user_msg.strip(),
                'answer': assistant_msg.strip(),
            })

    print(f"åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return test_data

def prepare_model(config, is_finetuned=False):
    """å‡†å¤‡æ¨¡å‹ - åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹"""
    model_name = config['base_model_path']

    if is_finetuned:
        print("å‡†å¤‡å¾®è°ƒåçš„æ¨¡å‹...")

        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        # å°è¯•åŠ è½½å¾®è°ƒæ¨¡å‹ï¼ˆå¯èƒ½æ˜¯LoRAæˆ–å…¨é‡å¾®è°ƒï¼‰
        try:
            from peft import PeftModel
            import json

            # ä»é…ç½®æ–‡ä»¶è·å–æ¨¡å‹æœç´¢è·¯å¾„
            model_paths = config.get('finetuned_model_paths', [
                "./qwen3-vl-2b-instruct-full",          # å…¨é‡å¾®è°ƒæ¨¡å‹è·¯å¾„
                "./qwen3-vl-2b-instruct-lora",          # LoRAå•å¡æ¨¡å‹è·¯å¾„
                "./qwen3-vl-2b-instruct-lora-multigpu", # LoRAå¤šå¡æ¨¡å‹è·¯å¾„
                "./logs/qwen3-vl-2b-instruct-lora",      # LoRAå•å¡æ¨¡å‹è·¯å¾„(æ—¥å¿—)
                "./logs/qwen3-vl-2b-instruct-lora-multigpu"  # LoRAå¤šå¡æ¨¡å‹è·¯å¾„(æ—¥å¿—)
            ])

            loaded_model = False
            for model_path in model_paths:
                if os.path.exists(model_path):
                    print(f"å‘ç°æ¨¡å‹è·¯å¾„: {model_path}")

                    # æ£€æŸ¥æ˜¯å¦ä¸ºå…¨é‡å¾®è°ƒæ¨¡å‹
                    config_path = os.path.join(model_path, "config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r', encoding='utf-8') as f:
                            config = json.load(f)

                        # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦æœ‰adapterç›¸å…³çš„ä¿¡æ¯æ¥åˆ¤æ–­æ¨¡å‹ç±»å‹
                        has_adapter_info = any(key in config for key in ["peft_type", "adapter_config", "adapters"])

                        if has_adapter_info:
                            # è¿™æ˜¯ä¸€ä¸ªLoRAæ¨¡å‹
                            print(f"åŠ è½½LoRAé€‚é…å™¨: {model_path}")
                            model = PeftModel.from_pretrained(model, model_path)
                            model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡è¿›è¡Œè¯„ä¼°
                            print("LoRAé€‚é…å™¨å·²åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­")
                            loaded_model = True
                            break
                        else:
                            # è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå…¨é‡å¾®è°ƒæ¨¡å‹
                            print(f"åŠ è½½å…¨é‡å¾®è°ƒæ¨¡å‹: {model_path}")
                            model = AutoModelForVision2Seq.from_pretrained(
                                model_path,
                                torch_dtype=torch.bfloat16,
                                device_map="auto",
                                trust_remote_code=True,
                                low_cpu_mem_usage=True
                            )
                            loaded_model = True
                            break
                    else:
                        # å¦‚æœæ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œå°è¯•ä½œä¸ºLoRAæ¨¡å‹åŠ è½½
                        try:
                            print(f"å°è¯•ä½œä¸ºLoRAæ¨¡å‹åŠ è½½: {model_path}")
                            model = PeftModel.from_pretrained(model, model_path)
                            model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡è¿›è¡Œè¯„ä¼°
                            print("LoRAé€‚é…å™¨å·²åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­")
                            loaded_model = True
                            break
                        except:
                            print(f"æ— æ³•ä½œä¸ºLoRAæ¨¡å‹åŠ è½½: {model_path}")
                            continue

            if not loaded_model:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¾®è°ƒæ¨¡å‹è·¯å¾„")

        except ImportError:
            print(f"è­¦å‘Š: PEFTåº“æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œå°è¯•ç›´æ¥åŠ è½½æ¨¡å‹")
            # å¦‚æœæ— æ³•å¯¼å…¥peftï¼Œå°è¯•ä½œä¸ºå®Œæ•´æ¨¡å‹åŠ è½½
            model_paths = config.get('finetuned_model_paths', [
                "./qwen3-vl-2b-instruct-full",
                "./qwen3-vl-2b-instruct-lora",
                "./qwen3-vl-2b-instruct-lora-multigpu",
                "./logs/qwen3-vl-2b-instruct-lora",
                "./logs/qwen3-vl-2b-instruct-lora-multigpu"
            ])

            for model_path in model_paths:
                if os.path.exists(model_path):
                    try:
                        print(f"ç›´æ¥åŠ è½½æ¨¡å‹: {model_path}")
                        model = AutoModelForVision2Seq.from_pretrained(
                            model_path,
                            torch_dtype=torch.bfloat16,
                            device_map="auto",
                            trust_remote_code=True,
                            low_cpu_mem_usage=True
                        )
                        print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
                        break
                    except Exception as e:
                        print(f"åŠ è½½æ¨¡å‹å¤±è´¥ {model_path}: {e}")
                        continue
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½å¾®è°ƒæ¨¡å‹å¤±è´¥: {e}")
    else:
        print("å‡†å¤‡åŸºç¡€æ¨¡å‹...")
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

    processor = AutoProcessor.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    return model, processor

def evaluate_model(model, processor, test_data, config, model_name="æ¨¡å‹", log_details=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nå¼€å§‹è¯„ä¼° {model_name}...")

    correct_predictions = 0
    total_samples = 0
    predictions = []
    references = []
    detailed_results = []  # å­˜å‚¨è¯¦ç»†ç»“æœ

    # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´
    test_samples = test_data[:min(config.get('max_test_samples', 60), len(test_data))]

    for i, item in enumerate(test_samples):
        print(f"å¤„ç†æµ‹è¯•æ ·æœ¬ {i+1}/{len(test_samples)}", end='', flush=True)

        try:
            question = item.get('question', '')
            expected_answer = item.get('answer', '')
            sample_id = item.get('id', f'sample_{i}')

            if not question or not expected_answer:
                print(" (è·³è¿‡ - ç¼ºå°‘é—®é¢˜æˆ–ç­”æ¡ˆ)")
                continue

            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": question
                }
            ]

            # åº”ç”¨å¯¹è¯æ¨¡æ¿
            text = processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # å‡†å¤‡è¾“å…¥
            inputs = processor(
                text=text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )

            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            # ç”Ÿæˆé¢„æµ‹
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=config.get('max_tokens', 256),
                    temperature=config.get('temperature', 0.1),
                    do_sample=False,
                    pad_token_id=processor.tokenizer.eos_token_id
                )

            # è§£ç ç”Ÿæˆç»“æœ
            generated_ids_trimmed = generated_ids[:, inputs['input_ids'].shape[1]:]
            predicted_answer = processor.tokenizer.decode(
                generated_ids_trimmed[0],
                skip_special_tokens=True
            ).strip()

            # æ£€æŸ¥é¢„æµ‹æ˜¯å¦ä¸æœŸæœ›ç­”æ¡ˆåŒ¹é…
            expected_clean = re.sub(r'[^\w\s]', '', expected_answer.lower().strip())
            predicted_clean = re.sub(r'[^\w\s]', '', predicted_answer.lower().strip())

            # å¤šç§åŒ¹é…ç­–ç•¥
            is_correct = False
            if expected_clean == predicted_clean:
                is_correct = True
            elif expected_clean in predicted_clean or predicted_clean in expected_clean:
                is_correct = True
            else:
                # å…³é”®è¯åŒ¹é…
                expected_words = set(expected_clean.split())
                predicted_words = set(predicted_clean.split())

                if len(expected_words) > 0:
                    common_words = expected_words.intersection(predicted_words)
                    if len(common_words) / len(expected_words) >= 0.6:  # 60%çš„å…³é”®è¯åŒ¹é…
                        is_correct = True

            if is_correct:
                correct_predictions += 1
                status = "âœ“"
            else:
                status = "âœ—"

            total_samples += 1

            predictions.append(predicted_answer)
            references.append(expected_answer)

            # è®°å½•è¯¦ç»†ç»“æœ
            detailed_result = {
                'sample_id': sample_id,
                'question': question,
                'expected_answer': expected_answer,
                'predicted_answer': predicted_answer,
                'is_correct': is_correct,
                'status': status
            }
            detailed_results.append(detailed_result)

            print(f" {status}")

        except Exception as e:
            print(f" é”™è¯¯: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè®°å½•è¯¥æ ·æœ¬çš„ç»“æœ
            detailed_result = {
                'sample_id': f'sample_{i}',
                'question': item.get('question', ''),
                'expected_answer': item.get('answer', ''),
                'predicted_answer': f'ERROR: {str(e)}',
                'is_correct': False,
                'status': 'âœ—'
            }
            detailed_results.append(detailed_result)
            continue

    accuracy = correct_predictions / total_samples if total_samples > 0 else 0

    print(f"\n{model_name} è¯„ä¼°å®Œæˆ")
    print(f"å‡†ç¡®ç‡: {correct_predictions}/{total_samples} ({accuracy:.4f})")

    return accuracy, predictions, references, detailed_results

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°")
    print("="*70)

    # åŠ è½½é…ç½®
    import sys
    config_path = sys.argv[1] if len(sys.argv) > 1 else "./config/evaluation_config.yaml"
    config = load_config(config_path)
    print(f"ä½¿ç”¨è¯„ä¼°é…ç½®: {config_path}")

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(config)

    if len(test_data) == 0:
        print("é”™è¯¯: æµ‹è¯•æ•°æ®ä¸ºç©º")
        return

    print(f"ä½¿ç”¨ {min(config.get('max_test_samples', 60), len(test_data))} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")

    # è¯„ä¼°åŸºç¡€æ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°åŸºç¡€æ¨¡å‹ (Qwen3-VL-2B-Instruct)...")
    base_model, base_processor = prepare_model(config, is_finetuned=False)
    base_acc, base_preds, base_refs, base_detailed_results = evaluate_model(base_model, base_processor, test_data, config, "åŸºç¡€æ¨¡å‹")

    # è¯„ä¼°å¾®è°ƒæ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°å¾®è°ƒæ¨¡å‹ (Qwen3-VL-2B-Instruct-LoRA)...")
    ft_model, ft_processor = prepare_model(config, is_finetuned=True)
    ft_acc, ft_preds, ft_refs, ft_detailed_results = evaluate_model(ft_model, ft_processor, test_data, config, "å¾®è°ƒæ¨¡å‹")

    # è®¡ç®—æ”¹è¿›
    acc_improvement = ft_acc - base_acc
    if base_acc != 0:
        improvement_percentage = (acc_improvement / base_acc) * 100
    else:
        improvement_percentage = float('inf') if acc_improvement > 0 else float('-inf')

    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    print("\n" + "="*70)
    print("å¾®è°ƒæ•ˆæœæ¯”è¾ƒç»“æœ")
    print("="*70)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {min(60, len(test_data))}")
    print()
    print("åŸºç¡€æ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {base_acc:.4f}")
    print()
    print("å¾®è°ƒæ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {ft_acc:.4f}")
    print()
    print("æ€§èƒ½æ”¹è¿›:")
    print(f"  ç»å¯¹æ”¹è¿›: {acc_improvement:+.4f}")
    print(f"  ç›¸å¯¹æ”¹è¿›: {improvement_percentage:+.2f}%")

    # åˆ†ææ”¹è¿›æƒ…å†µ
    print()
    if acc_improvement > 0.01:  # è¶…è¿‡1%çš„æ”¹è¿›
        print("ğŸ‰ å¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½!")
    elif acc_improvement > 0:
        print("âœ… å¾®è°ƒå¯¹æ¨¡å‹æœ‰è½»å¾®æ”¹è¿›")
    elif acc_improvement == 0:
        print("â†’ å¾®è°ƒå¯¹æ¨¡å‹æ€§èƒ½æ— å½±å“")
    else:
        print("âš ï¸  å¾®è°ƒåæ¨¡å‹æ€§èƒ½ç•¥æœ‰ä¸‹é™")

    # æ˜¾ç¤ºè¯¦ç»†ç»“æœç¤ºä¾‹
    print("\n" + "-"*70)
    print("ç»“æœå¯¹æ¯”ç¤ºä¾‹ (å‰3ä¸ªæ ·æœ¬):")
    print("-"*70)
    for i in range(min(3, len(test_data))):
        if i < len(base_preds) and i < len(ft_preds):
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜: {test_data[i]['question'][:100]}...")
            print(f"  ç­”æ¡ˆ: {test_data[i]['answer'][:100]}")
            print(f"  åŸºç¡€: {base_preds[i][:100]}")
            print(f"  å¾®è°ƒ: {ft_preds[i][:100]}")
            print()

    # åˆ›å»ºå®Œæ•´çš„å¯¹æ¯”ç»“æœ
    comparison_results = []
    for i in range(len(base_detailed_results)):
        if i < len(ft_detailed_results):
            comparison_entry = {
                'sample_id': base_detailed_results[i]['sample_id'],
                'question': base_detailed_results[i]['question'],
                'expected_answer': base_detailed_results[i]['expected_answer'],
                'base_prediction': base_detailed_results[i]['predicted_answer'],
                'base_is_correct': base_detailed_results[i]['is_correct'],
                'base_status': base_detailed_results[i]['status'],
                'ft_prediction': ft_detailed_results[i]['predicted_answer'],
                'ft_is_correct': ft_detailed_results[i]['is_correct'],
                'ft_status': ft_detailed_results[i]['status'],
                'improved': ft_detailed_results[i]['is_correct'] and not base_detailed_results[i]['is_correct'],
                'regressed': not ft_detailed_results[i]['is_correct'] and base_detailed_results[i]['is_correct']
            }
            comparison_results.append(comparison_entry)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_results = {
        'test_samples_count': min(config.get('max_test_samples', 60), len(test_data)),
        'base_model_accuracy': base_acc,
        'fine_tuned_model_accuracy': ft_acc,
        'absolute_improvement': acc_improvement,
        'relative_improvement_percent': improvement_percentage,
        'timestamp': str(datetime.datetime.now()),
        'test_samples_preview': [
            {
                'question': test_data[i]['question'][:200],
                'expected': test_data[i]['answer'][:200] if i < len(test_data) else '',
                'base_prediction': base_preds[i][:200] if i < len(base_preds) else '',
                'fine_tuned_prediction': ft_preds[i][:200] if i < len(ft_preds) else ''
            }
            for i in range(min(config.get('max_preview_samples', 3), len(test_data)))
        ],
        'detailed_comparison_results': comparison_results,
        'base_model_detailed_results': base_detailed_results,
        'fine_tuned_model_detailed_results': ft_detailed_results
    }

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    result_file = config['results_output_dir'] + "qwen3_vl_finetuning_evaluation.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)

    # ä¿å­˜è¯¦ç»†çš„æ–‡æœ¬æ—¥å¿—
    log_file = config['results_output_dir'] + "fine_tuning_evaluation_final.log"
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è¯¦ç»†æ—¥å¿—\n")
        f.write("="*70 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {datetime.datetime.now()}\n")
        f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {min(config.get('max_test_samples', 60), len(test_data))}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_acc:.4f}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {ft_acc:.4f}\n")
        f.write(f"ç»å¯¹æ”¹è¿›: {acc_improvement:+.4f}\n")
        f.write(f"ç›¸å¯¹æ”¹è¿›: {improvement_percentage:+.2f}%\n")

        f.write("\n" + "="*70 + "\n")
        f.write("è¯¦ç»†å¯¹æ¯”ç»“æœ:\n")
        f.write("="*70 + "\n")

        improved_count = 0
        regressed_count = 0
        same_count = 0

        for i, result in enumerate(comparison_results):
            f.write(f"æ ·æœ¬ {i+1} (ID: {result['sample_id']}):\n")
            f.write(f"  é—®é¢˜: {result['question']}\n")
            f.write(f"  æ ‡å‡†ç­”æ¡ˆ: {result['expected_answer']}\n")
            f.write(f"  åŸºç¡€æ¨¡å‹é¢„æµ‹: {result['base_prediction']} [{result['base_status']}]\n")
            f.write(f"  å¾®è°ƒæ¨¡å‹é¢„æµ‹: {result['ft_prediction']} [{result['ft_status']}]\n")

            if result['improved']:
                f.write(f"  ç»“æœ: âœ… å¾®è°ƒæ¨¡å‹æ”¹è¿›\n")
                improved_count += 1
            elif result['regressed']:
                f.write(f"  ç»“æœ: âŒ å¾®è°ƒæ¨¡å‹é€€æ­¥\n")
                regressed_count += 1
            else:
                f.write(f"  ç»“æœ: â†”ï¸  æ— å˜åŒ–\n")
                same_count += 1

            f.write("\n")

        f.write("="*70 + "\n")
        f.write("ç»Ÿè®¡æ‘˜è¦:\n")
        f.write("="*70 + "\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(comparison_results)}\n")
        f.write(f"å¾®è°ƒæ”¹è¿›æ ·æœ¬æ•°: {improved_count}\n")
        f.write(f"å¾®è°ƒé€€æ­¥æ ·æœ¬æ•°: {regressed_count}\n")
        f.write(f"æ— å˜åŒ–æ ·æœ¬æ•°: {same_count}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_acc:.4f}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {ft_acc:.4f}\n")
        f.write(f"å‡†ç¡®ç‡æå‡: {acc_improvement:+.4f}\n")

    print(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print(f"è¯¦ç»†å¯¹æ¯”æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

    print("\n" + "="*70)
    print("è¯„ä¼°å®Œæˆ!")
    print("="*70)

if __name__ == "__main__":
    main()