#!/usr/bin/env python
"""
Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•å¾®è°ƒå‰åæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¯¹æ¯”
"""
import json
import torch
import datetime
from datasets import Dataset
from transformers import AutoModelForVision2Seq, AutoProcessor
from peft import PeftModel
import os
from PIL import Image
import base64
from io import BytesIO
import re
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import numpy as np
import os
# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs('./logs', exist_ok=True)

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
    print("ä» ./vlm_test_dataset.json åŠ è½½æµ‹è¯•æ•°æ®...")

    try:
        #with open('./vlm_test_dataset.json', 'r', encoding='utf-8') as f:
        with open('./vlm_finetune_dataset.json', 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    except FileNotFoundError:
        print("è­¦å‘Š: æ‰¾ä¸åˆ°vlm_test_dataset.jsonï¼Œå°è¯•ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æœ€åéƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†...")
        with open('./vlm_finetune_dataset_fixed.json', 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        # ä½¿ç”¨æœ€å10%ä½œä¸ºæµ‹è¯•é›†
        split_idx = int(0.9 * len(raw_data))
        raw_data = raw_data[split_idx:]

    # ç®€å•çš„æ•°æ®æå– - ä»conversationså­—æ®µä¸­æå–é—®é¢˜å’Œç­”æ¡ˆ
    test_data = []
    for item in raw_data:
        conversations = item.get('conversations', [])
        if not conversations or len(conversations) < 0:
            continue

        # æå–ç¬¬ä¸€ä¸ªç”¨æˆ·é—®é¢˜å’Œå¯¹åº”åŠ©æ‰‹å›ç­”
        user_msg = ""
        assistant_msg = ""

        for conv in conversations:
            role = conv.get('role', '')
            content = conv.get('content', [])  # contentå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨

            if role == 'user':
                if isinstance(content, str):
                    user_msg = content
                elif isinstance(content, list):
                    for content_item in content:
                        if isinstance(content_item, dict):
                            content_type = content_item.get('type', '')
                            if content_type == 'text':
                                user_msg += content_item.get('text', '')
                            elif content_type == 'image':
                                # æ·»åŠ å›¾åƒæ ‡è®°ï¼Œå®é™…å›¾åƒå°†åœ¨æ¨¡å‹æ¨ç†æ—¶å¤„ç†
                                user_msg += " [IMAGE]"
            elif role == 'assistant':
                if isinstance(content, str):
                    assistant_msg = content
                elif isinstance(content, list):
                    for content_item in content:
                        if isinstance(content_item, dict):
                            content_type = content_item.get('type', '')
                            if content_type == 'text':
                                assistant_msg += content_item.get('text', '')

        if user_msg and assistant_msg:
            test_data.append({
                'id': item.get('id', f'test_{len(test_data)}'),
                'question': user_msg.strip(),
                'answer': assistant_msg.strip(),
            })

    print(f"åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return test_data

def prepare_model(is_finetuned=False):
    """å‡†å¤‡æ¨¡å‹ - åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹"""
    model_name = "/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct"

    if is_finetuned:
        print("å‡†å¤‡å¾®è°ƒåçš„æ¨¡å‹...")

        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        # å°è¯•åŠ è½½LoRAé€‚é…å™¨
        try:
            from peft import PeftModel
            # Check for both single GPU and multi GPU model paths in root directory
            lora_paths = [
                # "./qwen3-vl-2b-instruct-lora",           # Single GPU version
                "./qwen3-vl-2b-instruct-lora-multigpu",  # Multi GPU version
            ]

            lora_path = None
            for path in lora_paths:
                if os.path.exists(path):
                    lora_path = path
                    print(f"åŠ è½½LoRAé€‚é…å™¨: {lora_path}")
                    break

            if lora_path:
                model = PeftModel.from_pretrained(model, lora_path)
                model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡è¿›è¡Œè¯„ä¼°
                print("LoRAé€‚é…å™¨å·²åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­")
            else:
                print(f"è­¦å‘Š: LoRAé€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨äºä»»ä½•é¢„æœŸä½ç½®: {lora_paths}")
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½LoRAé€‚é…å™¨å¤±è´¥: {e}")
    else:
        print("å‡†å¤‡åŸºç¡€æ¨¡å‹...")
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

    processor = AutoProcessor.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    return model, processor

def evaluate_model(model, processor, test_data, model_name="æ¨¡å‹", log_details=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nå¼€å§‹è¯„ä¼° {model_name}...")

    correct_predictions = 0
    total_samples = 0
    predictions = []
    references = []
    detailed_results = []  # å­˜å‚¨è¯¦ç»†ç»“æœ

    # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´
    test_samples = test_data[:min(60, len(test_data))]

    for i, item in enumerate(test_samples):
        print(f"å¤„ç†æµ‹è¯•æ ·æœ¬ {i+1}/{len(test_samples)}", end='', flush=True)

        try:
            question = item.get('question', '')
            expected_answer = item.get('answer', '')
            sample_id = item.get('id', f'sample_{i}')

            if not question or not expected_answer:
                print(" (è·³è¿‡ - ç¼ºå°‘é—®é¢˜æˆ–ç­”æ¡ˆ)")
                continue

            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": question
                }
            ]

            # åº”ç”¨å¯¹è¯æ¨¡æ¿
            text = processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # å‡†å¤‡è¾“å…¥
            inputs = processor(
                text=text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )

            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            # ç”Ÿæˆé¢„æµ‹
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.1,
                    do_sample=False,
                    pad_token_id=processor.tokenizer.eos_token_id
                )

            # è§£ç ç”Ÿæˆç»“æœ
            generated_ids_trimmed = generated_ids[:, inputs['input_ids'].shape[1]:]
            predicted_answer = processor.tokenizer.decode(
                generated_ids_trimmed[0],
                skip_special_tokens=True
            ).strip()

            # æ£€æŸ¥é¢„æµ‹æ˜¯å¦ä¸æœŸæœ›ç­”æ¡ˆåŒ¹é…
            expected_clean = re.sub(r'[^\w\s]', '', expected_answer.lower().strip())
            predicted_clean = re.sub(r'[^\w\s]', '', predicted_answer.lower().strip())

            # å¤šç§åŒ¹é…ç­–ç•¥
            is_correct = False
            if expected_clean == predicted_clean:
                is_correct = True
            elif expected_clean in predicted_clean or predicted_clean in expected_clean:
                is_correct = True
            else:
                # å…³é”®è¯åŒ¹é…
                expected_words = set(expected_clean.split())
                predicted_words = set(predicted_clean.split())

                if len(expected_words) > 0:
                    common_words = expected_words.intersection(predicted_words)
                    if len(common_words) / len(expected_words) >= 0.6:  # 60%çš„å…³é”®è¯åŒ¹é…
                        is_correct = True

            if is_correct:
                correct_predictions += 1
                status = "âœ“"
            else:
                status = "âœ—"

            total_samples += 1

            predictions.append(predicted_answer)
            references.append(expected_answer)

            # è®°å½•è¯¦ç»†ç»“æœ
            detailed_result = {
                'sample_id': sample_id,
                'question': question,
                'expected_answer': expected_answer,
                'predicted_answer': predicted_answer,
                'is_correct': is_correct,
                'status': status
            }
            detailed_results.append(detailed_result)

            print(f" {status}")

        except Exception as e:
            print(f" é”™è¯¯: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè®°å½•è¯¥æ ·æœ¬çš„ç»“æœ
            detailed_result = {
                'sample_id': f'sample_{i}',
                'question': item.get('question', ''),
                'expected_answer': item.get('answer', ''),
                'predicted_answer': f'ERROR: {str(e)}',
                'is_correct': False,
                'status': 'âœ—'
            }
            detailed_results.append(detailed_result)
            continue

    accuracy = correct_predictions / total_samples if total_samples > 0 else 0

    print(f"\n{model_name} è¯„ä¼°å®Œæˆ")
    print(f"å‡†ç¡®ç‡: {correct_predictions}/{total_samples} ({accuracy:.4f})")

    return accuracy, predictions, references, detailed_results

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°")
    print("="*70)

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data()

    if len(test_data) == 0:
        print("é”™è¯¯: æµ‹è¯•æ•°æ®ä¸ºç©º")
        return

    print(f"ä½¿ç”¨ {min(60, len(test_data))} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")

    # è¯„ä¼°åŸºç¡€æ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°åŸºç¡€æ¨¡å‹ (Qwen3-VL-2B-Instruct)...")
    base_model, base_processor = prepare_model(is_finetuned=False)
    base_acc, base_preds, base_refs, base_detailed_results = evaluate_model(base_model, base_processor, test_data, "åŸºç¡€æ¨¡å‹")

    # è¯„ä¼°å¾®è°ƒæ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°å¾®è°ƒæ¨¡å‹ (Qwen3-VL-2B-Instruct-LoRA)...")
    ft_model, ft_processor = prepare_model(is_finetuned=True)
    ft_acc, ft_preds, ft_refs, ft_detailed_results = evaluate_model(ft_model, ft_processor, test_data, "å¾®è°ƒæ¨¡å‹")

    # è®¡ç®—æ”¹è¿›
    acc_improvement = ft_acc - base_acc
    if base_acc != 0:
        improvement_percentage = (acc_improvement / base_acc) * 100
    else:
        improvement_percentage = float('inf') if acc_improvement > 0 else float('-inf')

    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    print("\n" + "="*70)
    print("å¾®è°ƒæ•ˆæœæ¯”è¾ƒç»“æœ")
    print("="*70)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {min(60, len(test_data))}")
    print()
    print("åŸºç¡€æ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {base_acc:.4f}")
    print()
    print("å¾®è°ƒæ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {ft_acc:.4f}")
    print()
    print("æ€§èƒ½æ”¹è¿›:")
    print(f"  ç»å¯¹æ”¹è¿›: {acc_improvement:+.4f}")
    print(f"  ç›¸å¯¹æ”¹è¿›: {improvement_percentage:+.2f}%")

    # åˆ†ææ”¹è¿›æƒ…å†µ
    print()
    if acc_improvement > 0.01:  # è¶…è¿‡1%çš„æ”¹è¿›
        print("ğŸ‰ å¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½!")
    elif acc_improvement > 0:
        print("âœ… å¾®è°ƒå¯¹æ¨¡å‹æœ‰è½»å¾®æ”¹è¿›")
    elif acc_improvement == 0:
        print("â†’ å¾®è°ƒå¯¹æ¨¡å‹æ€§èƒ½æ— å½±å“")
    else:
        print("âš ï¸  å¾®è°ƒåæ¨¡å‹æ€§èƒ½ç•¥æœ‰ä¸‹é™")

    # æ˜¾ç¤ºè¯¦ç»†ç»“æœç¤ºä¾‹
    print("\n" + "-"*70)
    print("ç»“æœå¯¹æ¯”ç¤ºä¾‹ (å‰3ä¸ªæ ·æœ¬):")
    print("-"*70)
    for i in range(min(3, len(test_data))):
        if i < len(base_preds) and i < len(ft_preds):
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜: {test_data[i]['question'][:100]}...")
            print(f"  ç­”æ¡ˆ: {test_data[i]['answer'][:100]}")
            print(f"  åŸºç¡€: {base_preds[i][:100]}")
            print(f"  å¾®è°ƒ: {ft_preds[i][:100]}")
            print()

    # åˆ›å»ºå®Œæ•´çš„å¯¹æ¯”ç»“æœ
    comparison_results = []
    for i in range(len(base_detailed_results)):
        if i < len(ft_detailed_results):
            comparison_entry = {
                'sample_id': base_detailed_results[i]['sample_id'],
                'question': base_detailed_results[i]['question'],
                'expected_answer': base_detailed_results[i]['expected_answer'],
                'base_prediction': base_detailed_results[i]['predicted_answer'],
                'base_is_correct': base_detailed_results[i]['is_correct'],
                'base_status': base_detailed_results[i]['status'],
                'ft_prediction': ft_detailed_results[i]['predicted_answer'],
                'ft_is_correct': ft_detailed_results[i]['is_correct'],
                'ft_status': ft_detailed_results[i]['status'],
                'improved': ft_detailed_results[i]['is_correct'] and not base_detailed_results[i]['is_correct'],
                'regressed': not ft_detailed_results[i]['is_correct'] and base_detailed_results[i]['is_correct']
            }
            comparison_results.append(comparison_entry)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_results = {
        'test_samples_count': min(60, len(test_data)),
        'base_model_accuracy': base_acc,
        'fine_tuned_model_accuracy': ft_acc,
        'absolute_improvement': acc_improvement,
        'relative_improvement_percent': improvement_percentage,
        'timestamp': str(datetime.datetime.now()),
        'test_samples_preview': [
            {
                'question': test_data[i]['question'][:200],
                'expected': test_data[i]['answer'][:200] if i < len(test_data) else '',
                'base_prediction': base_preds[i][:200] if i < len(base_preds) else '',
                'fine_tuned_prediction': ft_preds[i][:200] if i < len(ft_preds) else ''
            }
            for i in range(min(3, len(test_data)))
        ],
        'detailed_comparison_results': comparison_results,
        'base_model_detailed_results': base_detailed_results,
        'fine_tuned_model_detailed_results': ft_detailed_results
    }

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    result_file = "./logs/qwen3_vl_finetuning_evaluation.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)

    # ä¿å­˜è¯¦ç»†çš„æ–‡æœ¬æ—¥å¿—
    log_file = "./logs/fine_tuning_evaluation_final.log"
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è¯¦ç»†æ—¥å¿—\n")
        f.write("="*70 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {datetime.datetime.now()}\n")
        f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {min(60, len(test_data))}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_acc:.4f}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {ft_acc:.4f}\n")
        f.write(f"ç»å¯¹æ”¹è¿›: {acc_improvement:+.4f}\n")
        f.write(f"ç›¸å¯¹æ”¹è¿›: {improvement_percentage:+.2f}%\n")

        f.write("\n" + "="*70 + "\n")
        f.write("è¯¦ç»†å¯¹æ¯”ç»“æœ:\n")
        f.write("="*70 + "\n")

        improved_count = 0
        regressed_count = 0
        same_count = 0

        for i, result in enumerate(comparison_results):
            f.write(f"æ ·æœ¬ {i+1} (ID: {result['sample_id']}):\n")
            f.write(f"  é—®é¢˜: {result['question']}\n")
            f.write(f"  æ ‡å‡†ç­”æ¡ˆ: {result['expected_answer']}\n")
            f.write(f"  åŸºç¡€æ¨¡å‹é¢„æµ‹: {result['base_prediction']} [{result['base_status']}]\n")
            f.write(f"  å¾®è°ƒæ¨¡å‹é¢„æµ‹: {result['ft_prediction']} [{result['ft_status']}]\n")

            if result['improved']:
                f.write(f"  ç»“æœ: âœ… å¾®è°ƒæ¨¡å‹æ”¹è¿›\n")
                improved_count += 1
            elif result['regressed']:
                f.write(f"  ç»“æœ: âŒ å¾®è°ƒæ¨¡å‹é€€æ­¥\n")
                regressed_count += 1
            else:
                f.write(f"  ç»“æœ: â†”ï¸  æ— å˜åŒ–\n")
                same_count += 1

            f.write("\n")

        f.write("="*70 + "\n")
        f.write("ç»Ÿè®¡æ‘˜è¦:\n")
        f.write("="*70 + "\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(comparison_results)}\n")
        f.write(f"å¾®è°ƒæ”¹è¿›æ ·æœ¬æ•°: {improved_count}\n")
        f.write(f"å¾®è°ƒé€€æ­¥æ ·æœ¬æ•°: {regressed_count}\n")
        f.write(f"æ— å˜åŒ–æ ·æœ¬æ•°: {same_count}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_acc:.4f}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {ft_acc:.4f}\n")
        f.write(f"å‡†ç¡®ç‡æå‡: {acc_improvement:+.4f}\n")

    print(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print(f"è¯¦ç»†å¯¹æ¯”æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

    print("\n" + "="*70)
    print("è¯„ä¼°å®Œæˆ!")
    print("="*70)

if __name__ == "__main__":
    main()