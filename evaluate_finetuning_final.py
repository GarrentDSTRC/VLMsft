#!/usr/bin/env python
"""
Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•å¾®è°ƒå‰åæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¯¹æ¯”
"""
import json
import torch
import datetime
import os
from datasets import Dataset
from transformers import Qwen3VLForConditionalGeneration, Qwen3VLProcessor
from peft import PeftModel
from PIL import Image
import base64
from io import BytesIO
import re
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import numpy as np
import yaml

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs('./logs', exist_ok=True)

def load_config(config_path="./config/evaluation_config.yaml"):
    """åŠ è½½è¯„ä¼°é…ç½®æ–‡ä»¶"""
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
    if not os.path.exists(config_path):
        print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        return {
            'test_dataset_path': './vlm_finetune_dataset.json',
            'base_model_path': '/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct',
            'results_output_dir': './logs/',
            'max_test_samples': 60,
            'max_preview_samples': 3,
            'max_tokens': 256,
            'temperature': 0.1
        }

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def load_test_data(config):
    """åŠ è½½æµ‹è¯•æ•°æ®é›† - é€‚é…Llama Factoryæ ¼å¼"""
    print(f"ä» {config['test_dataset_path']} åŠ è½½æµ‹è¯•æ•°æ®...")

    try:
        # å°è¯•åŠ è½½æ–°çš„Llama Factoryæ ¼å¼æ•°æ®
        with open(config['test_dataset_path'], 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    except FileNotFoundError:
        print("è­¦å‘Š: æ‰¾ä¸åˆ°æŒ‡å®šçš„æµ‹è¯•æ•°æ®é›†ï¼Œå°è¯•åŠ è½½action_data.json...")
        try:
            with open("./action_data.json", 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except FileNotFoundError:
            print("é”™è¯¯: æ‰¾ä¸åˆ°action_data.json")
            return []

    # å¤„ç†Llama Factoryæ ¼å¼çš„æ•°æ®
    test_data = []
    for item in raw_data:
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output = item.get('output', {})

        # æ„å»ºé—®é¢˜ - å°†instructionå’Œinputç»„åˆ
        question = instruction + "\n\n" + input_text if input_text else instruction

        # å¤„ç†è¾“å‡º - outputå¯èƒ½æ˜¯å­—å…¸æ ¼å¼
        if isinstance(output, dict):
            if 'reasoning' in output and 'action' in output:
                # æŒ‰ç…§JSONæ ¼å¼æ„å»ºç­”æ¡ˆ
                answer = json.dumps(output, ensure_ascii=False, indent=2)
            else:
                # å¦‚æœåªæœ‰éƒ¨åˆ†å­—æ®µï¼Œå°è¯•æ„å»ºç­”æ¡ˆ
                answer_parts = []
                for key, value in output.items():
                    if isinstance(value, list):
                        answer_parts.append(f"{key}: {str(value)}")
                    else:
                        answer_parts.append(f"{key}: {value}")
                answer = "\n".join(answer_parts)
        else:
            answer = str(output)

        # è·å–å›¾åƒè·¯å¾„
        image_paths = item.get('images', [])

        if question and answer:
            test_data.append({
                'id': item.get('id', f'test_{len(test_data)}'),
                'question': question.strip(),
                'answer': answer.strip(),
                'images': image_paths  # ä¿å­˜å›¾åƒè·¯å¾„ä¿¡æ¯
            })

    print(f"åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return test_data

def load_model_for_evaluation(model_path, adapter_path=None, device="cuda"):
    """
    æ™ºèƒ½åŠ è½½æ¨¡å‹ï¼šè‡ªåŠ¨è¯†åˆ«LoRA/å…¨é‡æ¨¡å‹
    """
    # æ£€æµ‹æ¨¡å‹ç±»å‹
    is_lora = (os.path.exists(os.path.join(model_path, "adapter_model.bin")) or
               os.path.exists(os.path.join(model_path, "adapter_model.safetensors"))) and \
              os.path.exists(os.path.join(model_path, "adapter_config.json"))

    # æ£€æµ‹å…¨é‡å¾®è°ƒæ¨¡å‹
    is_full = (os.path.exists(os.path.join(model_path, "pytorch_model.bin")) or
               os.path.exists(os.path.join(model_path, "model.safetensors"))) and \
              os.path.exists(os.path.join(model_path, "config.json"))

    if is_lora or adapter_path:
        # LoRAæ¨¡å¼ï¼šéœ€æŒ‡å®šåŸºç¡€æ¨¡å‹
        if adapter_path:
            # å¦‚æœæä¾›äº†adapter_pathï¼Œä½¿ç”¨å®ƒä½œä¸ºLoRAè·¯å¾„
            adapter_dir = adapter_path
        else:
            # å¦åˆ™ä½¿ç”¨model_pathä½œä¸ºLoRAè·¯å¾„
            adapter_dir = model_path

        # ä»adapter_config.jsonä¸­è·å–åŸºç¡€æ¨¡å‹åç§°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        adapter_config_path = os.path.join(adapter_dir, "adapter_config.json")
        if os.path.exists(adapter_config_path):
            with open(adapter_config_path, 'r', encoding='utf-8') as f:
                adapter_config = json.load(f)
            base_model = adapter_config.get("base_model_name_or_path", "/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct")
        else:
            base_model = "/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct"

        print(f"â†’ æ£€æµ‹åˆ°LoRAæ¨¡å‹ï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")

        model = Qwen3VLForConditionalGeneration.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )

        # åŠ è½½é€‚é…å™¨
        from peft import PeftModel
        model = PeftModel.from_pretrained(model, adapter_dir, is_trainable=False)
        print(f"âœ“ LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ: {adapter_dir}")

    elif is_full:
        # å…¨é‡æ¨¡å‹ï¼šç›´æ¥åŠ è½½
        print(f"â†’ åŠ è½½å…¨é‡å¾®è°ƒæ¨¡å‹: {model_path}")
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )
    else:
        # å¦‚æœéƒ½ä¸æ˜¯ï¼Œå°è¯•ä½œä¸ºåŸºç¡€æ¨¡å‹åŠ è½½
        print(f"â†’ å°è¯•ä½œä¸ºåŸºç¡€æ¨¡å‹åŠ è½½: {model_path}")
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )

    model.eval()
    # ç¡®å®šprocessorçš„è·¯å¾„ - å¦‚æœæ˜¯LoRAï¼Œå¯èƒ½éœ€è¦ä»åŸºç¡€æ¨¡å‹è·¯å¾„åŠ è½½
    if is_lora and not os.path.exists(os.path.join(model_path, "tokenizer_config.json")):
        # å¦‚æœæ˜¯LoRAä¸”ç›®æ ‡è·¯å¾„ç¼ºå°‘tokenizeré…ç½®ï¼Œä»åŸºç¡€æ¨¡å‹åŠ è½½
        processor = Qwen3VLProcessor.from_pretrained(
            base_model if 'base_model' in locals() else model_path,
            trust_remote_code=True
        )
    else:
        # å¦åˆ™ä»ç›®æ ‡è·¯å¾„åŠ è½½
        processor = Qwen3VLProcessor.from_pretrained(
            model_path,
            trust_remote_code=True
        )

    return model, processor


def prepare_model(config, is_finetuned=False):
    """å‡†å¤‡æ¨¡å‹ - åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹"""
    model_name = config['base_model_path']

    # ç¡®å®šä½¿ç”¨çš„è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        # ä½¿ç”¨ç‰¹å®šçš„GPUè®¾å¤‡è€Œä¸æ˜¯autoï¼Œä»¥é¿å…å¤šGPUè®¾å¤‡ä¸åŒ¹é…é—®é¢˜
        device = torch.device('cuda:0')  # å¼ºåˆ¶ä½¿ç”¨ç¬¬ä¸€å—GPU

    if is_finetuned:
        print("å‡†å¤‡å¾®è°ƒåçš„æ¨¡å‹...")

        # ä»é…ç½®æ–‡ä»¶è·å–æ¨¡å‹æœç´¢è·¯å¾„
        model_paths = config.get('finetuned_model_paths', [
            "./qwen3-vl-2b-instruct-full",          # å…¨é‡å¾®è°ƒæ¨¡å‹è·¯å¾„
            "./qwen3-vl-2b-instruct-lora",          # LoRAå•å¡æ¨¡å‹è·¯å¾„
            "./qwen3-vl-2b-instruct-lora-multigpu", # LoRAå¤šå¡æ¨¡å‹è·¯å¾„
            "./logs/qwen3-vl-2b-instruct-lora",      # LoRAå•å¡æ¨¡å‹è·¯å¾„(æ—¥å¿—)
            "./logs/qwen3-vl-2b-instruct-lora-multigpu"  # LoRAå¤šå¡æ¨¡å‹è·¯å¾„(æ—¥å¿—)
        ])

        loaded_model = False
        model = None
        processor = None

        for model_path in model_paths:
            if os.path.exists(model_path):
                print(f"å‘ç°æ¨¡å‹è·¯å¾„: {model_path}")

                try:
                    # é¦–å…ˆå°è¯•ç›´æ¥åŠ è½½
                    model, processor = load_model_for_evaluation(model_path, device=str(device))
                    loaded_model = True
                    print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
                    break
                except Exception as e:
                    print(f"ç›´æ¥åŠ è½½æ¨¡å‹å¤±è´¥ {model_path}: {e}")

                    # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•æœç´¢å­ç›®å½•ä¸­çš„æ£€æŸ¥ç‚¹
                    if os.path.isdir(model_path):
                        subdirs = [d for d in os.listdir(model_path)
                                  if os.path.isdir(os.path.join(model_path, d))]

                        # æŒ‰åç§°æ’åºï¼Œä¼˜å…ˆå°è¯•checkpoint-*ç›®å½•
                        subdirs_sorted = sorted(subdirs,
                                              key=lambda x: (not x.startswith('checkpoint-'), x))

                        for subdir in subdirs_sorted:
                            subdir_path = os.path.join(model_path, subdir)
                            print(f"å°è¯•å­ç›®å½•: {subdir_path}")

                            try:
                                model, processor = load_model_for_evaluation(subdir_path, device=str(device))
                                loaded_model = True
                                print(f"æˆåŠŸä»å­ç›®å½•åŠ è½½æ¨¡å‹: {subdir_path}")
                                break
                            except Exception as sub_e:
                                print(f"å­ç›®å½•åŠ è½½å¤±è´¥ {subdir_path}: {sub_e}")
                                continue

                    if loaded_model:
                        break

        if not loaded_model:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¾®è°ƒæ¨¡å‹è·¯å¾„")
            # å¦‚æœæ²¡æ‰¾åˆ°å¾®è°ƒæ¨¡å‹ï¼Œè¿”å›åŸºç¡€æ¨¡å‹
            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                device_map=device,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            processor = Qwen3VLProcessor.from_pretrained(
                model_name,
                trust_remote_code=True
            )
    else:
        print("å‡†å¤‡åŸºç¡€æ¨¡å‹...")
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map=device,  # ä½¿ç”¨æŒ‡å®šçš„å•ä¸€è®¾å¤‡
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        processor = Qwen3VLProcessor.from_pretrained(
            model_name,
            trust_remote_code=True
        )

    return model, processor

def evaluate_model(model, processor, test_data, config, model_name="æ¨¡å‹", log_details=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nå¼€å§‹è¯„ä¼° {model_name}...")

    correct_predictions = 0
    total_samples = 0
    predictions = []
    references = []
    detailed_results = []  # å­˜å‚¨è¯¦ç»†ç»“æœ

    # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´
    test_samples = test_data[:min(config.get('max_test_samples', 60), len(test_data))]

    for i, item in enumerate(test_samples):
        print(f"å¤„ç†æµ‹è¯•æ ·æœ¬ {i+1}/{len(test_samples)}", end='', flush=True)

        try:
            question = item.get('question', '')
            expected_answer = item.get('answer', '')
            sample_id = item.get('id', f'sample_{i}')

            if not question or not expected_answer:
                print(" (è·³è¿‡ - ç¼ºå°‘é—®é¢˜æˆ–ç­”æ¡ˆ)")
                continue

            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒä¿¡æ¯
            import base64
            from PIL import Image
            import io

            if 'images' in item and item['images']:
                # åŒ…å«å›¾åƒçš„æ¶ˆæ¯æ ¼å¼
                image_path = item['images'][0]  # å‡è®¾åªå¤„ç†ç¬¬ä¸€ä¸ªå›¾åƒ

                # æ£€æŸ¥å›¾åƒè·¯å¾„æ˜¯æœ¬åœ°æ–‡ä»¶è¿˜æ˜¯base64
                if image_path.startswith('data:image'):
                    # å¤„ç†base64å›¾åƒ
                    base64_str = image_path.split(',')[1]
                    image_bytes = base64.b64decode(base64_str)
                    pil_image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
                else:
                    # å¤„ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
                    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨å‡ ä¸ªå¯èƒ½çš„ä½ç½®æŸ¥æ‰¾
                    import os
                    possible_paths = [
                        image_path,  # åŸå§‹è·¯å¾„
                        os.path.join(os.path.dirname(config['test_dataset_path']), image_path),  # ç›¸å¯¹äºæ•°æ®é›†æ–‡ä»¶
                        os.path.join('.', image_path),  # ç›¸å¯¹å½“å‰ç›®å½•
                        os.path.join('..', image_path),  # ä¸Šçº§ç›®å½•
                    ]

                    pil_image = None
                    for img_path in possible_paths:
                        if os.path.exists(img_path):
                            pil_image = Image.open(img_path).convert('RGB')
                            break

                    if pil_image is None:
                        print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°å›¾åƒæ–‡ä»¶ {image_path}")
                        # å¦‚æœæ‰¾ä¸åˆ°å›¾åƒï¼ŒæŒ‰çº¯æ–‡æœ¬å¤„ç†
                        messages = [
                            {
                                "role": "user",
                                "content": question
                            }
                        ]

                        # åº”ç”¨å¯¹è¯æ¨¡æ¿
                        text = processor.apply_chat_template(
                            messages,
                            tokenize=False,
                            add_generation_prompt=True
                        )

                        # å‡†å¤‡è¾“å…¥
                        inputs = processor(
                            text=text,
                            return_tensors="pt",
                            padding=True,
                            truncation=True,
                            max_length=2048
                        )
                    else:
                        # ä½¿ç”¨messagesæ ¼å¼ï¼ŒåŒ…å«å›¾åƒæ ‡è®°ï¼Œç„¶ååº”ç”¨æ¨¡æ¿
                        messages = [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image"},
                                    {"type": "text", "text": question}
                                ]
                            }
                        ]

                        # åº”ç”¨å¯¹è¯æ¨¡æ¿
                        text = processor.apply_chat_template(
                            messages,
                            tokenize=False,
                            add_generation_prompt=True
                        )

                        # ä½¿ç”¨processorå¤„ç†æ–‡æœ¬å’Œå›¾åƒ
                        inputs = processor(
                            text=text,
                            images=[pil_image],
                            return_tensors="pt"
                        )
            else:
                # çº¯æ–‡æœ¬æ¶ˆæ¯æ ¼å¼
                messages = [
                    {
                        "role": "user",
                        "content": question
                    }
                ]

                # åº”ç”¨å¯¹è¯æ¨¡æ¿
                text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )

                # å‡†å¤‡è¾“å…¥
                inputs = processor(
                    text=text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048
                )

            # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
            device = next(model.parameters()).device  # è·å–æ¨¡å‹å‚æ•°æ‰€åœ¨çš„è®¾å¤‡
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # è®¾ç½®ç”Ÿæˆé…ç½®
            model.generation_config.max_new_tokens = config.get('max_tokens', 256)
            model.generation_config.pad_token_id = processor.tokenizer.eos_token_id

            # ç”Ÿæˆé¢„æµ‹
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs
                )

            # è§£ç ç”Ÿæˆç»“æœ
            generated_ids_trimmed = generated_ids[:, inputs['input_ids'].shape[1]:]

            # ç¡®ä¿è§£ç åœ¨CPUä¸Šè¿›è¡Œä»¥é¿å…è®¾å¤‡ä¸åŒ¹é…
            generated_ids_trimmed = generated_ids_trimmed.cpu()
            predicted_answer = processor.tokenizer.decode(
                generated_ids_trimmed[0],
                skip_special_tokens=True
            ).strip()

            # æ£€æŸ¥é¢„æµ‹æ˜¯å¦ä¸æœŸæœ›ç­”æ¡ˆåŒ¹é…
            expected_clean = re.sub(r'[^\w\s]', '', expected_answer.lower().strip())
            predicted_clean = re.sub(r'[^\w\s]', '', predicted_answer.lower().strip())

            # å¤šç§åŒ¹é…ç­–ç•¥
            is_correct = False
            if expected_clean == predicted_clean:
                is_correct = True
            elif expected_clean in predicted_clean or predicted_clean in expected_clean:
                is_correct = True
            else:
                # å…³é”®è¯åŒ¹é…
                expected_words = set(expected_clean.split())
                predicted_words = set(predicted_clean.split())

                if len(expected_words) > 0:
                    common_words = expected_words.intersection(predicted_words)
                    if len(common_words) / len(expected_words) >= 0.8:  # 60%çš„å…³é”®è¯åŒ¹é…
                        is_correct = True

            if is_correct:
                correct_predictions += 1
                status = "âœ“"
            else:
                status = "âœ—"

            total_samples += 1

            predictions.append(predicted_answer)
            references.append(expected_answer)

            # è®°å½•è¯¦ç»†ç»“æœ
            detailed_result = {
                'sample_id': sample_id,
                'question': question,
                'expected_answer': expected_answer,
                'predicted_answer': predicted_answer,
                'is_correct': is_correct,
                'status': status
            }
            detailed_results.append(detailed_result)

            print(f" {status}")

        except Exception as e:
            print(f" é”™è¯¯: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè®°å½•è¯¥æ ·æœ¬çš„ç»“æœ
            detailed_result = {
                'sample_id': f'sample_{i}',
                'question': item.get('question', ''),
                'expected_answer': item.get('answer', ''),
                'predicted_answer': f'ERROR: {str(e)}',
                'is_correct': False,
                'status': 'âœ—'
            }
            detailed_results.append(detailed_result)
            continue

    accuracy = correct_predictions / total_samples if total_samples > 0 else 0

    print(f"\n{model_name} è¯„ä¼°å®Œæˆ")
    print(f"å‡†ç¡®ç‡: {correct_predictions}/{total_samples} ({accuracy:.4f})")

    return accuracy, predictions, references, detailed_results

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°")
    print("="*70)

    # åŠ è½½é…ç½®
    import sys
    config_path = sys.argv[1] if len(sys.argv) > 1 else "./config/evaluation_config.yaml"
    config = load_config(config_path)
    print(f"ä½¿ç”¨è¯„ä¼°é…ç½®: {config_path}")

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(config)

    if len(test_data) == 0:
        print("é”™è¯¯: æµ‹è¯•æ•°æ®ä¸ºç©º")
        return

    print(f"ä½¿ç”¨ {min(config.get('max_test_samples', 60), len(test_data))} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")

    # è¯„ä¼°å¾®è°ƒæ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°å¾®è°ƒæ¨¡å‹ (Qwen3-VL-2B-Instruct-LoRA)...")
    ft_model, ft_processor = prepare_model(config, is_finetuned=True)
    ft_acc, ft_preds, ft_refs, ft_detailed_results = evaluate_model(ft_model, ft_processor, test_data, config, "å¾®è°ƒæ¨¡å‹")

    # è¯„ä¼°åŸºç¡€æ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°åŸºç¡€æ¨¡å‹ (Qwen3-VL-2B-Instruct)...")
    base_model, base_processor = prepare_model(config, is_finetuned=False)
    base_acc, base_preds, base_refs, base_detailed_results = evaluate_model(base_model, base_processor, test_data, config, "åŸºç¡€æ¨¡å‹")

    # è®¡ç®—æ”¹è¿›
    acc_improvement = ft_acc - base_acc
    if base_acc != 0:
        improvement_percentage = (acc_improvement / base_acc) * 100
    else:
        improvement_percentage = float('inf') if acc_improvement > 0 else float('-inf')

    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    print("\n" + "="*70)
    print("å¾®è°ƒæ•ˆæœæ¯”è¾ƒç»“æœ")
    print("="*70)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {min(60, len(test_data))}")
    print()
    print("å¾®è°ƒæ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {ft_acc:.4f}")
    print()
    print("åŸºç¡€æ¨¡å‹:")
    print(f"  å‡†ç¡®ç‡: {base_acc:.4f}")
    print()
    print("æ€§èƒ½æ”¹è¿›:")
    print(f"  ç»å¯¹æ”¹è¿›: {acc_improvement:+.4f}")
    print(f"  ç›¸å¯¹æ”¹è¿›: {improvement_percentage:+.2f}%")

    # åˆ†ææ”¹è¿›æƒ…å†µ
    print()
    if acc_improvement > 0.01:  # è¶…è¿‡1%çš„æ”¹è¿›
        print("ğŸ‰ å¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½!")
    elif acc_improvement > 0:
        print("âœ… å¾®è°ƒå¯¹æ¨¡å‹æœ‰è½»å¾®æ”¹è¿›")
    elif acc_improvement == 0:
        print("â†’ å¾®è°ƒå¯¹æ¨¡å‹æ€§èƒ½æ— å½±å“")
    else:
        print("âš ï¸  å¾®è°ƒåæ¨¡å‹æ€§èƒ½ç•¥æœ‰ä¸‹é™")

    # æ˜¾ç¤ºè¯¦ç»†ç»“æœç¤ºä¾‹
    print("\n" + "-"*70)
    print("ç»“æœå¯¹æ¯”ç¤ºä¾‹ (å‰3ä¸ªæ ·æœ¬):")
    print("-"*70)
    for i in range(min(3, len(test_data))):
        if i < len(base_preds) and i < len(ft_preds):
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜: {test_data[i]['question'][:100]}...")
            print(f"  ç­”æ¡ˆ: {test_data[i]['answer'][:100]}")
            print(f"  å¾®è°ƒ: {ft_preds[i][:100]}")
            print(f"  åŸºç¡€: {base_preds[i][:100]}")
            print()

    # åˆ›å»ºå®Œæ•´çš„å¯¹æ¯”ç»“æœ
    comparison_results = []
    for i in range(len(base_detailed_results)):
        if i < len(ft_detailed_results):
            comparison_entry = {
                'sample_id': base_detailed_results[i]['sample_id'],
                'question': base_detailed_results[i]['question'],
                'expected_answer': base_detailed_results[i]['expected_answer'],
                'ft_prediction': ft_detailed_results[i]['predicted_answer'],
                'ft_is_correct': ft_detailed_results[i]['is_correct'],
                'ft_status': ft_detailed_results[i]['status'],
                'base_prediction': base_detailed_results[i]['predicted_answer'],
                'base_is_correct': base_detailed_results[i]['is_correct'],
                'base_status': base_detailed_results[i]['status'],
                'improved': ft_detailed_results[i]['is_correct'] and not base_detailed_results[i]['is_correct'],
                'regressed': not ft_detailed_results[i]['is_correct'] and base_detailed_results[i]['is_correct']
            }
            comparison_results.append(comparison_entry)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_results = {
        'test_samples_count': min(config.get('max_test_samples', 60), len(test_data)),
        'fine_tuned_model_accuracy': ft_acc,
        'base_model_accuracy': base_acc,
        'absolute_improvement': acc_improvement,
        'relative_improvement_percent': improvement_percentage,
        'timestamp': str(datetime.datetime.now()),
        'test_samples_preview': [
            {
                'question': test_data[i]['question'][:200],
                'expected': test_data[i]['answer'][:200] if i < len(test_data) else '',
                'fine_tuned_prediction': ft_preds[i][:200] if i < len(ft_preds) else '',
                'base_prediction': base_preds[i][:200] if i < len(base_preds) else ''
            }
            for i in range(min(config.get('max_preview_samples', 3), len(test_data)))
        ],
        'detailed_comparison_results': comparison_results,
        'fine_tuned_model_detailed_results': ft_detailed_results,
        'base_model_detailed_results': base_detailed_results
    }

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    result_file = config['results_output_dir'] + "qwen3_vl_finetuning_evaluation.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)

    # ä¿å­˜è¯¦ç»†çš„æ–‡æœ¬æ—¥å¿—
    log_file = config['results_output_dir'] + "fine_tuning_evaluation_final.log"
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è¯¦ç»†æ—¥å¿—\n")
        f.write("="*70 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {datetime.datetime.now()}\n")
        f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {min(config.get('max_test_samples', 60), len(test_data))}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {ft_acc:.4f}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_acc:.4f}\n")
        f.write(f"ç»å¯¹æ”¹è¿›: {acc_improvement:+.4f}\n")
        f.write(f"ç›¸å¯¹æ”¹è¿›: {improvement_percentage:+.2f}%\n")

        f.write("\n" + "="*70 + "\n")
        f.write("è¯¦ç»†å¯¹æ¯”ç»“æœ:\n")
        f.write("="*70 + "\n")

        improved_count = 0
        regressed_count = 0
        same_count = 0

        for i, result in enumerate(comparison_results):
            f.write(f"æ ·æœ¬ {i+1} (ID: {result['sample_id']}):\n")
            f.write(f"  é—®é¢˜: {result['question']}\n")
            f.write(f"  æ ‡å‡†ç­”æ¡ˆ: {result['expected_answer']}\n")
            f.write(f"  å¾®è°ƒæ¨¡å‹é¢„æµ‹: {result['ft_prediction']} [{result['ft_status']}]\n")
            f.write(f"  åŸºç¡€æ¨¡å‹é¢„æµ‹: {result['base_prediction']} [{result['base_status']}]\n")

            if result['improved']:
                f.write(f"  ç»“æœ: âœ… å¾®è°ƒæ¨¡å‹æ”¹è¿›\n")
                improved_count += 1
            elif result['regressed']:
                f.write(f"  ç»“æœ: âŒ å¾®è°ƒæ¨¡å‹é€€æ­¥\n")
                regressed_count += 1
            else:
                f.write(f"  ç»“æœ: â†”ï¸  æ— å˜åŒ–\n")
                same_count += 1

            f.write("\n")

        f.write("="*70 + "\n")
        f.write("ç»Ÿè®¡æ‘˜è¦:\n")
        f.write("="*70 + "\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(comparison_results)}\n")
        f.write(f"å¾®è°ƒæ”¹è¿›æ ·æœ¬æ•°: {improved_count}\n")
        f.write(f"å¾®è°ƒé€€æ­¥æ ·æœ¬æ•°: {regressed_count}\n")
        f.write(f"æ— å˜åŒ–æ ·æœ¬æ•°: {same_count}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {ft_acc:.4f}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_acc:.4f}\n")
        f.write(f"å‡†ç¡®ç‡æå‡: {acc_improvement:+.4f}\n")

    print(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print(f"è¯¦ç»†å¯¹æ¯”æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

    print("\n" + "="*70)
    print("è¯„ä¼°å®Œæˆ!")
    print("="*70)

if __name__ == "__main__":
    main()