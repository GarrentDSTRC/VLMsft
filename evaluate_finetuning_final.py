#!/usr/bin/env python
"""
Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•å¾®è°ƒå‰åæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¯¹æ¯”
"""
import json
import torch
import datetime
import os
from datasets import Dataset
from transformers import Qwen3VLForConditionalGeneration, Qwen3VLProcessor
from peft import PeftModel
from PIL import Image
import base64
from io import BytesIO
import re
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import numpy as np
import yaml

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs('./logs', exist_ok=True)

def load_config(config_path="./config/evaluation_config.yaml"):
    """åŠ è½½è¯„ä¼°é…ç½®æ–‡ä»¶"""
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
    if not os.path.exists(config_path):
        print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        return {
            'test_dataset_path': './vlm_finetune_dataset.json',
            'base_model_path': '/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct',
            'results_output_dir': './logs/',
            'max_test_samples': 60,
            'max_preview_samples': 3,
            'max_tokens': 256,
            'temperature': 0.1
        }

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def load_test_data(config):
    """åŠ è½½æµ‹è¯•æ•°æ®é›† - é€‚é…Llama Factoryæ ¼å¼"""
    print(f"ä» {config['test_dataset_path']} åŠ è½½æµ‹è¯•æ•°æ®...")

    try:
        # å°è¯•åŠ è½½æ–°çš„Llama Factoryæ ¼å¼æ•°æ®
        with open(config['test_dataset_path'], 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    except FileNotFoundError:
        print("è­¦å‘Š: æ‰¾ä¸åˆ°æŒ‡å®šçš„æµ‹è¯•æ•°æ®é›†ï¼Œå°è¯•åŠ è½½action_data.json...")
        try:
            with open("./action_data.json", 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except FileNotFoundError:
            print("é”™è¯¯: æ‰¾ä¸åˆ°action_data.json")
            return []

    # å¤„ç†Llama Factoryæ ¼å¼çš„æ•°æ®
    test_data = []
    for item in raw_data:
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output = item.get('output', {})

        # æ„å»ºé—®é¢˜ - å°†instructionå’Œinputç»„åˆ
        question = instruction + "\n\n" + input_text if input_text else instruction

        # å¤„ç†è¾“å‡º - outputå¯èƒ½æ˜¯å­—å…¸æ ¼å¼
        if isinstance(output, dict):
            if 'reasoning' in output and 'action' in output:
                # æŒ‰ç…§JSONæ ¼å¼æ„å»ºç­”æ¡ˆ
                answer = json.dumps(output, ensure_ascii=False, indent=2)
            else:
                # å¦‚æœåªæœ‰éƒ¨åˆ†å­—æ®µï¼Œå°è¯•æ„å»ºç­”æ¡ˆ
                answer_parts = []
                for key, value in output.items():
                    if isinstance(value, list):
                        answer_parts.append(f"{key}: {str(value)}")
                    else:
                        answer_parts.append(f"{key}: {value}")
                answer = "\n".join(answer_parts)
        else:
            answer = str(output)

        # è·å–å›¾åƒè·¯å¾„
        image_paths = item.get('images', [])

        if question and answer:
            test_data.append({
                'id': item.get('id', f'test_{len(test_data)}'),
                'question': question.strip(),
                'answer': answer.strip(),
                'images': image_paths  # ä¿å­˜å›¾åƒè·¯å¾„ä¿¡æ¯
            })

    print(f"åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return test_data

def load_model_for_evaluation(model_path, adapter_path=None, device="cuda"):
    """
    æ™ºèƒ½åŠ è½½æ¨¡å‹ï¼šè‡ªåŠ¨è¯†åˆ«LoRA/å…¨é‡æ¨¡å‹
    """
    # æ£€æµ‹æ¨¡å‹ç±»å‹
    is_lora = (os.path.exists(os.path.join(model_path, "adapter_model.bin")) or
               os.path.exists(os.path.join(model_path, "adapter_model.safetensors"))) and \
              os.path.exists(os.path.join(model_path, "adapter_config.json"))

    # æ£€æµ‹å…¨é‡å¾®è°ƒæ¨¡å‹
    is_full = (os.path.exists(os.path.join(model_path, "pytorch_model.bin")) or
               os.path.exists(os.path.join(model_path, "model.safetensors"))) and \
              os.path.exists(os.path.join(model_path, "config.json"))

    if is_lora or adapter_path:
        # LoRAæ¨¡å¼ï¼šéœ€æŒ‡å®šåŸºç¡€æ¨¡å‹
        if adapter_path:
            # å¦‚æœæä¾›äº†adapter_pathï¼Œä½¿ç”¨å®ƒä½œä¸ºLoRAè·¯å¾„
            adapter_dir = adapter_path
        else:
            # å¦åˆ™ä½¿ç”¨model_pathä½œä¸ºLoRAè·¯å¾„
            adapter_dir = model_path

        # ä»adapter_config.jsonä¸­è·å–åŸºç¡€æ¨¡å‹åç§°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        adapter_config_path = os.path.join(adapter_dir, "adapter_config.json")
        if os.path.exists(adapter_config_path):
            with open(adapter_config_path, 'r', encoding='utf-8') as f:
                adapter_config = json.load(f)
            base_model = adapter_config.get("base_model_name_or_path", "/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct")
        else:
            base_model = "/root/.cache/modelscope/hub/models/qwen/Qwen3-VL-2B-Instruct"

        print(f"â†’ æ£€æµ‹åˆ°LoRAæ¨¡å‹ï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")

        model = Qwen3VLForConditionalGeneration.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )

        # åŠ è½½é€‚é…å™¨
        from peft import PeftModel
        model = PeftModel.from_pretrained(model, adapter_dir, is_trainable=False)
        print(f"âœ“ LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ: {adapter_dir}")

    elif is_full:
        # å…¨é‡æ¨¡å‹ï¼šç›´æ¥åŠ è½½
        print(f"â†’ åŠ è½½å…¨é‡å¾®è°ƒæ¨¡å‹: {model_path}")
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )
    else:
        # å¦‚æœéƒ½ä¸æ˜¯ï¼Œå°è¯•ä½œä¸ºåŸºç¡€æ¨¡å‹åŠ è½½
        print(f"â†’ å°è¯•ä½œä¸ºåŸºç¡€æ¨¡å‹åŠ è½½: {model_path}")
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )

    model.eval()
    # ç¡®å®šprocessorçš„è·¯å¾„ - å¦‚æœæ˜¯LoRAï¼Œå¯èƒ½éœ€è¦ä»åŸºç¡€æ¨¡å‹è·¯å¾„åŠ è½½
    if is_lora and not os.path.exists(os.path.join(model_path, "tokenizer_config.json")):
        # å¦‚æœæ˜¯LoRAä¸”ç›®æ ‡è·¯å¾„ç¼ºå°‘tokenizeré…ç½®ï¼Œä»åŸºç¡€æ¨¡å‹åŠ è½½
        processor = Qwen3VLProcessor.from_pretrained(
            base_model if 'base_model' in locals() else model_path,
            trust_remote_code=True
        )
    else:
        # å¦åˆ™ä»ç›®æ ‡è·¯å¾„åŠ è½½
        processor = Qwen3VLProcessor.from_pretrained(
            model_path,
            trust_remote_code=True
        )

    return model, processor


def prepare_model(config, is_finetuned=False):
    """å‡†å¤‡æ¨¡å‹ - åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹"""
    model_name = config['base_model_path']

    # ç¡®å®šä½¿ç”¨çš„è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        # ä½¿ç”¨ç‰¹å®šçš„GPUè®¾å¤‡è€Œä¸æ˜¯autoï¼Œä»¥é¿å…å¤šGPUè®¾å¤‡ä¸åŒ¹é…é—®é¢˜
        device = torch.device('cuda:0')  # å¼ºåˆ¶ä½¿ç”¨ç¬¬ä¸€å—GPU

    if is_finetuned:
        print("å‡†å¤‡å¾®è°ƒåçš„æ¨¡å‹...")

        # ä»é…ç½®æ–‡ä»¶è·å–æ¨¡å‹æœç´¢è·¯å¾„
        model_paths = config.get('finetuned_model_paths', [
            "./qwen3-vl-2b-instruct-full",          # å…¨é‡å¾®è°ƒæ¨¡å‹è·¯å¾„
            "./qwen3-vl-2b-instruct-lora",          # LoRAå•å¡æ¨¡å‹è·¯å¾„
            "./qwen3-vl-2b-instruct-lora-multigpu", # LoRAå¤šå¡æ¨¡å‹è·¯å¾„
            "./logs/qwen3-vl-2b-instruct-lora",      # LoRAå•å¡æ¨¡å‹è·¯å¾„(æ—¥å¿—)
            "./logs/qwen3-vl-2b-instruct-lora-multigpu"  # LoRAå¤šå¡æ¨¡å‹è·¯å¾„(æ—¥å¿—)
        ])

        loaded_model = False
        model = None
        processor = None

        for model_path in model_paths:
            if os.path.exists(model_path):
                print(f"å‘ç°æ¨¡å‹è·¯å¾„: {model_path}")

                try:
                    # é¦–å…ˆå°è¯•ç›´æ¥åŠ è½½
                    model, processor = load_model_for_evaluation(model_path, device=str(device))
                    loaded_model = True
                    print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
                    break
                except Exception as e:
                    print(f"ç›´æ¥åŠ è½½æ¨¡å‹å¤±è´¥ {model_path}: {e}")

                    # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•æœç´¢å­ç›®å½•ä¸­çš„æ£€æŸ¥ç‚¹
                    if os.path.isdir(model_path):
                        subdirs = [d for d in os.listdir(model_path)
                                  if os.path.isdir(os.path.join(model_path, d))]

                        # æŒ‰åç§°æ’åºï¼Œä¼˜å…ˆå°è¯•checkpoint-*ç›®å½•
                        subdirs_sorted = sorted(subdirs,
                                              key=lambda x: (not x.startswith('checkpoint-'), x))

                        for subdir in subdirs_sorted:
                            subdir_path = os.path.join(model_path, subdir)
                            print(f"å°è¯•å­ç›®å½•: {subdir_path}")

                            try:
                                model, processor = load_model_for_evaluation(subdir_path, device=str(device))
                                loaded_model = True
                                print(f"æˆåŠŸä»å­ç›®å½•åŠ è½½æ¨¡å‹: {subdir_path}")
                                break
                            except Exception as sub_e:
                                print(f"å­ç›®å½•åŠ è½½å¤±è´¥ {subdir_path}: {sub_e}")
                                continue

                    if loaded_model:
                        break

        if not loaded_model:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¾®è°ƒæ¨¡å‹è·¯å¾„")
            # å¦‚æœæ²¡æ‰¾åˆ°å¾®è°ƒæ¨¡å‹ï¼Œè¿”å›åŸºç¡€æ¨¡å‹
            model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                device_map=device,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            processor = Qwen3VLProcessor.from_pretrained(
                model_name,
                trust_remote_code=True
            )
    else:
        print("å‡†å¤‡åŸºç¡€æ¨¡å‹...")
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map=device,  # ä½¿ç”¨æŒ‡å®šçš„å•ä¸€è®¾å¤‡
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        processor = Qwen3VLProcessor.from_pretrained(
            model_name,
            trust_remote_code=True
        )

    return model, processor

def evaluate_model(model, processor, test_data, config, model_name="æ¨¡å‹", log_details=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nå¼€å§‹è¯„ä¼° {model_name}...")

    total_samples = 0
    predictions = []
    references = []
    detailed_results = []  # å­˜å‚¨è¯¦ç»†ç»“æœ
    direction_predictions = []  # å­˜å‚¨æ–¹å‘é¢„æµ‹ç»“æœ
    distance_errors = []  # å­˜å‚¨è·ç¦»è¯¯å·®

    # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´
    test_samples = test_data[:min(config.get('max_test_samples', 60), len(test_data))]

    for i, item in enumerate(test_samples):
        print(f"å¤„ç†æµ‹è¯•æ ·æœ¬ {i+1}/{len(test_samples)}", end='', flush=True)

        try:
            question = item.get('question', '')
            expected_answer = item.get('answer', '')
            sample_id = item.get('id', f'sample_{i}')
            image_paths = item.get('images', [])

            if not question or not expected_answer:
                print(" (è·³è¿‡ - ç¼ºå°‘é—®é¢˜æˆ–ç­”æ¡ˆ)")
                continue

            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒä¿¡æ¯
            import base64
            from PIL import Image
            import io

            if image_paths and len(image_paths) > 0:
                # å¤„ç†å¤šä¸ªå›¾åƒï¼ˆå¦‚æµ‹è¯•æ•°æ®ä¸­çš„åŒå›¾åƒåœºæ™¯ï¼‰
                pil_images = []

                for img_path in image_paths:
                    # æ£€æŸ¥å›¾åƒè·¯å¾„æ˜¯æœ¬åœ°æ–‡ä»¶è¿˜æ˜¯base64
                    if img_path.startswith('data:image'):
                        # å¤„ç†base64å›¾åƒ
                        base64_str = img_path.split(',')[1]
                        image_bytes = base64.b64decode(base64_str)
                        pil_image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
                    else:
                        # å¤„ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
                        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨å‡ ä¸ªå¯èƒ½çš„ä½ç½®æŸ¥æ‰¾
                        import os
                        possible_paths = [
                            img_path,  # åŸå§‹è·¯å¾„
                            os.path.join(os.path.dirname(config['test_dataset_path']), img_path),  # ç›¸å¯¹äºæ•°æ®é›†æ–‡ä»¶
                            os.path.join('.', img_path),  # ç›¸å¯¹å½“å‰ç›®å½•
                            os.path.join('..', img_path),  # ä¸Šçº§ç›®å½•
                        ]

                        pil_image = None
                        for path_option in possible_paths:
                            if os.path.exists(path_option):
                                pil_image = Image.open(path_option).convert('RGB')
                                print(f"æˆåŠŸåŠ è½½å›¾åƒ: {path_option}")
                                break

                        if pil_image is None:
                            print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°å›¾åƒæ–‡ä»¶ {img_path}")
                            # å¦‚æœæ‰¾ä¸åˆ°å›¾åƒï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                            continue

                    pil_images.append(pil_image)

                # æ„å»ºåŒ…å«å¤šä¸ªå›¾åƒçš„æ¶ˆæ¯
                content_list = []
                for _ in pil_images:
                    content_list.append({"type": "image"})
                content_list.append({"type": "text", "text": question})

                messages = [
                    {
                        "role": "user",
                        "content": content_list
                    }
                ]

                # åº”ç”¨å¯¹è¯æ¨¡æ¿
                text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )

                # ä½¿ç”¨processorå¤„ç†æ–‡æœ¬å’Œå¤šä¸ªå›¾åƒ
                inputs = processor(
                    text=text,
                    images=pil_images,
                    return_tensors="pt"
                )
            else:
                # çº¯æ–‡æœ¬æ¶ˆæ¯æ ¼å¼
                messages = [
                    {
                        "role": "user",
                        "content": question
                    }
                ]

                # åº”ç”¨å¯¹è¯æ¨¡æ¿
                text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )

                # å‡†å¤‡è¾“å…¥
                inputs = processor(
                    text=text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048
                )

            # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
            device = next(model.parameters()).device  # è·å–æ¨¡å‹å‚æ•°æ‰€åœ¨çš„è®¾å¤‡
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # è®¾ç½®ç”Ÿæˆé…ç½®
            model.generation_config.max_new_tokens = config.get('max_tokens', 256)
            model.generation_config.pad_token_id = processor.tokenizer.eos_token_id

            # ç”Ÿæˆé¢„æµ‹
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs
                )

            # è§£ç ç”Ÿæˆç»“æœ
            generated_ids_trimmed = generated_ids[:, inputs['input_ids'].shape[1]:]
            # ç¡®ä¿è§£ç åœ¨CPUä¸Šè¿›è¡Œä»¥é¿å…è®¾å¤‡ä¸åŒ¹é…
            generated_ids_trimmed = generated_ids_trimmed.cpu()
            predicted_answer = processor.tokenizer.decode(
                generated_ids_trimmed[0],
                skip_special_tokens=True
            ).strip()
            print("predictanswer",predicted_answer)
            # å°è¯•è§£æJSONæ ¼å¼çš„é¢„æµ‹ç»“æœ
            direction_correct = False
            distance_error = 0
            try:
                # å°è¯•æå–JSON
                json_str = None
                patterns = [
                    r'\{[^{}]*"direction"[^{}]*"distance"[^{}]*\}',
                    r'\{.*?\}',
                    predicted_answer.strip()
                ]

                for pattern in patterns:
                    if pattern.startswith('{'):
                        json_str = pattern
                        break
                    match = re.search(pattern, predicted_answer, re.DOTALL)
                    if match:
                        json_str = match.group()
                        break

                if json_str:
                    pred_result = json.loads(json_str)
                    pred_direction = str(pred_result.get("direction", "-")).strip()
                    pred_distance = abs(int(pred_result.get("distance", 0)))  # å–ç»å¯¹å€¼

                    # è§£ææœŸæœ›ç­”æ¡ˆ
                    expected_result = json.loads(expected_answer)
                    exp_direction = str(expected_result.get("direction", "-")).strip()
                    exp_distance = abs(int(expected_result.get("distance", 0)))  # å–ç»å¯¹å€¼

                    # æ£€æŸ¥æ–¹å‘é¢„æµ‹æ˜¯å¦æ­£ç¡®
                    direction_correct = (pred_direction == exp_direction)

                    # è®¡ç®—è·ç¦»è¯¯å·®
                    distance_error = abs(pred_distance - exp_distance)

            except (json.JSONDecodeError, ValueError, TypeError):
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œæ ‡è®°ä¸ºé”™è¯¯
                direction_correct = False
                distance_error = float('inf')  # ä½¿ç”¨æ— ç©·å¤§è¡¨ç¤ºæ— æ³•è®¡ç®—çš„è¯¯å·®

            total_samples += 1

            predictions.append(predicted_answer)
            references.append(expected_answer)
            direction_predictions.append(direction_correct)
            distance_errors.append(distance_error)

            # è®°å½•è¯¦ç»†ç»“æœ
            detailed_result = {
                'sample_id': sample_id,
                'question': question,
                'expected_answer': expected_answer,
                'predicted_answer': predicted_answer,
                'direction_correct': direction_correct,
                'distance_error': distance_error,
                'status': 'âœ“' if direction_correct else 'âœ—'
            }
            detailed_results.append(detailed_result)

            print(f" {detailed_result['status']}")

        except Exception as e:
            print(f" é”™è¯¯: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè®°å½•è¯¥æ ·æœ¬çš„ç»“æœ
            detailed_result = {
                'sample_id': f'sample_{i}',
                'question': item.get('question', ''),
                'expected_answer': item.get('answer', ''),
                'predicted_answer': f'ERROR: {str(e)}',
                'direction_correct': False,
                'distance_error': float('inf'),
                'status': 'âœ—'
            }
            detailed_results.append(detailed_result)
            direction_predictions.append(False)
            distance_errors.append(float('inf'))
            continue

    # è®¡ç®—æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡
    direction_accuracy = sum(1 for x in direction_predictions if x) / total_samples if total_samples > 0 else 0

    # è®¡ç®—å¹³å‡è·ç¦»è¯¯å·®ï¼ˆæ’é™¤æ— æ³•è®¡ç®—çš„æ ·æœ¬ï¼‰
    valid_distance_errors = [err for err in distance_errors if err != float('inf')]
    avg_distance_error = sum(valid_distance_errors) / len(valid_distance_errors) if valid_distance_errors else float('inf')

    print(f"\n{model_name} è¯„ä¼°å®Œæˆ")
    print(f"æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: {sum(1 for x in direction_predictions if x)}/{total_samples} ({direction_accuracy:.4f})")
    print(f"å¹³å‡è·ç¦»è¯¯å·®: {avg_distance_error:.4f}")

    return direction_accuracy, avg_distance_error, predictions, references, detailed_results

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°")
    print("="*70)

    # åŠ è½½é…ç½®
    import sys
    config_path = sys.argv[1] if len(sys.argv) > 1 else "./config/evaluation_config.yaml"
    config = load_config(config_path)
    print(f"ä½¿ç”¨è¯„ä¼°é…ç½®: {config_path}")

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(config)

    if len(test_data) == 0:
        print("é”™è¯¯: æµ‹è¯•æ•°æ®ä¸ºç©º")
        return

    print(f"ä½¿ç”¨ {min(config.get('max_test_samples', 60), len(test_data))} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")

    # è¯„ä¼°å¾®è°ƒæ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°å¾®è°ƒæ¨¡å‹ (Qwen3-VL-2B-Instruct-LoRA)...")
    ft_model, ft_processor = prepare_model(config, is_finetuned=True)
    ft_dir_acc, ft_avg_dist_err, ft_preds, ft_refs, ft_detailed_results = evaluate_model(ft_model, ft_processor, test_data, config, "å¾®è°ƒæ¨¡å‹")

    # è¯„ä¼°åŸºç¡€æ¨¡å‹
    print("\n" + "-"*60)
    print("è¯„ä¼°åŸºç¡€æ¨¡å‹ (Qwen3-VL-2B-Instruct)...")
    base_model, base_processor = prepare_model(config, is_finetuned=False)
    base_dir_acc, base_avg_dist_err, base_preds, base_refs, base_detailed_results = evaluate_model(base_model, base_processor, test_data, config, "åŸºç¡€æ¨¡å‹")

    # è®¡ç®—æ”¹è¿›
    dir_acc_improvement = ft_dir_acc - base_dir_acc
    dist_err_improvement = ft_avg_dist_err - base_avg_dist_err  # è´Ÿå€¼è¡¨ç¤ºæ”¹è¿›

    if base_dir_acc != 0:
        dir_improvement_percentage = (dir_acc_improvement / base_dir_acc) * 100
    else:
        dir_improvement_percentage = float('inf') if dir_acc_improvement > 0 else float('-inf')

    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    print("\n" + "="*70)
    print("å¾®è°ƒæ•ˆæœæ¯”è¾ƒç»“æœ")
    print("="*70)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {min(60, len(test_data))}")
    print()
    print("å¾®è°ƒæ¨¡å‹:")
    print(f"  æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: {ft_dir_acc:.4f}")
    print(f"  å¹³å‡è·ç¦»è¯¯å·®: {ft_avg_dist_err:.4f}")
    print()
    print("åŸºç¡€æ¨¡å‹:")
    print(f"  æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: {base_dir_acc:.4f}")
    print(f"  å¹³å‡è·ç¦»è¯¯å·®: {base_avg_dist_err:.4f}")
    print()
    print("æ€§èƒ½æ”¹è¿›:")
    print(f"  æ–¹å‘å‡†ç¡®ç‡æ”¹è¿›: {dir_acc_improvement:+.4f}")
    print(f"  æ–¹å‘å‡†ç¡®ç‡ç›¸å¯¹æ”¹è¿›: {dir_improvement_percentage:+.2f}%")
    print(f"  è·ç¦»è¯¯å·®æ”¹è¿›: {dist_err_improvement:+.4f}")

    # åˆ†ææ”¹è¿›æƒ…å†µ
    print()
    if dir_acc_improvement > 0.01:  # è¶…è¿‡1%çš„æ”¹è¿›
        print("ğŸ‰ å¾®è°ƒæ˜¾è‘—æå‡äº†æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡!")
    elif dir_acc_improvement > 0:
        print("âœ… å¾®è°ƒå¯¹æ–¹å‘é¢„æµ‹æœ‰è½»å¾®æ”¹è¿›")
    elif dir_acc_improvement == 0:
        print("â†’ å¾®è°ƒå¯¹æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡æ— å½±å“")
    else:
        print("âš ï¸  å¾®è°ƒåæ–¹å‘é¢„æµ‹å‡†ç¡®ç‡ç•¥æœ‰ä¸‹é™")

    if dist_err_improvement < -0.1:  # è·ç¦»è¯¯å·®å‡å°‘è¶…è¿‡0.1
        print("âœ… å¾®è°ƒæ˜¾è‘—é™ä½äº†è·ç¦»é¢„æµ‹è¯¯å·®!")
    elif dist_err_improvement < 0:
        print("âœ… å¾®è°ƒç•¥å¾®é™ä½äº†è·ç¦»é¢„æµ‹è¯¯å·®")
    elif dist_err_improvement == 0:
        print("â†’ å¾®è°ƒå¯¹è·ç¦»é¢„æµ‹è¯¯å·®æ— å½±å“")
    else:
        print("âš ï¸  å¾®è°ƒåè·ç¦»é¢„æµ‹è¯¯å·®æœ‰æ‰€å¢åŠ ")

    # æ˜¾ç¤ºè¯¦ç»†ç»“æœç¤ºä¾‹
    print("\n" + "-"*70)
    print("ç»“æœå¯¹æ¯”ç¤ºä¾‹ (å‰3ä¸ªæ ·æœ¬):")
    print("-"*70)
    for i in range(min(3, len(test_data))):
        if i < len(base_preds) and i < len(ft_preds):
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜: {test_data[i]['question'][:100]}...")
            print(f"  ç­”æ¡ˆ: {test_data[i]['answer'][:100]}")
            print(f"  å¾®è°ƒ: {ft_preds[i][:100]}")
            print(f"  åŸºç¡€: {base_preds[i][:100]}")
            print()

    # åˆ›å»ºå®Œæ•´çš„å¯¹æ¯”ç»“æœ
    comparison_results = []
    for i in range(len(base_detailed_results)):
        if i < len(ft_detailed_results):
            comparison_entry = {
                'sample_id': base_detailed_results[i]['sample_id'],
                'question': base_detailed_results[i]['question'],
                'expected_answer': base_detailed_results[i]['expected_answer'],
                'ft_prediction': ft_detailed_results[i]['predicted_answer'],
                'ft_direction_correct': ft_detailed_results[i]['direction_correct'],
                'ft_distance_error': ft_detailed_results[i]['distance_error'],
                'ft_status': ft_detailed_results[i]['status'],
                'base_prediction': base_detailed_results[i]['predicted_answer'],
                'base_direction_correct': base_detailed_results[i]['direction_correct'],
                'base_distance_error': base_detailed_results[i]['distance_error'],
                'base_status': base_detailed_results[i]['status'],
                'dir_improved': ft_detailed_results[i]['direction_correct'] and not base_detailed_results[i]['direction_correct'],
                'dir_regressed': not ft_detailed_results[i]['direction_correct'] and base_detailed_results[i]['direction_correct'],
                'dist_improved': ft_detailed_results[i]['distance_error'] < base_detailed_results[i]['distance_error'],
                'dist_regressed': ft_detailed_results[i]['distance_error'] > base_detailed_results[i]['distance_error']
            }
            comparison_results.append(comparison_entry)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_results = {
        'test_samples_count': min(config.get('max_test_samples', 60), len(test_data)),
        'fine_tuned_model_direction_accuracy': ft_dir_acc,
        'fine_tuned_model_avg_distance_error': ft_avg_dist_err,
        'base_model_direction_accuracy': base_dir_acc,
        'base_model_avg_distance_error': base_avg_dist_err,
        'direction_accuracy_improvement': dir_acc_improvement,
        'distance_error_improvement': dist_err_improvement,
        'direction_relative_improvement_percent': dir_improvement_percentage,
        'timestamp': str(datetime.datetime.now()),
        'test_samples_preview': [
            {
                'question': test_data[i]['question'][:200],
                'expected': test_data[i]['answer'][:200] if i < len(test_data) else '',
                'fine_tuned_prediction': ft_preds[i][:200] if i < len(ft_preds) else '',
                'base_prediction': base_preds[i][:200] if i < len(base_preds) else ''
            }
            for i in range(min(config.get('max_preview_samples', 3), len(test_data)))
        ],
        'detailed_comparison_results': comparison_results,
        'fine_tuned_model_detailed_results': ft_detailed_results,
        'base_model_detailed_results': base_detailed_results
    }

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    result_file = config['results_output_dir'] + "qwen3_vl_finetuning_evaluation.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)

    # ä¿å­˜è¯¦ç»†çš„æ–‡æœ¬æ—¥å¿—
    log_file = config['results_output_dir'] + "fine_tuning_evaluation_final.log"
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("Qwen3-VL-2B-Instruct æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°è¯¦ç»†æ—¥å¿—\n")
        f.write("="*70 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {datetime.datetime.now()}\n")
        f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {min(config.get('max_test_samples', 60), len(test_data))}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹æ–¹å‘å‡†ç¡®ç‡: {ft_dir_acc:.4f}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å¹³å‡è·ç¦»è¯¯å·®: {ft_avg_dist_err:.4f}\n")
        f.write(f"åŸºç¡€æ¨¡å‹æ–¹å‘å‡†ç¡®ç‡: {base_dir_acc:.4f}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å¹³å‡è·ç¦»è¯¯å·®: {base_avg_dist_err:.4f}\n")
        f.write(f"æ–¹å‘å‡†ç¡®ç‡æ”¹è¿›: {dir_acc_improvement:+.4f}\n")
        f.write(f"è·ç¦»è¯¯å·®æ”¹è¿›: {dist_err_improvement:+.4f}\n")

        f.write("\n" + "="*70 + "\n")
        f.write("è¯¦ç»†å¯¹æ¯”ç»“æœ:\n")
        f.write("="*70 + "\n")

        dir_improved_count = 0
        dir_regressed_count = 0
        dist_improved_count = 0
        dist_regressed_count = 0

        for i, result in enumerate(comparison_results):
            f.write(f"æ ·æœ¬ {i+1} (ID: {result['sample_id']}):\n")
            f.write(f"  é—®é¢˜: {result['question']}\n")
            f.write(f"  æ ‡å‡†ç­”æ¡ˆ: {result['expected_answer']}\n")
            f.write(f"  å¾®è°ƒæ¨¡å‹é¢„æµ‹: {result['ft_prediction']} [æ–¹å‘{'âœ“' if result['ft_direction_correct'] else 'âœ—'}, è·ç¦»è¯¯å·®: {result['ft_distance_error']:.2f}]\n")
            f.write(f"  åŸºç¡€æ¨¡å‹é¢„æµ‹: {result['base_prediction']} [æ–¹å‘{'âœ“' if result['base_direction_correct'] else 'âœ—'}, è·ç¦»è¯¯å·®: {result['base_distance_error']:.2f}]\n")

            if result['dir_improved']:
                f.write(f"  æ–¹å‘é¢„æµ‹: âœ… å¾®è°ƒæ¨¡å‹æ”¹è¿›\n")
                dir_improved_count += 1
            elif result['dir_regressed']:
                f.write(f"  æ–¹å‘é¢„æµ‹: âŒ å¾®è°ƒæ¨¡å‹é€€æ­¥\n")
                dir_regressed_count += 1
            else:
                f.write(f"  æ–¹å‘é¢„æµ‹: â†”ï¸  æ— å˜åŒ–\n")

            if result['dist_improved']:
                f.write(f"  è·ç¦»é¢„æµ‹: âœ… å¾®è°ƒæ¨¡å‹æ”¹è¿› (è¯¯å·®å‡å°‘ {result['base_distance_error'] - result['ft_distance_error']:.2f})\n")
                dist_improved_count += 1
            elif result['dist_regressed']:
                f.write(f"  è·ç¦»é¢„æµ‹: âŒ å¾®è°ƒæ¨¡å‹é€€æ­¥ (è¯¯å·®å¢åŠ  {result['ft_distance_error'] - result['base_distance_error']:.2f})\n")
                dist_regressed_count += 1
            else:
                f.write(f"  è·ç¦»é¢„æµ‹: â†”ï¸  æ— å˜åŒ–\n")

            f.write("\n")

        f.write("="*70 + "\n")
        f.write("ç»Ÿè®¡æ‘˜è¦:\n")
        f.write("="*70 + "\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(comparison_results)}\n")
        f.write(f"æ–¹å‘é¢„æµ‹æ”¹è¿›æ ·æœ¬æ•°: {dir_improved_count}\n")
        f.write(f"æ–¹å‘é¢„æµ‹é€€æ­¥æ ·æœ¬æ•°: {dir_regressed_count}\n")
        f.write(f"è·ç¦»é¢„æµ‹æ”¹è¿›æ ·æœ¬æ•°: {dist_improved_count}\n")
        f.write(f"è·ç¦»é¢„æµ‹é€€æ­¥æ ·æœ¬æ•°: {dist_regressed_count}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹æ–¹å‘å‡†ç¡®ç‡: {ft_dir_acc:.4f}\n")
        f.write(f"åŸºç¡€æ¨¡å‹æ–¹å‘å‡†ç¡®ç‡: {base_dir_acc:.4f}\n")
        f.write(f"æ–¹å‘å‡†ç¡®ç‡æå‡: {dir_acc_improvement:+.4f}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å¹³å‡è·ç¦»è¯¯å·®: {ft_avg_dist_err:.4f}\n")
        f.write(f"åŸºç¡€æ¨¡å‹å¹³å‡è·ç¦»è¯¯å·®: {base_avg_dist_err:.4f}\n")
        f.write(f"è·ç¦»è¯¯å·®æ”¹è¿›: {dist_err_improvement:+.4f}\n")

    print(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print(f"è¯¦ç»†å¯¹æ¯”æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

    print("\n" + "="*70)
    print("è¯„ä¼°å®Œæˆ!")
    print("="*70)

if __name__ == "__main__":
    main()