[
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向前移动，向中心那株高大的深色珊瑚丛探索，注意保持与底部珊瑚的距离。",
    "output": {
      "reasoning": "目标是中心位置的高大深色珊瑚丛。当前视野中，该目标位于正前方。底部布满了各种绿色珊瑚，为了避免碰撞并保持安全距离，机器人需要先微调高度，然后缓慢向前推进以接近目标进行探索。",
      "action": [
        "Ascend slightly",
        "Go straight slowly",
        "Approach the dark coral",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0001.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方那个巨大的深色柱状物靠近，注意避开两侧的珊瑚丛。",
    "output": {
      "reasoning": "图像中心偏右位置有一个巨大的深色柱状物，这是目标物体。两侧分布着不同颜色的珊瑚丛（左侧为深绿色，右侧为浅黄色分支状）。为了安全靠近目标，机器人需要保持在中央航道，匀速前进，并根据目标位置微调方向，同时确保与两侧珊瑚保持安全距离。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Slightly adjust heading to the right",
        "Keep distance from corals on both sides",
        "Approach the dark columnar object",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0002.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方那个漂浮的探测器靠近，注意避开左侧的尖刺状珊瑚，保持匀速前进。",
    "output": {
      "reasoning": "漂浮探测器位于图像左上方，左侧中景处有明显的尖刺状珊瑚。为了安全接近探测器，机器人需要先向左前方调整航向，在绕过珊瑚障碍的同时保持匀速前进，最后接近目标。",
      "action": [
        "Turn slightly left",
        "Move forward at steady speed",
        "Adjust heading to avoid spiky coral on the left",
        "Approach the floating detector",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0003.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过这些圆柱状物体，向光亮处探索，保持匀速并注意避开底部的障碍物。",
    "output": {
      "reasoning": "当前视野中有多个绿色的圆柱状结构分布在前方，上方有明亮的光源。为了执行指令，机器人需要对准圆柱体之间的缝隙前进，同时适当提升高度以向光亮处移动并避开底部的障碍物，保持平稳的航行速度。",
      "action": [
        "Go straight",
        "Navigate through the gap between cylinders",
        "Ascend slightly towards the light",
        "Maintain constant speed",
        "Monitor and avoid bottom obstacles"
      ]
    },
    "images": [
      "./data/images/0004.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速靠近中央的石柱，注意避开地面的碎石。",
    "output": {
      "reasoning": "中央石柱位于视野正前方，路径地面散落着碎石。机器人需要保持匀速直线前进，同时在行进过程中注意避开地面的障碍物，最终靠近目标石柱。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Avoid ground debris",
        "Approach the central pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0005.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速航行，绕过左侧的绿色沉车，向右前方的石柱靠近并进行近距离探测。",
    "output": {
      "reasoning": "图像中左侧有一辆明显的绿色沉车，右前方背景中矗立着一根石柱。根据指令，机器人需要先向前航行，在保持安全距离的情况下绕过左侧的沉车，随后调整航向指向右前方的石柱，最终靠近石柱进行探测任务。",
      "action": [
        "Go straight",
        "Turn right slightly to bypass the car",
        "Go straight towards the stone pillar",
        "Approach the stone pillar",
        "Stop and perform detection"
      ]
    },
    "images": [
      "./data/images/0006.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，注意避开右侧高耸的珊瑚礁，向深处探索。",
    "output": {
      "reasoning": "当前视野中，左侧有一艘潜艇正在航行，右侧有一座较高且显眼的白色珊瑚礁。前方海域视线开阔并逐渐变深。为了执行指令，机器人需要保持匀速航行，并与右侧的珊瑚礁保持安全距离，持续向深水区推进。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Keep distance from the right coral reef",
        "Move forward into deep water"
      ]
    },
    "images": [
      "./data/images/0007.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前穿过前方的洞口，注意避开底部的岩石，保持匀速前进。",
    "output": {
      "reasoning": "图像显示前方有一个宽阔的水下洞口，阳光从上方射入。底部散布着大小不一的岩石。为了安全穿过，机器人需要调整高度以避开底部的障碍物，并对准洞口中心保持匀速直线航行。",
      "action": [
        "Adjust depth to clear bottom rocks",
        "Go straight towards the center of the opening",
        "Maintain steady speed",
        "Pass through the opening"
      ]
    },
    "images": [
      "./data/images/0008.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近前方的绿色舱体并观察其窗口，同时注意右侧悬挂的红色浮标。",
    "output": {
      "reasoning": "图像显示绿色舱体位于前方，其侧面分布有多个观察窗。红色浮标悬挂在右侧较远位置。为了执行指令，机器人需要保持匀速前进，并向右微调方向以对准舱体窗口，同时在航行路径中避开右侧的浮标障碍。",
      "action": [
        "Go straight",
        "Slightly turn right",
        "Approach the green cabin windows",
        "Maintain distance from the red buoy",
        "Stop and observe"
      ]
    },
    "images": [
      "./data/images/0009.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向前靠近橙色圆柱体开口处进行详细探测，注意保持与底部的安全距离。",
    "output": {
      "reasoning": "图像中橙色圆柱体位于前方，其开口正对右前方。为了执行详细探测任务，机器人需要先调整航向对准开口，随后在保持离底高度的情况下缓慢接近，最后在开口处悬停。",
      "action": [
        "Turn slightly right to align with the opening",
        "Maintain altitude above the seabed",
        "Move forward slowly",
        "Approach the cylinder opening",
        "Stop and hover"
      ]
    },
    "images": [
      "./data/images/0010.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "继续向前匀速航行，穿过前方的珊瑚群，注意观察上方和右侧的岩石障碍。",
    "output": {
      "reasoning": "机器人当前位于珊瑚礁上方，上方和右侧存在大型岩石结构。为了安全穿过珊瑚群，机器人需要保持稳定的航向和深度，避开上方和右侧的障碍物，同时确保不触碰下方的珊瑚。",
      "action": [
        "Maintain depth",
        "Go straight",
        "Monitor right side obstacles",
        "Monitor top obstacles",
        "Pass through coral area"
      ]
    },
    "images": [
      "./data/images/0011.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前并向右侧移动，绕过前方的珊瑚礁，靠近观察那个大型圆柱体目标，注意保持匀速并与障碍物保持安全距离。",
    "output": {
      "reasoning": "图像中前方和左侧分布有珊瑚礁，大型圆柱体目标位于右前方。机器人需要先向前移动，然后向右转向以避开左侧的珊瑚礁群，随后直线靠近圆柱体目标，并在安全距离内停止以进行观察。",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight",
        "Approach the cylindrical target",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0012.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿着光束照亮的中心区域向前探索，注意观察两侧岩壁的距离。",
    "output": {
      "reasoning": "图像显示一个由两侧岩壁构成的狭窄水下通道，中心区域有上方射入的明亮光束。为了执行指令，机器人需要锁定光束所在的中心路径，保持匀速直线行驶，并时刻感知左右两侧岩壁的边界以维持安全间距。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Stay centered in the light beam",
        "Monitor distance to side walls",
        "Explore forward"
      ]
    },
    "images": [
      "./data/images/0013.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从两块大礁石之间穿过，向前方垂下的标记物靠近。",
    "output": {
      "reasoning": "图像显示左右两侧各有一块巨大的礁石，中间形成了一个明显的通道。前方上方有一个垂下的白色标记物。为了执行指令，机器人需要调整航向对准两礁石间的缝隙，保持稳定速度穿过，然后向标记物靠拢。",
      "action": [
        "Align with the gap between the reefs",
        "Go straight at constant speed",
        "Pass between the two large reefs",
        "Turn slightly right towards the hanging marker",
        "Approach the marker",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0014.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，缓慢向前靠近中心的发光结构，注意避开左侧和上方的粗大线缆。",
    "output": {
      "reasoning": "图像中心有一个明显的发光结构作为目标。左侧和上方分布着粗大的黄蓝相间的线缆，构成了主要的障碍区域。为了安全靠近目标，机器人需要保持在中心路径，缓慢向前推进，同时实时监控与左侧及上方线缆的距离，确保不发生碰撞。",
      "action": [
        "Move forward slowly",
        "Maintain central alignment",
        "Keep distance from left cables",
        "Keep distance from top cables",
        "Approach the glowing structure",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0015.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着蓝色波纹管道向上方水面移动，注意避开左侧的黄色缆绳网，保持匀速前进。",
    "output": {
      "reasoning": "图像中心有一条明显的蓝色波纹管道延伸至水面，左侧分布着大面积的黄色缆绳网。为了安全到达水面，机器人需要锁定蓝色管道作为路径参考，并向右侧微调以远离左侧的缆绳网，随后沿着管道斜向上方匀速推进。",
      "action": [
        "Align with the blue corrugated pipe",
        "Shift slightly to the right to avoid the yellow net",
        "Move forward and upward along the pipe",
        "Maintain steady speed",
        "Ascend towards the surface"
      ]
    },
    "images": [
      "./data/images/0016.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢移动，靠近黄色圆圈标记的海底区域进行探测，注意保持匀速并避开周围的珊瑚礁。",
    "output": {
      "reasoning": "目标探测区域位于图像右前方的海底，由黄色圆圈标出。周围分布有珊瑚礁，需要规划一条向右前方的路径并避开障碍。机器人需要先向右调整航向，然后缓慢匀速前进，最终停在目标区域上方进行探测。",
      "action": [
        "Turn right",
        "Go straight slowly",
        "Keep distance from corals",
        "Approach the yellow circle",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0017.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前方的光源处航行，注意避开两侧的珊瑚礁群。",
    "output": {
      "reasoning": "图像显示正上方有强烈的光源，而左右两侧分布着密集的珊瑚礁群。机器人目前处于两者之间的开阔水域。为了执行指令，机器人需要保持稳定的速度，沿着中心路径向光源方向航行，同时实时监测与两侧珊瑚礁的距离以确保安全避障。",
      "action": [
        "Maintain steady speed",
        "Go straight",
        "Keep center between reefs",
        "Approach light source",
        "Monitor obstacles"
      ]
    },
    "images": [
      "./data/images/0018.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "绕过正前方的大石柱，向右侧后方那群较小的遗迹建筑群匀速前进，注意观察底部沙地的起伏。",
    "output": {
      "reasoning": "正前方有一个巨大的石柱阻挡路径，目标遗迹群位于石柱右后方。需要先向右侧移动绕过石柱，然后调整航向朝向遗迹群，并保持匀速前进，同时监控沙地高度以防碰撞。",
      "action": [
        "Turn right",
        "Go straight to bypass the pillar",
        "Turn right towards the ruins",
        "Move forward steadily",
        "Monitor seabed depth",
        "Approach the ruins"
      ]
    },
    "images": [
      "./data/images/0019.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "绕过左侧沉没的汽车，向右前方那个圆柱形建筑靠近，注意保持匀速并观察底部沙地的变化。",
    "output": {
      "reasoning": "图像左侧有一辆沉没的汽车作为主要障碍物，目标圆柱形建筑位于右前方。规划路径为先向右侧转向以避开左侧汽车，随后直线向目标建筑靠近，同时在移动过程中保持恒定速度并监测底部沙地的纹理变化。",
      "action": [
        "Turn right",
        "Go straight",
        "Maintain constant speed",
        "Monitor seabed changes",
        "Approach the cylindrical building",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0020.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前穿过面前的珊瑚与岩石缝隙，保持匀速并注意近处的地形起伏。",
    "output": {
      "reasoning": "图像显示正前方有一个由左侧珊瑚和右侧岩石壁构成的狭窄缝隙。光线从上方射入，路径清晰。为了安全穿过，机器人需要保持在缝隙中央，并根据底部地形起伏微调高度，同时保持匀速前进。",
      "action": [
        "Go straight",
        "Stay centered",
        "Adjust depth for terrain",
        "Maintain speed",
        "Pass through"
      ]
    },
    "images": [
      "./data/images/0021.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前穿过面前的岩石通道，朝着远处的亮光处前进，注意避开底部的礁石并保持匀速。",
    "output": {
      "reasoning": "当前环境为一个半封闭的水下岩石通道，正前方远处有明显的亮光出口。路径底部散布着不规则的礁石。为了安全完成指令，机器人需要保持航向对准亮光中心，同时适当控制高度以避开底部的礁石障碍，并维持稳定的推进速度。",
      "action": [
        "Go straight",
        "Maintain altitude to avoid bottom reefs",
        "Head towards the light source",
        "Maintain constant speed",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0022.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从右侧绕过面前的大型红色浮标，向深处那个较小的目标靠近，注意观察底部沙地的起伏。",
    "output": {
      "reasoning": "图像中心偏左位置有一个大型红色浮标，遮挡了前进路径。根据指令，需要从右侧绕过该障碍物。远方右侧可见一个较小的同类目标。机器人需要先向右转向避开大浮标，随后保持匀速向深处目标行进，同时需实时监测下方沙地的起伏以维持安全高度。",
      "action": [
        "Maintain constant speed",
        "Turn right",
        "Go straight to bypass the large buoy",
        "Turn left slightly to align with the distant target",
        "Go straight towards the small target",
        "Monitor sandy bottom terrain",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0023.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方悬挂的橙色网状结构靠近进行观察，注意保持匀速并与缆绳保持安全距离。",
    "output": {
      "reasoning": "图像中心显示一个悬挂的橙色网状篮筐结构，上方有连接缆绳。目标位于正前方。为了安全观察，机器人需要缓慢直线前进，同时避开上方的缆绳路径，在合适距离停下。",
      "action": [
        "Go straight",
        "Maintain steady speed",
        "Keep distance from cables",
        "Stop near target"
      ]
    },
    "images": [
      "./data/images/0024.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前方发光的绿色石柱靠近，并注意避开左侧的石壁。",
    "output": {
      "reasoning": "发光的绿色石柱位于前方中心偏右位置，左侧有大型石壁结构。为了安全靠近目标并避开左侧障碍，机器人需要保持匀速前进，并适度向右修正航向以拉开与左侧石壁的距离。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Slightly turn right to avoid left wall",
        "Approach the green pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0025.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，穿过左侧珊瑚群与右侧石柱之间的通道，注意观察石柱表面的纹理特征。",
    "output": {
      "reasoning": "图像中心显示出一条清晰的通道，左侧是大型珊瑚结构，右侧是一根具有独特纹理的石柱。为了执行指令，机器人需要对准通道中心，保持稳定速度向前航行，并在经过石柱时保持观察视角以记录其表面纹理。",
      "action": [
        "Align with the channel center",
        "Go straight at constant speed",
        "Pass between the left coral and right pillar",
        "Observe the texture of the stone pillar",
        "Maintain forward navigation"
      ]
    },
    "images": [
      "./data/images/0026.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近并检查底部的绿色圆柱形设备，注意保持匀速并避开右侧的岩石区域。",
    "output": {
      "reasoning": "图像前景底部有一个显著的绿色圆柱形设备，即为目标检查物。右侧区域分布着多块黑色孔洞状岩石，构成障碍。为了安全靠近目标，机器人需要保持平稳速度向前推进，同时在路径上稍微偏左以确保与右侧岩石保持安全距离，最后在设备上方悬停进行检查。",
      "action": [
        "Go straight",
        "Slightly steer left to avoid right-side rocks",
        "Maintain steady speed",
        "Approach the green cylindrical device",
        "Stop and hover for inspection"
      ]
    },
    "images": [
      "./data/images/0027.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前行驶，绕过右侧巨大的珊瑚礁，向远处发光的绿色目标靠近，注意保持平稳速度。",
    "output": {
      "reasoning": "图像显示右侧有一个巨大的多孔珊瑚礁，远处左前方有一个发绿光的目标。为了到达目标并避开右侧障碍，机器人需要先向前行驶，然后向左微调以绕过珊瑚礁，最后直行接近发光目标。",
      "action": [
        "Go straight",
        "Turn slightly left to bypass the reef",
        "Go straight towards the green light",
        "Approach target",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0028.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，避开左侧的大型珊瑚礁，向中央的石柱靠近并进行环绕探测。",
    "output": {
      "reasoning": "图像中心有一个显著的石柱目标，左侧底部有大型珊瑚礁。机器人需要保持匀速前进，并向右微调以避开左侧障碍物，随后接近石柱并执行环绕动作。",
      "action": [
        "Go straight",
        "Slightly turn right",
        "Approach the central pillar",
        "Circle the pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0029.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近正前方的黑色石柱，注意避开底部的珊瑚礁，保持平稳航行。",
    "output": {
      "reasoning": "黑色石柱位于正前方，底部周围分布有明显的珊瑚礁。为了安全靠近，机器人需要保持稳定的航行高度以避开底部的障碍物，然后直线向前航行，并在接近石柱时减速停止。",
      "action": [
        "Go straight",
        "Maintain safe altitude",
        "Approach the pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0030.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近面前巨大的球形珊瑚并观察其内部，注意保持匀速，避免发生碰撞。",
    "output": {
      "reasoning": "视野正前方是一个巨大的球形珊瑚，其中心位置有一个明显的孔洞。为了执行观察内部的任务，机器人需要保持匀速直线前进，逐渐靠近该孔洞，并确保在发生碰撞前停止。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Approach the opening",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0031.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向左前方缓慢移动，注意避开正前方巨大的珊瑚礁结构。",
    "output": {
      "reasoning": "图像显示正前方有一个巨大的珊瑚礁结构，为了避开它并向左前方移动，机器人需要先向左转弯以调整航向，避开障碍物路径，然后保持匀速直线行驶。左侧区域相对开阔，适合安全通行。",
      "action": [
        "Turn left",
        "Go straight",
        "Maintain constant speed",
        "Keep distance from reef"
      ]
    },
    "images": [
      "./data/images/0032.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢移动，绕过左侧的绿色大型设备，注意观察底部的光影变化。",
    "output": {
      "reasoning": "图像左侧占据了大量的绿色大型设备，为了绕过它并向右前方移动，机器人需要先向右调整航向，避开左侧障碍物，随后缓慢前进。同时，底部光影交错，需要保持低速以进行细致观察。",
      "action": [
        "Turn right",
        "Move forward slowly",
        "Keep distance from green equipment",
        "Observe seabed light patterns",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0033.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着右侧的管道结构匀速向前探索，注意避开左侧的礁石区域。",
    "output": {
      "reasoning": "图像显示右侧有一条明显的管道结构，左侧是崎岖的礁石。为了执行指令，机器人需要保持在管道左侧并与其平行，同时确保与左侧礁石保持安全距离，匀速向前推进。",
      "action": [
        "Align with the pipe",
        "Maintain distance from left reef",
        "Go straight at constant speed",
        "Monitor surroundings"
      ]
    },
    "images": [
      "./data/images/0034.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前飞行，穿过前方的缆绳区域，注意避开左侧缠绕的黄色线缆。",
    "output": {
      "reasoning": "当前视野中，左侧下方有明显的黄色缠绕线缆，前方有横向的网状结构。为了安全穿过，需要保持稳定的前进速度，并微调航向偏向右侧，以避开左侧的障碍物，确保在缆绳区域中心穿行。",
      "action": [
        "Maintain constant speed",
        "Slightly turn right",
        "Go straight",
        "Pass through the cable area",
        "Keep monitoring left obstacles"
      ]
    },
    "images": [
      "./data/images/0035.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢移动，穿过缆绳之间的空隙，注意避开左侧的网状障碍物，保持匀速航行。",
    "output": {
      "reasoning": "图像显示左侧存在明显的网状障碍物，右侧有连接潜水器的缆绳。为了向右前方移动并穿过空隙，机器人需要先向右微调航向，避开左侧网状物，然后缓慢匀速地从缆绳之间的空间穿过。",
      "action": [
        "Turn right",
        "Go straight slowly",
        "Navigate through the gap between cables",
        "Maintain constant speed"
      ]
    },
    "images": [
      "./data/images/0036.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，抵近观察前方的青色垂直结构，注意避开底部的线缆并留意左侧潜水员的位置。",
    "output": {
      "reasoning": "图像中心是一个显著的青色垂直结构，这是主要的观察目标。底部海床上布有线缆，机器人需要保持一定高度以防挂蹭。左侧有一名潜水员，在前进过程中需持续监测其位置以确保安全距离。路径前方无明显障碍，将采取匀速直线前进并最终抵近观察的策略。",
      "action": [
        "Maintain altitude to avoid cables",
        "Go straight at constant speed",
        "Monitor diver on the left",
        "Slow down",
        "Approach cyan structure",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0037.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从蓝色管道和右侧潜航器之间向前穿过，注意避开左侧的石柱。",
    "output": {
      "reasoning": "图像显示蓝色管道位于左侧，大型潜航器位于右侧，左侧边缘有石柱。目标路径位于管道和潜航器之间的缝隙。机器人需要调整航向对准该间隙，保持匀速前进，并确保与左侧石柱保持安全距离。",
      "action": [
        "Adjust heading to the gap",
        "Maintain constant speed",
        "Go straight through the gap",
        "Keep away from the left pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0038.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右前方绕过中心的大型绿色环状珊瑚，注意避开近处的紫色斑点珊瑚。",
    "output": {
      "reasoning": "图像中心是一个大型绿色环状珊瑚，右下角近处有一个紫色斑点珊瑚。为了实现向右前方绕过中心目标并避开近处障碍，机器人需要保持匀速，先向右微调航向以避开右下方的紫色珊瑚，随后向右前方推进以绕过中心的大型珊瑚。",
      "action": [
        "Maintain constant speed",
        "Turn slightly right",
        "Go straight to clear the purple spotted coral",
        "Adjust heading to the right front",
        "Bypass the green ring coral",
        "Continue forward"
      ]
    },
    "images": [
      "./data/images/0039.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方的大型珊瑚靠近，保持匀速并注意观察其表面的细节。",
    "output": {
      "reasoning": "大型珊瑚位于机器人正前方，视野清晰且无明显障碍物。为了观察细节，机器人需要匀速直线前进，并在接近目标时减速以进行近距离观察。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Approach the coral",
        "Slow down",
        "Stop and observe"
      ]
    },
    "images": [
      "./data/images/0040.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过紫色珊瑚和右侧石柱之间的空隙，保持匀速并注意观察周围环境。",
    "output": {
      "reasoning": "图像中，前方中心偏左位置有一个紫色的圆形珊瑚，右侧是一个高大的深色石柱。两者之间存在明显的通行空间。机器人需要调整航向对准该空隙，随后保持稳定速度直线穿过，并在移动过程中持续监测周围环境。",
      "action": [
        "Align with the gap",
        "Go straight",
        "Maintain constant speed",
        "Monitor surroundings",
        "Pass through the gap"
      ]
    },
    "images": [
      "./data/images/0041.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方那个红白相间的浮标靠近，保持匀速，注意避开两侧的大型岩石。",
    "output": {
      "reasoning": "目标红白相间的浮标位于正前方海底，两侧分布有大型岩石。路径相对开阔，机器人需要保持在中央航道匀速前进，以避开两侧障碍物并接近目标。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Keep center between rocks",
        "Approach the buoy",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0042.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方的大石柱缓慢靠近，保持匀速，同时注意观察光影下的地面障碍物。",
    "output": {
      "reasoning": "图像中心偏右位置有一根巨大的石柱，光线从上方射入形成强烈的光影效果。地面散落着一些小型石块障碍物。为了安全靠近石柱，需要先微调方向对准目标，然后以恒定低速前进，并持续监测地面障碍物以防碰撞。",
      "action": [
        "Turn slightly right to align with the pillar",
        "Move forward at a slow and constant speed",
        "Monitor ground obstacles in the light and shadow",
        "Adjust path to avoid small rocks",
        "Approach the base of the pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0043.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前靠近中央的巨大柱子，保持匀速并注意观察光影下的环境变化。",
    "output": {
      "reasoning": "中央巨大的柱子位于视野正前方，路径清晰且光影效果明显。机器人需要保持当前航向，匀速向前推进以接近目标，同时监测周围环境。",
      "action": [
        "Go straight",
        "Maintain steady speed",
        "Approach the central pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0044.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近大型条纹潜航器进行近距离观察，保持匀速并注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "大型条纹潜航器位于正前方，占据了视野的主要部分。底部有明显的珊瑚礁分布。为了安全地进行近距离观察，机器人需要保持当前深度以避开底部的障碍物，并稳步向前推进直到接近目标。",
      "action": [
        "Go straight",
        "Maintain depth",
        "Approach the submersible",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0045.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近正前方的深色立柱，注意避开两侧的珊瑚礁。",
    "output": {
      "reasoning": "正前方有一个深色立柱，左右两侧分布着珊瑚礁。路径中央相对开阔，机器人需要保持当前航向，匀速向前移动以靠近立柱，同时通过保持居中来避开两侧的珊瑚。",
      "action": [
        "Maintain heading",
        "Go straight at constant speed",
        "Approach the dark pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0046.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿着岩壁间的狭窄通道继续向前航行，注意避开两侧的突出物并跟随前方的潜水员。",
    "output": {
      "reasoning": "当前机器人处于两面岩壁构成的狭窄通道中，前方有两名潜水员正在行进。光线从上方射入，路径清晰但空间受限。需要保持航向居中以避开两侧岩壁的突出部分，并维持恒定速度跟随前方潜水员。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Adjust to stay centered",
        "Follow the divers",
        "Avoid side protrusions"
      ]
    },
    "images": [
      "./data/images/0047.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着左侧的弧形结构和右侧的珊瑚群之间的通道稳步前行，注意保持匀速并观察前方深处的环境变化。",
    "output": {
      "reasoning": "图像显示左侧有一个巨大的弧形人工结构，右侧分布着若干珊瑚礁石，中间形成了一条清晰的沙质通道。为了执行指令，机器人需要对准通道中心，保持稳定速度向前推进，并持续监测前方深水区域的视觉变化。",
      "action": [
        "Align to the center of the channel",
        "Go straight steadily",
        "Maintain constant speed",
        "Monitor the environment ahead"
      ]
    },
    "images": [
      "./data/images/0048.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速靠近前方的蓝色柱状结构，注意观察周围光影变化并避开两侧的珊瑚礁。",
    "output": {
      "reasoning": "蓝色柱状结构位于视野正前方，左右两侧分布有珊瑚礁。为了安全靠近目标，机器人需要保持直线行驶，确保与两侧珊瑚礁保持安全距离，并根据光影变化调整观察策略。",
      "action": [
        "Go straight at constant speed",
        "Maintain center path between corals",
        "Approach the blue pillar",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0049.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过珊瑚群，保持匀速并注意避开左侧巨大的扇形珊瑚。",
    "output": {
      "reasoning": "图像显示左侧有一个巨大的扇形珊瑚，前方是珊瑚群之间的通道。为了安全穿过并避开左侧障碍物，机器人需要先微调方向向右，然后保持匀速直线前进。",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Maintain constant speed",
        "Go straight through the gap",
        "Monitor left obstacle"
      ]
    },
    "images": [
      "./data/images/0050.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方绕过底部的珊瑚群，向那艘大型潜航器靠近，注意保持匀速并观察周边环境。",
    "output": {
      "reasoning": "大型潜航器位于图像中心偏右位置，底部前景有密集的珊瑚群作为障碍物。为了安全靠近目标，机器人需要先向右前方调整航向以绕过珊瑚群，随后直线接近潜航器，并在过程中保持匀速和环境监测。",
      "action": [
        "Turn right",
        "Go straight to bypass the coral groups",
        "Maintain constant speed",
        "Approach the large submersible",
        "Observe surroundings",
        "Stop near target"
      ]
    },
    "images": [
      "./data/images/0051.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着当前轨迹继续向斜上方浮动，穿过前方的深水区，保持匀速并注意避开右侧的岩石结构。",
    "output": {
      "reasoning": "机器人当前正处于斜向上浮的轨迹中，前方是开阔的深水区。右侧可见明显的岩石结构，需要保持当前航向并略微偏左以确保安全间距，同时维持恒定动力输出以实现匀速浮升。",
      "action": [
        "Maintain current diagonal upward trajectory",
        "Keep constant speed",
        "Monitor distance to the rock structure on the right",
        "Slightly adjust heading to the left for safety clearance",
        "Continue floating upwards through the deep water area"
      ]
    },
    "images": [
      "./data/images/0052.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方青色皮卡车靠近，保持匀速并注意避开左侧的巨大石柱。",
    "output": {
      "reasoning": "图像中青色皮卡车位于前方中右侧，左侧有一根巨大的石柱。为了安全靠近目标，机器人需要先微调航向以确保与左侧石柱保持安全距离，随后匀速向前推进，最终停在皮卡车旁。",
      "action": [
        "Adjust heading slightly right",
        "Go straight at steady speed",
        "Keep distance from left pillar",
        "Approach the pickup truck",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0053.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从右侧石柱与左侧圆形遗迹之间的空隙穿过，注意观察沉船附近的细节。",
    "output": {
      "reasoning": "图像显示左侧有一个圆形遗迹，右侧有一个高大的石柱，两者之间存在明显的间隙。间隙后方可见一处沉船（或沉没车辆）遗迹。为了执行指令，机器人需要保持航向，从间隙中穿过，并逐渐靠近沉船以进行细节观察。",
      "action": [
        "Go straight",
        "Adjust heading to center",
        "Pass through the gap",
        "Approach the shipwreck",
        "Slow down for observation",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0054.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，从前方两块大型礁石中间穿过，注意避开左侧突出的珊瑚结构。",
    "output": {
      "reasoning": "图像显示前方左右两侧各有一块大型礁石，中间形成了一个可通行的间隙。左侧近处有明显的珊瑚突出物。为了安全穿过，机器人需要保持匀速，先向右侧微调以避开左侧的珊瑚结构，随后修正航向对准礁石间的缝隙并直行穿过。",
      "action": [
        "Go straight",
        "Turn right to avoid left coral",
        "Go straight",
        "Turn left to align with the gap",
        "Go straight through the gap"
      ]
    },
    "images": [
      "./data/images/0055.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前靠近中心的大型结构物，注意避开两侧的礁石。",
    "output": {
      "reasoning": "图像中心有一个大型潜艇状结构物，两侧分布着黑色礁石。为了安全靠近目标，需要保持航向居中，匀速直线前进，并时刻留意与两侧礁石的距离以防碰撞。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Keep center path",
        "Approach the central structure",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0056.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，注意避开左侧的大型珊瑚结构，向前方光线充足的开阔水域探索。",
    "output": {
      "reasoning": "图像左侧有大型珊瑚结构，需保持安全距离。前方及右侧区域光线充足且水域开阔，是理想的探索方向。机器人应在保持匀速的同时，微调航向向右避开障碍，随后向光亮处直线航行。",
      "action": [
        "Maintain constant speed",
        "Steer slightly right to avoid left coral",
        "Go straight towards the light",
        "Explore open water"
      ]
    },
    "images": [
      "./data/images/0057.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方橙色结构靠近，并尝试对准其绿色开口进行抵近观察，注意保持姿态平稳。",
    "output": {
      "reasoning": "橙色结构位于机器人正前方，其绿色的开口清晰可见。为了执行抵近观察，机器人需要直线前进，并在接近时微调姿态以对准开口中心，同时控制速度以保持平稳。",
      "action": [
        "Go straight",
        "Align with green opening",
        "Slowly approach",
        "Hover and observe"
      ]
    },
    "images": [
      "./data/images/0061.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速行驶，穿过带有红色浮标的条纹结构和远处黑色潜水器之间的空隙，注意观察底部沙地的变化。",
    "output": {
      "reasoning": "图像左侧可见带有红色浮标的条纹结构，右侧远处有黑色潜水器。目标路径位于两者之间的空隙。机器人需要保持匀速前进，对准空隙穿过，并实时监测底部沙地的纹理和孔洞变化。",
      "action": [
        "Go straight",
        "Adjust heading towards the gap",
        "Maintain constant speed",
        "Monitor seabed changes",
        "Pass through the gap",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0062.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左侧绕过面前的大型珊瑚结构，同时保持匀速并注意观察背景中的发光目标。",
    "output": {
      "reasoning": "图像中心是一个巨大的圆形珊瑚结构，阻挡了前进路径。左侧背景中有一个明显的发光目标。根据指令，机器人需要向左转向以避开珊瑚，保持匀速前进绕过障碍物，并持续观察背景中的发光点。",
      "action": [
        "Turn left",
        "Go straight",
        "Maintain constant speed",
        "Track glowing target",
        "Bypass coral"
      ]
    },
    "images": [
      "./data/images/0065.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从右侧大型珊瑚和左侧珊瑚群之间穿过去，注意观察底部地形变化。",
    "output": {
      "reasoning": "图像右侧有一个巨大的伞状珊瑚，左侧下方有一组较小的珊瑚群，两者之间存在明显的通道。指令要求从中间穿过并观察底部，因此需要微调航向对准间隙，保持匀速前进，并向下俯视观察地形。",
      "action": [
        "Adjust heading to the gap",
        "Go straight at constant speed",
        "Tilt camera down to observe terrain",
        "Maintain course between corals",
        "Continue forward"
      ]
    },
    "images": [
      "./data/images/0066.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速航行，注意避开右侧的大型珊瑚礁，并保持对底部散落目标的持续观察。",
    "output": {
      "reasoning": "当前视野中，右侧存在一个显著的大型珊瑚礁，底部散落着多个带斑点的水生生物（目标）。为了安全执行指令，机器人需要保持向前的动力，同时向左微调方向以远离右侧障碍物，并维持较低的航行高度或视角以确保持续观察底部的散落目标。",
      "action": [
        "Go straight",
        "Slightly turn left to avoid the reef on the right",
        "Maintain constant speed",
        "Keep observing the bottom targets",
        "Continue forward"
      ]
    },
    "images": [
      "./data/images/0067.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着右侧的黄色管线向前匀速航行，注意避开底部的珊瑚礁并观察左侧潜航器的动态。",
    "output": {
      "reasoning": "右侧黄色管线清晰可见，可作为导航参照。底部有散落的珊瑚礁，需保持安全高度以防碰撞。左侧有一台潜航器正在并行或停留，需在前进过程中持续观察其动态。路径前方开阔，适合匀速航行。",
      "action": [
        "Align with the yellow pipeline",
        "Maintain altitude to avoid corals",
        "Go straight at constant speed",
        "Monitor the submersible on the left",
        "Follow the pipeline"
      ]
    },
    "images": [
      "./data/images/0068.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green striped cylindrical object on the seabed while maintaining a safe distance from the coral formations on either side.",
    "output": {
      "reasoning": "The large green striped cylindrical object lies horizontally on the seabed, centered in the image. Coral formations are visible on both left and right sides—left side has a dense cluster near the front-left, right side has branching corals extending toward the object’s midsection. To approach safely, the robot should first align centrally relative to the object while avoiding lateral drift into corals. Given the robot’s current position (above and slightly ahead of the object, as inferred from perspective and bubble trail), the optimal path is: (1) descend gently to seabed level to reduce vertical clearance risk, (2) move forward while maintaining central alignment, (3) adjust slight left/right as needed to keep equal distance from both coral clusters. No immediate turn is required since the object is directly ahead; however, minor course corrections may be needed during forward motion.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward steadily toward the green striped cylinder",
        "Maintain central alignment between left and right coral formations",
        "If drifting left, correct slightly right; if drifting right, correct slightly left"
      ]
    },
    "images": [
      "./data/images/temp_90_0045_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central dark rock with yellow bands and inspect its top surface for embedded sensors or markings.",
    "output": {
      "reasoning": "From the image, the central dark rock with yellow bands is clearly visible in the foreground, slightly left of center. It has a distinct layered structure and yellow banding, matching the description. The robot appears to be positioned at a moderate distance (~1–2 m) and slightly elevated (due to water surface reflection and lighting). To approach it for inspection of its top surface, the robot should first move forward toward the rock while maintaining alignment. Once near, a slight upward tilt or gentle ascent may be needed to position the camera directly above the top surface for optimal inspection. No immediate obstacles block the direct path. Therefore, the plan is: (1) advance straight toward the rock, (2) fine-tune position to center over it, (3) ascend slightly to view the top surface.",
      "action": [
        "Go straight toward the central dark rock with yellow bands",
        "Slow down and center position over the rock",
        "Gently ascend to inspect the top surface"
      ]
    },
    "images": [
      "./data/images/0080.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge cluster on the seabed while maintaining a safe distance from the large coral structure above.",
    "output": {
      "reasoning": "From the image, the yellow sponge cluster is visible on the seabed in the lower-right foreground, near white-tipped coral formations. A large, dark coral structure looms above and slightly behind the robot’s current position (center-left to upper-middle of frame), posing a collision risk if the robot ascends or drifts upward. To safely approach the sponge cluster: (1) The robot must first descend slightly to increase vertical clearance from the overhead coral; (2) Then move forward and slightly right toward the sponge cluster, staying low near the seabed; (3) Avoid any upward pitch or ascent during transit. The current heading is roughly aligned with the target but requires lateral correction to the right and controlled descent.",
      "action": [
        "Descend slightly",
        "Turn right 15 degrees",
        "Go straight slowly",
        "Maintain depth while approaching sponge cluster"
      ]
    },
    "images": [
      "./data/images/temp_90_0011_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine positioned at the cave entrance, maintaining a safe distance from surrounding rocks and coral formations.",
    "output": {
      "reasoning": "The submarine is centered at the cave entrance, illuminated by light rays from above. It is flanked by rocks and coral on both sides and slightly ahead. To approach safely: (1) The robot must first align centrally to avoid left/right obstacles; (2) Then advance straight while monitoring proximity to side formations; (3) Stop before entering the narrowest part of the cave to maintain safe distance. The current view shows the submarine directly ahead, so no lateral repositioning is needed initially—only controlled forward motion with awareness of surrounding terrain.",
      "action": [
        "Go straight slowly",
        "Maintain central alignment with the submarine",
        "Stop when 2 meters away from the submarine"
      ]
    },
    "images": [
      "./data/images/temp_90_0056_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-tipped cylindrical pipe while avoiding the overhead net structure and the nearby submersible vehicle.",
    "output": {
      "reasoning": "From the image, the yellow-tipped cylindrical pipe is in the lower-right foreground, extending diagonally upward toward the center. An overhead net structure spans the upper portion of the scene, and a submersible vehicle is positioned to the left of the pipe, near the center. To approach the pipe safely: (1) The robot must avoid ascending into the net—so maintain depth below it; (2) The submersible is on the left, so approach from the right side of the pipe or slightly behind to avoid collision; (3) The pipe’s yellow tip is visible ahead and slightly to the right, indicating the target direction. Therefore, the safest path is to first move right to clear the submersible, then proceed forward while staying low under the net, and finally adjust to align with the pipe’s axis toward the yellow tip.",
      "action": [
        "Turn right slightly",
        "Go straight while maintaining depth below the net",
        "Approach the yellow-tipped pipe from its right side",
        "Align with the pipe and move forward toward the yellow tip"
      ]
    },
    "images": [
      "./data/images/temp_90_0066_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral formation from the front while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The image shows a large, prominent green coral formation centered in the frame, directly ahead of the robot's current position. The coral is well-lit and occupies most of the mid-to-near field, indicating it is relatively close. To approach it from the front while maintaining a safe distance, the robot should proceed straight but at a controlled, cautious speed—avoiding any lateral movement that might risk contact. Given the clear frontal alignment and absence of immediate obstacles in the direct path, a single 'Go straight' action is sufficient for this step; subsequent steps (not required here) would involve stopping at a predetermined safe distance (e.g., 1–2 meters) based on sensor feedback.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_90_0040_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green SUV and inspect its front right wheel area for signs of entanglement or damage.",
    "output": {
      "reasoning": "The green SUV is visible in the center-left of the image, resting on the seabed. Its front right wheel is partially visible from the current viewpoint (robot is slightly ahead and to the left of the vehicle). To inspect the front right wheel area, I need to: (1) move closer to the vehicle while maintaining a safe distance to avoid disturbing sediment; (2) reposition to get a clear view of the front right wheel—likely requiring a slight rightward translation and/or rotation to align with that side; (3) stabilize and hover for inspection. The seabed is uneven with debris, so careful forward motion is needed. No immediate obstacles block the path to the vehicle’s front right side.",
      "action": [
        "Go straight slowly toward the green SUV",
        "Turn slightly right to align with the front right wheel",
        "Move forward gently to get closer to the front right wheel area",
        "Hover steadily for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0006_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green spherical object with circular indentations on the seabed and inspect its surface texture.",
    "output": {
      "reasoning": "The image shows a green spherical object with circular indentations on the seabed, located centrally in the mid-ground. The robot (with glowing green eyes) is positioned to the left and slightly behind it. To approach and inspect its surface texture, the robot must first move forward toward the object, then possibly adjust laterally to align closely for detailed inspection. Given the relatively clear path and no immediate obstacles between the robot and the target, the plan is: (1) advance straight toward the green sphere, (2) fine-tune position to get closer (e.g., slight right turn if needed to center), (3) move very close for surface inspection without contact.",
      "action": [
        "Go straight toward the green spherical object",
        "Slightly turn right to align with the object's center",
        "Move forward slowly to get within close inspection distance",
        "Hover steadily to inspect surface texture"
      ]
    },
    "images": [
      "./data/images/temp_90_0028_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible device suspended among the vertical pipes and inspect its lower attachment point.",
    "output": {
      "reasoning": "The green cylindrical submersible device is visible in the center-right of the image, suspended among vertical pipes. It is positioned slightly above the seafloor and surrounded by rock-like structures. The lower attachment point—likely the orange conical component hanging beneath the cylinder—is partially visible but not yet clearly inspectable from the current viewpoint. To inspect it closely, the robot must first approach the device while avoiding nearby obstacles (e.g., left-side rock formation and foreground boulders). Given the device’s position relative to the robot (appears front-right), the optimal path is: (1) move forward cautiously to reduce distance, (2) adjust rightward to align with the device’s central axis, (3) descend slightly to bring the lower attachment into clear view, and (4) stabilize for inspection. No immediate red obstacles are present; primary constraints are spatial clearance and orientation.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Descend gently",
        "Hover and inspect the lower attachment point"
      ]
    },
    "images": [
      "./data/images/0085.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater instrument suspended vertically near the coral reef and inspect its lower sensor housing.",
    "output": {
      "reasoning": "The red cylindrical instrument is clearly visible, suspended vertically in the center of the frame, with its lower sensor housing (black conical section) facing the camera. The robot appears to be already aligned with the instrument’s axis and at a moderate distance (~2–3 m). To inspect the lower sensor housing closely, the robot should first close the distance while maintaining alignment, then position itself slightly below or directly in front for optimal visual inspection. No immediate obstacles block the path; coral reefs are on both sides but not obstructing the central approach. Since the instrument is stationary and vertically oriented, a straight-forward approach followed by a slight downward adjustment (if needed) will suffice.",
      "action": [
        "Go straight toward the red cylindrical instrument",
        "Slow down as you near the lower sensor housing",
        "Position yourself directly in front of the lower sensor housing for inspection"
      ]
    },
    "images": [
      "./data/images/0078.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue corrugated pipe lying on the seabed and inspect its surface for damage or marine growth, while avoiding the net draped over it.",
    "output": {
      "reasoning": "The image shows a large blue corrugated pipe lying horizontally on the seabed, partially covered by a net draped over its upper surface. The robot is currently positioned at a distance and slightly to the left of the pipe (from the viewer’s perspective), with sunlight filtering from above creating shadows and highlights. To inspect the pipe’s surface for damage or marine growth while avoiding the net, the robot should: (1) approach from the side (not from above or through the net), ideally along the exposed lower or lateral surface; (2) maintain safe clearance from the net, which hangs loosely and may shift; (3) move parallel to the pipe to enable systematic visual inspection. Given the pipe extends diagonally from lower-left to upper-right in the frame, the optimal path is to first move forward toward the near end of the pipe, then align laterally to its right side (unobstructed by net), and proceed alongside it.",
      "action": [
        "Go straight toward the near end of the blue pipe",
        "Turn right to align parallel to the pipe’s right side",
        "Move slowly alongside the pipe while maintaining 0.5 m clearance",
        "Inspect the pipe surface visually for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_90_0065_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the left while maintaining a safe distance from the large rock formation on the right.",
    "output": {
      "reasoning": "The yellow sponge is visible on the left side of the image, near the bottom-left corner. A large dark rock formation occupies the right side of the scene, extending vertically. The robot is currently centered in the frame, facing forward. To approach the sponge while avoiding the rock, I need to: (1) gently steer left to align toward the sponge, (2) maintain forward motion at a safe distance from the right-side rock (i.e., avoid drifting right), and (3) possibly adjust depth or fine-tune heading as it gets closer. Since the sponge is relatively close and low, a slight left turn followed by straight advance should suffice without risking collision with the rock on the right.",
      "action": [
        "Turn slightly left",
        "Go straight",
        "Maintain slight leftward heading to stay clear of right rock"
      ]
    },
    "images": [
      "./data/images/0077.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, ribbed green structure in the center while avoiding the overgrown column on the left.",
    "output": {
      "reasoning": "The image shows two prominent vertical structures: on the left, an overgrown, irregularly textured column (to be avoided), and in the center, a tall, ribbed green structure with clear vertical ridges (the target). The robot is positioned slightly in front and to the left of the target, with the obstructive column occupying the left visual field. To approach the central structure safely, the robot must first shift rightward to clear the left-side obstacle, then proceed straight toward the target. Given the underwater lighting and visibility, small incremental movements are safer. Therefore, the plan is: (1) turn slightly right to align away from the overgrown column, (2) move forward cautiously toward the central structure, maintaining clearance from the left obstacle.",
      "action": [
        "Turn right slightly",
        "Go straight toward the ribbed green structure"
      ]
    },
    "images": [
      "./data/images/0063.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, weathered columnar structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a tall, weathered columnar structure centrally located in the scene, with visible marine growth (e.g., corals and encrustations) on its surface and surrounding seabed. The robot appears to be positioned at a moderate distance (~2–3 meters) in front of the structure, slightly below it (given upward-looking perspective). To approach and inspect its surface, the robot should first move forward toward the base, then circle or adjust laterally to examine different facets—especially since the current view shows only the front-right side. Given the structure’s height and potential shadowing, a slight upward pitch may help inspect upper sections after closing range. No immediate obstacles block the direct path forward.",
      "action": [
        "Go straight toward the columnar structure",
        "Slow down as you near the base (within 1 meter)",
        "Circle clockwise around the structure at close range",
        "Tilt camera upward slightly to inspect upper sections",
        "Pause briefly at each quadrant to observe surface details"
      ]
    },
    "images": [
      "./data/images/temp_90_0030_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green-painted pipeline straight ahead while maintaining a safe distance from the adjacent yellow-banded cables on the left.",
    "output": {
      "reasoning": "The image shows a green-painted pipeline running straight ahead along the seabed, serving as the primary navigation reference. To the left of the green pipeline, there is a bundle of yellow-banded cables—these are obstacles that must be avoided by maintaining a safe lateral distance. The robot is currently aligned with the green pipeline and positioned slightly to its right, which is appropriate given the hazard on the left. To follow the instruction precisely, the robot should continue moving forward while subtly adjusting rightward (if needed) to ensure it stays centered relative to the green pipeline and avoids drifting toward the yellow cables. No turns are required; only steady forward motion with minor course correction to preserve safe clearance.",
      "action": [
        "Go straight",
        "Slightly adjust right to maintain safe distance from yellow-banded cables"
      ]
    },
    "images": [
      "./data/images/0090.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe structure on the left and inspect the circular end cap marked with the number '1'.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe structure on the left side of the frame, with multiple circular end caps visible. The cap marked with the number '1' is clearly in view and positioned closest to the robot, slightly below and to the left. To approach and inspect it, the robot should first move forward toward the structure while maintaining alignment with the target cap. Since the cap is already within visual range and relatively close, no major lateral correction is needed—just a slight leftward adjustment may help center the cap in the field of view before closing in. After approaching, a final small forward motion will bring the robot near enough for detailed inspection.",
      "action": [
        "Move forward slightly",
        "Turn left slightly to align with the cap marked '1'",
        "Move forward toward the circular end cap marked '1'",
        "Stop and hover for inspection"
      ]
    },
    "images": [
      "./data/images/0070.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green mechanical structure on the seabed and inspect its central cylindrical component for signs of biofouling or damage.",
    "output": {
      "reasoning": "The green mechanical structure is clearly visible in the center-right of the image, resting on the seabed. It features a prominent central cylindrical component with radial supports and circular ports—this is the target for inspection. The robot (submersible in upper left) is currently at a distance and slightly elevated; to approach and inspect the central cylinder, it must first descend toward the seabed, then move forward (straight) toward the structure, and finally position itself close and centered relative to the cylinder for detailed visual inspection. No immediate obstacles block the direct path, though some coral/rock formations are nearby—maintaining a safe lateral margin is advisable. Given the lighting from above, approaching from the front-left will optimize illumination on the cylinder’s surface.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward straight toward the green mechanical structure",
        "Adjust left slightly to center on the central cylindrical component",
        "Hover steadily 1–2 meters in front of the cylinder",
        "Perform slow clockwise rotation to inspect full circumference"
      ]
    },
    "images": [
      "./data/images/temp_90_0003_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular metallic structure on the left while avoiding the coral clusters on the seabed.",
    "output": {
      "reasoning": "The image shows a large circular metallic structure (likely a pipe or tunnel section) on the left side of the frame, partially embedded in the seabed. Coral clusters are visible on the seabed in the foreground and midground, especially near the center and right. To approach the structure while avoiding corals, the robot should first move slightly right to clear the nearest coral cluster in front, then proceed forward along the left side where the seabed appears clearer, maintaining safe distance from corals. The structure is at moderate distance and angled away, so a gentle leftward trajectory after initial clearance will align with it.",
      "action": [
        "Move slightly right to avoid the nearest coral cluster",
        "Go straight while keeping the metallic structure on the left",
        "Gently turn left to align with the entrance of the circular structure",
        "Approach the structure steadily at low speed"
      ]
    },
    "images": [
      "./data/images/temp_90_0048_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large dark rock formation on the right while maintaining a safe distance from the textured coral-covered boulders in the foreground.",
    "output": {
      "reasoning": "The image shows a large dark rock formation on the right side of the frame, extending vertically and occupying much of the right half. In the foreground (lower-left to center), there are textured, coral-covered boulders that appear close to the robot’s current position. To safely approach the large dark rock on the right while avoiding the foreground coral boulders, the robot should first move slightly left to clear the immediate coral obstacles, then proceed forward while gently turning right to align toward the target rock, maintaining lateral clearance. Given the depth and lighting, a cautious forward motion with minor rightward correction is appropriate—avoid sharp turns near obstacles.",
      "action": [
        "Move slightly left",
        "Go forward",
        "Turn slightly right",
        "Continue forward while maintaining distance from foreground coral"
      ]
    },
    "images": [
      "./data/images/temp_90_0021_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the underwater cave opening toward the sunlit area while avoiding the large rocks on the seabed.",
    "output": {
      "reasoning": "The image shows an underwater cave with a clear opening ahead, where sunlit rays penetrate from above—indicating the target destination. Large rocks are scattered on the seabed, particularly on the left and right sides near the entrance, but a central corridor appears navigable. The robot is currently positioned inside the cave, facing the opening. To safely reach the sunlit area: (1) First, maintain forward motion while staying centered to avoid the rocks flanking the path; (2) As the opening widens, slightly adjust right to bypass a prominent rock on the left mid-path; (3) Continue straight toward the brightest sunlit zone, ensuring clearance from the large rock cluster on the right seabed. No sharp turns are needed—smooth forward progression with minor lateral corrections suffices.",
      "action": [
        "Go straight while staying centered",
        "Slightly turn right to avoid the left-side rock",
        "Continue straight toward the sunlit area"
      ]
    },
    "images": [
      "./data/images/temp_90_0008_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle on the seabed and inspect its front illuminated viewport.",
    "output": {
      "reasoning": "The submarine-like vehicle is positioned on the seabed to the right of the current view, with its front illuminated viewport clearly visible and facing slightly toward the camera. There are rocks in the foreground (left and bottom) that must be navigated around. To approach the front viewport safely, I should first move forward while slightly adjusting right to clear the left-side rock, then continue straight toward the vehicle until close enough for inspection. The lighting from the viewport helps localize the target. No immediate obstacles block the direct path once the initial rocks are bypassed.",
      "action": [
        "Move forward slightly while turning right to avoid the left foreground rock",
        "Proceed straight toward the submarine-like vehicle",
        "Stop when aligned with and near the front illuminated viewport for inspection"
      ]
    },
    "images": [
      "./data/images/0083.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car between the two large columns and inspect its front left wheel area for marine growth or damage.",
    "output": {
      "reasoning": "The submerged car is positioned between two large columns — one on the left (a multi-column structure) and one on the right (a single thick column). From the current viewpoint, the car is slightly to the right of center and partially obscured by the right column. To approach it safely and position for inspection of the front left wheel, I need to: (1) move forward while slightly adjusting left to clear the right column and align with the car’s midline; (2) continue forward until close enough to inspect; (3) then maneuver to the car’s front-left side, ensuring stable positioning for visual inspection of the wheel area (which is likely covered in marine growth or corrosion given the environment).",
      "action": [
        "Move forward slightly while turning left to align with the car between the columns",
        "Continue forward until within 2 meters of the car",
        "Turn left and approach the front-left corner of the car",
        "Hover steadily and inspect the front left wheel area for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0054_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the right while avoiding the tall dark coral structures on the left.",
    "output": {
      "reasoning": "From the image, the robot is currently oriented forward with tall dark coral structures visible on the left side of the frame, posing a collision risk. A yellow sponge (target) is located on the right side, partially illuminated and near the bottom-right region. To approach it safely, the robot must first steer right to clear the left-side corals, then proceed forward while maintaining a safe lateral offset from the left obstacles. The lighting suggests the sponge is relatively close, so a moderate turn followed by straight movement should suffice.",
      "action": [
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_90_0050_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object with green circular feature on the seabed and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The red dome-shaped object with a green circular feature is clearly visible on the seabed in the foreground, slightly to the right of center. The robot appears to be positioned above and slightly behind it (based on perspective and shadow). To approach and inspect its surface, I need to: (1) descend gently to get closer to the seabed level; (2) move forward toward the object; (3) position the robot directly in front or slightly above the object for optimal visual inspection; (4) possibly circle slightly to view multiple angles if needed, but initial priority is stable close approach. No immediate obstacles block the direct path. Lighting is sufficient for visual inspection.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward toward the red dome-shaped object",
        "Stop at ~0.5m distance from the object",
        "Adjust orientation to face the object directly",
        "Perform slow clockwise rotation to inspect surface"
      ]
    },
    "images": [
      "./data/images/0060.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient stone column and inspect its surface for inscriptions or structural damage.",
    "output": {
      "reasoning": "The image shows a tall, ancient stone column centered in the frame, standing upright on the seabed. It is surrounded by smaller rock formations and marine growth (e.g., green algae or coral), but there is a clear open path from the robot’s current position toward the column’s base. The column appears to be ~5–7 meters away based on perspective and scale relative to surrounding features. To inspect its surface for inscriptions or damage, I must first approach it closely—ideally within 1–2 meters—while maintaining stable orientation to enable high-resolution visual inspection. Since the column is directly ahead, the optimal initial action is to move straight forward. Once near, a slow circumferential movement will allow full surface coverage.",
      "action": [
        "Go straight toward the stone column",
        "Slow down as you near the base (within 2 meters)",
        "Orbit the column clockwise at close range to inspect all sides",
        "Pause briefly at each quadrant to capture detailed imagery"
      ]
    },
    "images": [
      "./data/images/0088.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and white striped buoy anchored on the seabed, maintaining a safe distance from the surrounding coral and rocks.",
    "output": {
      "reasoning": "The red and white striped buoy is centrally located in the image, anchored on the seabed. Coral formations are visible to the left and right of the buoy, and a large rock structure is on the right side. To approach safely, the robot should first align directly toward the buoy while maintaining central positioning to avoid the nearby coral (left) and rock (right). Given the clear frontal view and moderate distance, the safest initial action is to move straight forward cautiously. As it gets closer, fine adjustments can be made—but for now, 'Go straight' is the optimal first step to initiate approach while staying in the clearest corridor.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0071.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the left green columnar structure and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows two prominent green columnar structures; the left one is closer and more clearly visible, with circular openings and surface texture suggesting possible biofouling or damage. The robot is currently positioned slightly in front and to the right of the left column, with clear water path toward it. To approach and inspect its surface, I need to: (1) move leftward to align with the column’s midline, (2) advance straight toward it to reach inspection distance (~0.5–1 m), and (3) possibly circle slightly or hover to enable close visual inspection of surface features. Given the current view, a gentle left turn followed by forward motion is safest and most direct.",
      "action": [
        "Turn left slightly",
        "Go straight toward the left green column",
        "Hover near the column surface for inspection"
      ]
    },
    "images": [
      "./data/images/0064.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car between the two stone pillars and inspect its front right side for damage or debris.",
    "output": {
      "reasoning": "The green car is positioned centrally between two stone pillars, slightly angled with its front right side facing the viewer. From the current perspective, the robot appears to be at a moderate distance in front of the car, with clear visibility of the target area. To inspect the front right side, the robot must first approach the car directly, then maneuver to position itself adjacent to and slightly ahead of the front right wheel/fender region. The path appears unobstructed; no immediate obstacles block forward movement. Step-by-step: (1) Move straight toward the car to reduce distance; (2) Once near, turn slightly right to align with the front right side; (3) Advance gently to get a close-up view for inspection.",
      "action": [
        "Go straight toward the green car",
        "Turn slightly right",
        "Move forward to inspect the front right side"
      ]
    },
    "images": [
      "./data/images/0073.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object on the seabed while avoiding the large rocks on either side.",
    "output": {
      "reasoning": "The red dome-shaped object is clearly visible on the seabed, centered in the lower part of the image. Two large rocks flank it: one on the left (larger, textured) and one on the right (also large, partially shadowed). The robot appears to be positioned slightly above and behind the target, with a clear forward path toward the red dome if it stays centered between the two rocks. To avoid collision, the robot should maintain a central trajectory—neither veering left nor right—and descend gently while moving forward. The water column above is relatively clear, so no vertical obstacles interfere. Step-by-step: first align forward toward the dome, then proceed straight while maintaining safe lateral clearance from both rocks, possibly with minor course corrections if needed (but none are required in current frame).",
      "action": [
        "Go straight toward the red dome-shaped object",
        "Maintain center alignment between the left and right large rocks",
        "Descend gently to reach seabed level near the red dome"
      ]
    },
    "images": [
      "./data/images/temp_90_0042_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spherical coral structure from the front-left to inspect its surface texture and openings.",
    "output": {
      "reasoning": "The large spherical coral is centered in the image, slightly right of frame center, resting on the seabed. The robot appears to be positioned front-left relative to the coral (based on perspective and lighting), with clear open space in that approach quadrant. To inspect its surface texture and openings from the front-left, I need to: (1) move forward toward the coral while maintaining a slight left bias to stay on the intended approach vector; (2) close distance carefully to enable detailed visual inspection without disturbing sediment; (3) stabilize at optimal inspection range (~1–2 m) facing the front-left quadrant of the coral. No immediate obstacles block this path.",
      "action": [
        "Move forward-left slightly",
        "Advance steadily toward the coral",
        "Stop at ~1.5 meters distance, aligned with front-left side",
        "Stabilize position for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0031_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car parked beside the moss-covered concrete pillar and inspect its driver-side door for damage.",
    "output": {
      "reasoning": "The green car is visible to the right of the moss-covered concrete pillar, parked parallel to it. The driver-side door (left side of the car from our perspective, since the car faces roughly toward the camera) is partially visible but not yet close enough for detailed inspection. To approach and inspect it: (1) First, move forward toward the car while staying clear of the pillar and the large spherical object on the left; (2) Then, position the robot alongside the driver’s side—this requires a slight leftward lateral movement once near the front of the car; (3) Finally, halt and orient the camera toward the driver-side door for close inspection. No immediate obstacles block the path, but care must be taken to avoid disturbing sediment or drifting into the pillar.",
      "action": [
        "Go straight toward the green car",
        "Move slightly left to align with the driver-side door",
        "Stop and face the driver-side door for inspection"
      ]
    },
    "images": [
      "./data/images/0082.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-lit underwater vehicle positioned centrally among the coral formations.",
    "output": {
      "reasoning": "The yellow-lit underwater vehicle is clearly visible in the center of the image, surrounded by coral formations on both sides and slightly ahead. Sunlight rays indicate depth and orientation; the vehicle faces the robot directly and appears at moderate distance (~2–3 m). No immediate obstacles block the direct path. To approach it safely and precisely, I should first align straight toward it, then close the distance gradually while maintaining central alignment to avoid colliding with nearby corals.",
      "action": [
        "Go straight toward the yellow-lit vehicle",
        "Maintain central alignment with the vehicle",
        "Slowly advance until near the vehicle"
      ]
    },
    "images": [
      "./data/images/temp_90_0046_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged pickup truck and inspect its front-left wheel area for damage or entanglement.",
    "output": {
      "reasoning": "The submerged pickup truck is clearly visible in the center of the image, oriented diagonally with its front facing slightly leftward. The front-left wheel is on the near side (left from our perspective) and partially obscured by sediment and nearby yellow cylindrical objects (possibly pipes or buoys). To inspect the front-left wheel area, I need to: (1) approach the vehicle from the front-left quadrant to get a clear view; (2) maintain safe distance initially to avoid disturbing sediment; (3) maneuver closer once aligned; (4) position the camera for close-up inspection of the wheel well and tire area. The lighting from above creates glare but also illuminates the target area adequately. No immediate obstacles block the direct path to the front-left side.",
      "action": [
        "Move forward slowly toward the front-left side of the truck",
        "Turn slightly left to align with the front-left wheel",
        "Descend gently to lower operating depth near the wheel level",
        "Advance cautiously to within 1 meter of the front-left wheel",
        "Hover and adjust orientation to face the wheel well directly",
        "Perform close visual inspection of the front-left wheel and surrounding area"
      ]
    },
    "images": [
      "./data/images/0058.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted green car from the front-left side and inspect the open driver’s door while avoiding the scattered circular debris on the seabed.",
    "output": {
      "reasoning": "The rusted green car is positioned diagonally on the seabed, with its open driver’s door facing toward the left side of the frame. From the current robot perspective (likely slightly ahead and to the right of the car), the front-left approach requires moving forward while angling left to reach the driver’s door without crossing the scattered circular debris—these are concentrated in the foreground and near the car’s front-right wheel area. To avoid them, I should first move forward cautiously, then veer left to align with the front-left quadrant of the car, and finally advance closer to inspect the open driver’s door. The debris appears mostly in the lower half of the image; keeping a slight upward trajectory (i.e., maintaining depth) may help clear them, but lateral avoidance is primary.",
      "action": [
        "Go straight slowly",
        "Turn left slightly",
        "Go straight toward the front-left corner of the car",
        "Approach the open driver's door carefully",
        "Hover and inspect the open driver's door"
      ]
    },
    "images": [
      "./data/images/temp_90_0020_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-eyed submersible robot hovering near the center of the frame, maintaining a safe distance from the surrounding coral formations.",
    "output": {
      "reasoning": "The yellow-eyed submersible robot is centrally located in the frame, slightly elevated and facing the camera. Coral formations are present on the left, right, and bottom foreground, forming a semi-enclosed corridor. To approach safely, I must avoid lateral drift into corals. The robot appears stationary, so a controlled forward advance with slight central alignment is appropriate. First, I will gently move forward while keeping the target centered; then fine-tune heading to maintain clearance from nearby corals (especially the large one on the lower right and the cluster on the left). No sharp turns are needed—only minor corrections if drifting.",
      "action": [
        "Go straight slowly",
        "Slightly adjust left to center the target",
        "Maintain steady forward motion while monitoring coral proximity"
      ]
    },
    "images": [
      "./data/images/temp_90_0055_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical coral structure in the center foreground and inspect its surface for signs of marine growth or damage.",
    "output": {
      "reasoning": "The image shows a green cylindrical coral structure in the center foreground, slightly to the left of the robot's current heading. Sunlight streams from above, illuminating the scene and highlighting textures on the corals. The target coral is clearly visible and accessible; no immediate obstacles block the path. To inspect its surface closely, I need to: (1) move forward toward it while maintaining alignment, (2) possibly adjust laterally to center it in the field of view, and (3) get close enough for detailed visual inspection (within ~0.5–1 m). Given its position directly ahead and slightly left, a slight left correction followed by forward motion is optimal to approach head-on.",
      "action": [
        "Turn slightly left",
        "Go straight toward the green cylindrical coral",
        "Stop when ~0.5 meters away",
        "Hover and inspect the surface for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0004_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red ribbed lantern on the seabed while avoiding the nearby dark rock formations.",
    "output": {
      "reasoning": "The red ribbed lantern is clearly visible in the center-left of the image, resting on the seabed. A dark rock formation is directly beneath and slightly to the right of the lantern, casting a shadow. Another dark rock (possibly part of a larger formation) is visible to the far left, partially obscuring the view. To approach the lantern safely while avoiding rocks: (1) First, move slightly right to clear the left-side rock and align with the lantern’s open side; (2) Then proceed straight toward the lantern, maintaining a safe lateral distance from the dark rock directly under it; (3) Finally, close in gently to reach the lantern without contacting surrounding obstacles.",
      "action": [
        "Turn slightly right",
        "Go straight toward the red lantern",
        "Approach the lantern carefully from its left side"
      ]
    },
    "images": [
      "./data/images/temp_90_0023_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure on the left and inspect its front opening for debris or damage.",
    "output": {
      "reasoning": "The large green cylindrical structure is clearly visible on the left side of the image, with its front opening facing the camera. The robot appears to be at a moderate distance and slightly angled toward it. To approach and inspect the front opening, the robot should first align itself directly in front of the structure, then move forward carefully to get a closer view of the opening for debris or damage assessment. Since the opening is already in view, minimal lateral adjustment is needed—primarily forward motion with possible slight left correction to center the opening.",
      "action": [
        "Turn slightly left to center the green cylindrical structure's front opening in the field of view",
        "Go straight slowly toward the front opening",
        "Stop when within 1 meter of the opening for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0033_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar while avoiding the floating debris and illuminated green mesh structures above.",
    "output": {
      "reasoning": "The central stone pillar is directly ahead and well-lit, serving as the primary target. Floating debris appears as small, scattered bright spots in the mid- to far-field (e.g., lower right and center), and the illuminated green mesh structures form an overhead canopy—suggesting the robot must maintain a safe vertical clearance while advancing. Since the pillar is centered and the path appears unobstructed at ground level, the safest approach is to proceed straight while slightly adjusting upward or maintaining neutral pitch to avoid the overhead meshes, and subtly correcting laterally if debris drifts into the path. Given no immediate close-range obstacles directly in front, the first action is to go straight toward the pillar, monitoring for debris encroachment.",
      "action": [
        "Go straight toward the central stone pillar",
        "Maintain slight upward pitch to avoid overhead green mesh",
        "Monitor for floating debris and make minor lateral adjustments if needed"
      ]
    },
    "images": [
      "./data/images/temp_90_0044_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical structure secured by yellow ropes and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The image shows a large submerged cylindrical structure (likely a pipeline or buoy) secured by visible yellow ropes. It is positioned diagonally across the frame, with its nearest end on the right side and extending toward the upper left. The robot appears to be currently located to the left and slightly in front of the structure, with netting in the lower-left foreground possibly indicating proximity to a support platform or mooring area. To approach the cylinder for inspection, the robot should first move forward to reduce distance, then adjust laterally (rightward) to align parallel to the cylinder’s axis, ensuring close but safe proximity. Once aligned, it can proceed along the length of the cylinder while maintaining a consistent standoff distance for visual inspection. No immediate obstacles block the direct path, though care must be taken near the yellow ropes to avoid entanglement.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Turn right to align parallel to the cylinder",
        "Move slowly alongside the cylinder while maintaining ~1m distance",
        "Inspect surface continuously for cracks, corrosion, or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_90_0036_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater passage between the left rocky wall and the right sponge-covered rock formation, heading toward the sunlit opening ahead.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage flanked by a dark rocky wall on the left and a sponge-covered rock formation on the right. Sunlight streams from an opening ahead, indicating the goal direction. To safely navigate this constrained corridor, the robot must maintain central alignment—avoiding contact with either side—while progressing forward toward the light. Given the perspective, the passage appears to widen slightly ahead, so steady forward motion with minor lateral corrections is appropriate. No immediate obstacles block the path, but precise heading control is essential due to the narrow clearance.",
      "action": [
        "Go straight while maintaining center alignment between left rock wall and right sponge-covered formation",
        "Slightly adjust left if drifting toward right sponge formation",
        "Slightly adjust right if drifting toward left rocky wall",
        "Continue forward toward the sunlit opening"
      ]
    },
    "images": [
      "./data/images/temp_90_0014_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge-like organism on the left while maintaining a safe distance from the red robotic structure in the foreground.",
    "output": {
      "reasoning": "The image shows a red robotic structure in the foreground (right-lower quadrant), partially obstructing the view. A yellow sponge-like organism is visible on the left side, illuminated by sun rays, and appears to be at mid-depth. To approach it safely, I must first avoid the red structure—likely requiring a leftward movement to clear it. Since the target is already on the left, a gentle left turn followed by forward motion should align the robot toward the sponge while keeping lateral clearance from the red structure. Given the lighting and depth cues, the sponge is not extremely far, so a short forward advance after repositioning should suffice. No immediate obstacles lie directly between the current position and the sponge once the red structure is bypassed on its left side.",
      "action": [
        "Turn left slightly",
        "Move forward slowly",
        "Maintain slight leftward heading to keep distance from red structure"
      ]
    },
    "images": [
      "./data/images/0072.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the illuminated opening in the cave while avoiding the rocky outcrops on the left and right.",
    "output": {
      "reasoning": "The image shows an underwater cave with a bright illuminated opening ahead, serving as the target destination. Rocky outcrops are present on both left and right sides—especially prominent on the right (large textured rock mass) and left (irregular protrusions near the cave wall). The path to the opening appears relatively clear in the center but requires careful lateral positioning to avoid collisions. To safely navigate, the robot should first center itself within the passage, then proceed forward while maintaining clearance from both sides. Given the perspective, the opening is directly ahead but slightly elevated; thus, a slight upward tilt may help align with the exit plane, though primary motion is forward. No immediate turn is needed since the opening lies straight ahead, but minor corrections may be required mid-transit. For atomic steps: (1) ensure centered alignment, (2) advance steadily toward the light, (3) monitor proximity to obstacles and adjust if needed—but since we must output fixed steps without real-time feedback, we assume initial centering suffices for safe transit.",
      "action": [
        "Center yourself in the cave passage",
        "Go straight toward the illuminated opening",
        "Maintain steady forward motion while keeping equal distance from left and right rocky outcrops"
      ]
    },
    "images": [
      "./data/images/temp_90_0022_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its upper ornamental structure for signs of marine growth or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned slightly to the right of center and at a moderate distance from the robot's current location. The upper ornamental structure (capitals with carved details) is illuminated by overhead light, making it inspectable. To inspect it closely for marine growth or damage, I need to: (1) move forward toward the column while maintaining alignment, (2) adjust lateral position if needed to center the column in view, and (3) ascend slightly to bring the upper ornamental section into optimal inspection range (since the camera appears level or slightly low relative to the capital). No immediate obstacles block the direct path, though debris on the seabed suggests cautious forward motion.",
      "action": [
        "Go straight toward the central stone column",
        "Slightly adjust left to center the column in view",
        "Ascend gently to align camera with the upper ornamental structure",
        "Hover and inspect the capital for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0005_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green ribbed cylindrical object with the red fin-like protrusion and inspect its surface for附着物 or damage.",
    "output": {
      "reasoning": "The green ribbed cylindrical object with a red fin-like protrusion is visible on the right side of the image, partially adjacent to a large spotted rock. It is at moderate distance and slightly angled away from the robot’s current heading. To approach it for inspection, I need to first reorient toward it (turn right), then close the distance (go straight), and finally position closely and stably for surface inspection (adjust forward/backward and possibly slight yaw). No immediate obstacles block the path, though small fish are present but non-interfering. The goal is to get within ~0.5–1 m for clear visual inspection of附着物 (biofouling) or damage.",
      "action": [
        "Turn right",
        "Go straight",
        "Move forward slowly",
        "Hover and stabilize"
      ]
    },
    "images": [
      "./data/images/0086.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green striped buoy with the orange cone and inspect its surface for damage or attached sensors.",
    "output": {
      "reasoning": "The green striped buoy with an orange cone is clearly visible in the center-left of the image, resting on the seabed and partially supported by a rusted circular structure. The robot (camera platform) is positioned above and slightly behind it. To approach and inspect its surface, I need to: (1) move forward and slightly downward to reach the buoy’s level; (2) close in laterally to position the camera near the buoy’s side; (3) circle or adjust orientation to inspect the full surface, especially around the orange cone and possible sensor mounts (e.g., holes or protrusions on the buoy). No immediate obstacles block the direct path, though the rusted structure lies beneath—so maintain safe clearance (~0.5 m) to avoid contact.",
      "action": [
        "Move forward and descend slightly toward the green striped buoy",
        "Approach laterally to position within 1 meter of the buoy’s side",
        "Circle slowly around the buoy while maintaining close proximity",
        "Inspect surface details near the orange cone and any visible ports or attachments"
      ]
    },
    "images": [
      "./data/images/temp_90_0062_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front grille for marine growth or damage.",
    "output": {
      "reasoning": "The submerged car is clearly visible in the center-right of the image, resting on the seabed. Its front grille faces slightly toward the camera but angled leftward. The robot appears to be positioned at a moderate distance (~3–4 m) and slightly to the left-front of the car. To inspect the front grille closely, the robot must: (1) move forward to reduce distance, (2) adjust laterally (slightly right) to align directly with the grille’s centerline, and (3) approach until close enough for detailed visual inspection (within ~1 m). No immediate obstacles block the path; minor marine growth is already visible on the grille, but closer inspection is required per instruction.",
      "action": [
        "Go straight toward the car",
        "Slightly turn right to center on the front grille",
        "Continue forward slowly until within 1 meter of the grille",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0052_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge located on the seabed near the center-left of the frame, avoiding the large overhanging rock formation above and the submarine to the left.",
    "output": {
      "reasoning": "The yellow sponge is not directly visible in the current frame, but based on the instruction, it should be near the center-left seabed. The large overhanging rock formation dominates the upper-right portion of the image and casts a shadow; we must stay below it but avoid getting too close to prevent collision. A submarine is positioned on the left side, partially behind a rock—so we must steer rightward initially to avoid it. The seabed appears relatively clear in the center-left region, suggesting a safe path forward after clearing the submarine. Therefore, the plan is: (1) move slightly right to clear the submarine’s position, (2) proceed straight while maintaining depth to stay under the overhang, (3) adjust leftward gently as we approach the target zone to center on the sponge.",
      "action": [
        "Turn slightly right",
        "Go straight at constant depth",
        "Turn slightly left",
        "Approach the seabed gently"
      ]
    },
    "images": [
      "./data/images/temp_90_0007_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated yellow net structure while avoiding the suspended cables and the fish swimming nearby.",
    "output": {
      "reasoning": "The illuminated yellow net structure is the primary target, located centrally and slightly below the current viewpoint. Suspended cables are visible in the upper-left quadrant, extending diagonally downward—these must be avoided by steering right or maintaining clearance. A fish is swimming near the left side of the net, but it appears at a safe distance and moving away; no immediate evasive action needed. The robot is currently positioned somewhat above and slightly left of the net’s entrance. To approach safely: (1) first adjust rightward to clear the cables, (2) then descend gently while maintaining central alignment with the net opening, (3) proceed forward into the net structure without sudden movements to avoid disturbing the fish or entanglement.",
      "action": [
        "Turn right slightly",
        "Descend slowly",
        "Go straight toward the net entrance",
        "Enter the illuminated yellow net structure"
      ]
    },
    "images": [
      "./data/images/temp_90_0015_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column while avoiding the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, illuminated by light rays from above and positioned slightly to the right of center. Scattered debris (small rocks and fragments) lies on the seabed, particularly in the foreground and left side. To approach the column safely, I must navigate forward while steering clear of the debris field. The clearest path appears to be slightly right of center—avoiding the left-side debris—and maintaining a straight but cautious trajectory toward the column’s base. Since the column is directly ahead but offset right, a slight rightward correction early on will align the robot with the safest approach corridor.",
      "action": [
        "Turn slightly right",
        "Go straight slowly",
        "Adjust left slightly to avoid nearby debris",
        "Continue straight toward the base of the central stone column"
      ]
    },
    "images": [
      "./data/images/temp_90_0043_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater canyon, staying centered between the two rocky walls while avoiding the suspended debris and maintaining depth above the seabed coral formations.",
    "output": {
      "reasoning": "The robot is currently positioned in a narrow underwater canyon with rocky walls on both left and right. Sunlight penetrates from above, illuminating the central path. A suspended cylindrical debris object is visible directly ahead, slightly below the robot’s current depth. The seabed is covered with coral formations, requiring the robot to maintain sufficient altitude to avoid collision. To stay centered, the robot must balance lateral position relative to both walls; the left wall appears closer than the right in the current frame, suggesting a slight rightward drift correction may be needed. The debris lies along the central corridor, so a slight upward adjustment followed by forward motion will help clear it safely. After passing the debris, continued centering and depth maintenance are required.",
      "action": [
        "Slightly ascend to clear suspended debris",
        "Move forward while gently correcting right to stay centered",
        "Maintain steady depth above coral seabed",
        "Continue forward, monitoring wall proximity for centering"
      ]
    },
    "images": [
      "./data/images/temp_90_0047_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical perforated pipe structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The vertical perforated pipe structure is directly in front of the robot, centered in the field of view, and appears within close to moderate range. The lighting highlights its surface, suggesting good visibility for inspection. To inspect its surface thoroughly, the robot should first approach it closely (but safely) to resolve fine details like marine growth or damage. Since the pipe is upright and dominates the scene, a straight-forward approach is optimal. After reaching an appropriate inspection distance (~1–2 m), a slow circumferential movement (e.g., gentle right turn while maintaining distance) will allow full surface coverage. However, the initial step is to close the distance safely.",
      "action": [
        "Go straight toward the pipe structure",
        "Slow down as you near the pipe (within ~1.5 m)",
        "Begin a slow rightward arc to inspect the pipe surface"
      ]
    },
    "images": [
      "./data/images/0087.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall textured pillar on the right while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The image shows an underwater scene with a prominent tall textured pillar on the right side, matching the target described. Surrounding it are irregular rock formations—especially to the left and near the foreground—that must be avoided. The robot appears to be positioned slightly left of center, facing forward, with the pillar visible at a moderate distance (~3–5 m) to the right. To approach safely: (1) First, align the robot toward the pillar by turning right slightly (not too sharply, to avoid drifting into nearby rocks). (2) Then proceed forward cautiously while monitoring proximity to adjacent rocks—especially the large formation on the immediate left and the rounded rocks in the lower right. (3) Maintain a lateral offset (e.g., ~1 m) from the pillar’s base to avoid collision with its irregular surface or nearby debris. Given the lighting and visibility, small incremental movements are safest.",
      "action": [
        "Turn right slightly",
        "Go straight slowly",
        "Maintain slight rightward offset while advancing",
        "Pause and reassess distance to pillar and nearby rocks"
      ]
    },
    "images": [
      "./data/images/temp_90_0026_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green and purple striped cylindrical object on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large green and purple striped cylindrical object lies horizontally on the seabed in the foreground, slightly to the left of center. The robot is currently positioned above and behind it (mid-water), with clear visibility and no immediate obstacles blocking approach. To inspect its surface, I need to: (1) descend toward the seabed to reach the object’s level, (2) move forward slowly to close distance, (3) position the robot parallel and close to the cylinder for detailed visual inspection—ideally along its length to check for damage or marine growth. Given the object’s orientation (left-to-right across frame), approaching from the front-left and then moving rightward along its side is optimal. No red obstacles are present; minor rocks nearby do not impede path.",
      "action": [
        "Descend vertically toward the seabed",
        "Move forward slowly toward the cylindrical object",
        "Position parallel to the cylinder, ~0.5 m away",
        "Translate rightward along the cylinder’s length while maintaining proximity",
        "Pause briefly at mid-section to inspect surface details"
      ]
    },
    "images": [
      "./data/images/temp_90_0027_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue pickup truck and inspect its front right wheel for damage or entanglement.",
    "output": {
      "reasoning": "The blue pickup truck is visible in the center-right of the image, resting on the seabed between two large columns. Its front right wheel is partially visible and appears accessible from the robot’s current position (assumed to be slightly left and forward of the truck, based on perspective). To inspect the front right wheel, I need to: (1) move closer to the truck while avoiding the left column and seabed obstacles; (2) position the robot to the front-right side of the truck for optimal view of that wheel; (3) stabilize and focus on the wheel for inspection. Given the lighting and clarity, a direct approach with slight rightward correction should suffice.",
      "action": [
        "Go straight toward the blue pickup truck",
        "Turn slightly right to align with the front right side of the truck",
        "Move forward slowly to get within 1 meter of the front right wheel",
        "Hover and stabilize for visual inspection of the front right wheel"
      ]
    },
    "images": [
      "./data/images/temp_90_0053_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange cylindrical pipe lying on the seabed and inspect its open end for blockages or damage.",
    "output": {
      "reasoning": "The large orange cylindrical pipe is clearly visible on the seabed, oriented diagonally with its open end facing roughly toward the camera’s right side. The robot is currently positioned at a moderate distance (~3–4 m) and slightly to the left of the pipe’s axis. To inspect the open end, the robot must first approach the pipe while maintaining alignment with the open end (to avoid occlusion or glancing views). Given the pipe’s orientation, a slight rightward correction followed by forward motion will bring the robot into optimal inspection position. Once near (~1 m), a final fine adjustment may be needed to center the open end in the field of view for detailed visual inspection.",
      "action": [
        "Turn right slightly",
        "Go straight toward the pipe",
        "Adjust position to center the open end in view",
        "Hover and inspect the open end for blockages or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0010_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green mesh pipeline and inspect the red-and-black flexible hose attached to it.",
    "output": {
      "reasoning": "The image shows a green mesh pipeline lying on the seabed, with a red-and-black flexible hose visibly attached to it on the right side. The robot is currently positioned slightly left and above the pipeline, facing roughly toward it. To approach the pipeline and inspect the hose, the robot should first move forward (straight) to reduce distance, then adjust laterally (right) to align with the hose attachment point. Given the hose is coiled and prominent on the right side of the pipeline, a slight right turn after approaching will position the robot for close inspection. No immediate obstacles block the path.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Move forward toward the red-and-black hose",
        "Hover and inspect the hose connection"
      ]
    },
    "images": [
      "./data/images/0076.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted orange submersible vehicle and inspect its front green-rimmed viewport for damage or obstructions.",
    "output": {
      "reasoning": "The rusted orange submersible vehicle is directly in front of the robot, centered in the frame, with its green-rimmed viewport clearly visible and facing the camera. The distance appears moderate—close enough to approach without major navigation obstacles, but not yet at inspection range. To inspect the viewport for damage or obstructions, I need to: (1) move forward to reduce distance, (2) stabilize position directly in front of the viewport, and (3) possibly adjust pitch/yaw slightly to center the viewport in the field of view if needed. Given the clear line of sight and absence of immediate obstacles, a straightforward forward motion followed by fine positioning is optimal.",
      "action": [
        "Go straight",
        "Go straight",
        "Hover and center the green-rimmed viewport in view"
      ]
    },
    "images": [
      "./data/images/temp_90_0061_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted submarine hull on the right and inspect the visible porthole and surrounding corrosion.",
    "output": {
      "reasoning": "The image shows a rusted submarine hull on the right side of the frame, with a visible porthole near its conning tower and extensive corrosion along its surface. The robot is currently positioned at a moderate distance and slightly to the left of the hull. To approach and inspect the porthole and surrounding corrosion, the robot must first reposition to the right to align with the hull, then move forward steadily while maintaining proximity for detailed visual inspection. Given the underwater environment and potential drift, small, controlled movements are preferred. No immediate obstacles block the path to the hull.",
      "action": [
        "Turn right slightly",
        "Move forward slowly toward the submarine hull",
        "Adjust position to center the porthole in view",
        "Hover and inspect the porthole and surrounding corrosion"
      ]
    },
    "images": [
      "./data/images/temp_90_0038_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the right while maintaining a safe distance from the seabed and avoiding the scattered smaller corals.",
    "output": {
      "reasoning": "The robot is currently facing forward, with a large textured coral formation visible on the right side of the frame. Smaller scattered corals are present on the seabed in front and to the right, near the base of the large coral. To approach the large coral while avoiding smaller ones and maintaining safe altitude above the seabed, the robot should: (1) gently turn right to align toward the large coral without drifting into nearby small corals; (2) advance cautiously while keeping a consistent height (avoid descending); (3) fine-tune heading to stay parallel to the large coral’s edge at a safe lateral distance (~1–2 m), ensuring no collision with protruding branches or nearby small corals. The lighting suggests good visibility, supporting precise maneuvering.",
      "action": [
        "Turn right slightly",
        "Ascend slightly to maintain safe altitude above seabed",
        "Go straight slowly toward the large coral formation",
        "Adjust heading left slightly to maintain safe lateral distance from the large coral and avoid smaller corals"
      ]
    },
    "images": [
      "./data/images/temp_90_0067_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large perforated brown sponge in the foreground while avoiding the rocky outcrop to the right.",
    "output": {
      "reasoning": "The large perforated brown sponge is clearly visible in the foreground, centered-left in the image. A rocky outcrop occupies the right side of the frame, partially shadowed and extending vertically — this must be avoided. The robot appears to be positioned slightly behind and above the sponge, with open water/sandy bottom between. To approach the sponge safely: first, ensure lateral clearance from the right-side rock by moving slightly left; then proceed straight toward the sponge; finally, close in carefully to avoid collision due to perspective or current drift.",
      "action": [
        "Move slightly left to clear the rocky outcrop on the right",
        "Go straight toward the large perforated brown sponge",
        "Slowly approach and stop near the sponge"
      ]
    },
    "images": [
      "./data/images/temp_90_0041_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral formation in the center while avoiding the dense green seaweed patch to its right.",
    "output": {
      "reasoning": "The large green ridged coral formation is centrally located in the image, with distinct vertical ridges and a prominent arch-like structure. To its right (from the robot’s perspective), there is a dense patch of fine, bright green seaweed that must be avoided. The robot appears to be slightly left of the coral’s centerline and at a moderate distance. To approach safely: first, maintain a slight leftward offset to stay clear of the seaweed; then proceed straight toward the coral; if needed, make a minor right correction as it nears to center on the formation without drifting into the seaweed.",
      "action": [
        "Turn slightly left",
        "Go straight toward the central green ridged coral",
        "Adjust slightly right as you near the coral to center on it"
      ]
    },
    "images": [
      "./data/images/temp_90_0001_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater vehicle illuminated by sunbeams and inspect its front sensor array.",
    "output": {
      "reasoning": "The green underwater vehicle is directly ahead and centered in the frame, illuminated by sunbeams from above, indicating it is the primary target. Its front sensor array (a prominent circular yellow-lit aperture) faces the robot. The environment appears clear with no immediate obstacles between the robot and the vehicle. To inspect the front sensor array, the robot must first close the distance safely, then position itself for a close-up view—ideally slightly below or in front at a stable distance to avoid occlusion and ensure optimal lighting. Given the current alignment, the shortest safe path is straight ahead, followed by a slight forward adjustment to reach inspection range.",
      "action": [
        "Go straight toward the green vehicle",
        "Slow down as you near the vehicle",
        "Position yourself 1–2 meters in front of the vehicle, centered on the front sensor array",
        "Hover steadily to inspect the front sensor array"
      ]
    },
    "images": [
      "./data/images/temp_90_0018_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater vehicle lying horizontally among the coral formations, maintaining a safe distance to avoid disturbing the reef.",
    "output": {
      "reasoning": "The large cylindrical underwater vehicle is lying horizontally in the mid-to-lower portion of the image, surrounded by coral formations on both sides and below. The robot appears to be positioned slightly above and behind the vehicle (based on perspective and lighting). To approach it safely without disturbing the reef, I should: (1) descend gently to align with the vehicle’s vertical level while staying clear of the foreground corals; (2) move forward slowly along a path that keeps the vehicle centered and avoids close proximity to nearby coral clusters; (3) maintain lateral clearance—especially from the dense coral on the left—and adjust heading slightly right if needed to stay in open water above the reef bed. No immediate obstacles block a direct but cautious approach.",
      "action": [
        "Descend slightly to match the vehicle's depth",
        "Move forward slowly toward the vehicle",
        "Maintain slight rightward offset to avoid left-side coral",
        "Hold steady at a safe distance (~2–3 meters) once aligned"
      ]
    },
    "images": [
      "./data/images/temp_90_0012_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow and black patterned pipe on the right while maintaining a safe distance from the two submerged cylindrical robots on the left.",
    "output": {
      "reasoning": "The image shows the robot's current view: on the right is a prominent yellow-and-black patterned pipe (target), and on the left are two submerged cylindrical robots (obstacles). The pipe lies roughly along the forward-right direction, slightly ahead. To approach it safely, I must avoid drifting left toward the cylindrical robots. First, I need to align toward the pipe without moving too close to the left obstacles — a slight rightward correction is appropriate. Then, proceed forward while maintaining lateral clearance from the left-side robots. Since the pipe is already in view and relatively close, only a short sequence of controlled movements is needed.",
      "action": [
        "Turn slightly right",
        "Go straight slowly",
        "Maintain distance from left cylindrical robots"
      ]
    },
    "images": [
      "./data/images/temp_90_0068_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible vehicle lying on the seabed and inspect its front hatch and sensor array.",
    "output": {
      "reasoning": "The green cylindrical submersible is lying on the seabed, angled slightly with its front (hatch and sensor array) facing toward the upper-right of the image. From the robot’s current perspective (lower-left), the vehicle is diagonally ahead and to the right. To inspect the front hatch and sensor array, I must first approach it head-on—i.e., align with its longitudinal axis and close the distance. The seabed appears relatively clear of immediate obstacles near the target, though some small rocks and marine growth are present; thus, a controlled forward trajectory with slight rightward correction is safe. Once near, a final fine adjustment will position the robot directly in front of the hatch/sensor area for inspection.",
      "action": [
        "Turn right slightly",
        "Go straight toward the green submersible",
        "Approach until aligned with the front hatch",
        "Hover steadily in front of the hatch and sensor array for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0051_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange and black netted basket on the seabed and inspect its contents.",
    "output": {
      "reasoning": "The orange and black netted basket is clearly visible on the seabed in the foreground, slightly to the right of center. Sunlight from above illuminates it, confirming its position and accessibility. The robot appears to be at a moderate distance (~1–2 meters) and slightly elevated relative to the basket. To inspect its contents, the robot must first approach it closely—ideally positioning directly above or slightly in front—while maintaining stable orientation. No immediate obstacles block the path. Therefore, the plan is: (1) move forward toward the basket, (2) fine-tune position to hover directly over or near the opening, (3) stabilize for visual inspection.",
      "action": [
        "Go straight toward the orange and black netted basket",
        "Move slightly downward to align with the basket's opening",
        "Hover steadily above the basket for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0024_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green columnar structure and inspect its base for attached marine growth or debris.",
    "output": {
      "reasoning": "The central glowing green columnar structure is clearly visible and dominates the scene. It stands vertically with a flared top and illuminated segments, and its base is partially visible, surrounded by net-like debris and some marine growth (e.g., dark organic masses near the bottom). The robot appears to be positioned slightly to the left and at a moderate distance from the structure. To inspect the base closely, I need to: (1) move forward toward the structure while maintaining center alignment, (2) possibly adjust laterally to avoid nearby net debris on the left, and (3) descend slightly if needed to bring the base into optimal view. Given the current perspective, the most immediate action is to advance straight toward the column’s centerline, then fine-tune position once closer.",
      "action": [
        "Go straight toward the central green column",
        "Slightly adjust right to center on the column",
        "Continue forward until near the base",
        "Hover and tilt downward to inspect the base for marine growth or debris"
      ]
    },
    "images": [
      "./data/images/0084.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its carved surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned directly ahead of the robot's current viewpoint. It stands upright on the seabed, with carved surfaces facing multiple directions. To inspect its surface for inscriptions or damage, the robot must first close the distance safely without disturbing sediment (to maintain visibility). Given the clear path forward and moderate distance (~2–3 meters), a controlled forward movement is appropriate. Once near (within ~0.5 m), a slow circular maneuver around the column will allow full visual inspection of all sides. The lighting is sufficient for detail capture, and no immediate obstacles block the approach.",
      "action": [
        "Go straight toward the central stone column",
        "Slow down as you approach within 1 meter",
        "Orbit the column clockwise at close range to inspect all carved surfaces",
        "Pause briefly at each cardinal side to capture high-resolution visual data"
      ]
    },
    "images": [
      "./data/images/temp_90_0019_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large ribbed green structure on the seabed while maintaining a safe distance from the coral formation to its right.",
    "output": {
      "reasoning": "The image shows a large ribbed green structure (likely the target) lying horizontally on the seabed. To its right is a dense coral formation with fan-shaped corals, which must be avoided. The robot appears to be positioned slightly in front and to the left of the green structure, with clear space on the left side of the structure. To approach while maintaining a safe distance from the coral, the robot should first move forward cautiously toward the structure, then adjust laterally to stay left of the structure’s midline—ensuring it remains clear of the coral on the right. Since the structure is elongated and oriented roughly parallel to the camera’s view, a slight leftward bias during forward motion will help preserve clearance.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Continue straight while keeping the green structure centered-left in view",
        "Maintain distance so the coral formation remains clearly visible on the right edge of the frame"
      ]
    },
    "images": [
      "./data/images/temp_90_0032_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like structure and inspect the yellow bands and circular emblem on its hull.",
    "output": {
      "reasoning": "The submarine-like structure is visible diagonally across the frame, oriented from lower-left to upper-right. The yellow bands and circular emblem are on its hull, closer to the midsection (near center-right of image). The robot appears to be positioned slightly behind and to the left of the structure, with seaweed-covered seabed on the left side. To approach for inspection, the robot should first move forward toward the structure while slightly adjusting rightward to align with the hull’s midsection where the yellow bands and emblem are located. Once near, a slight turn may be needed to face the hull directly for clear visual inspection.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight slowly",
        "Stop and hover facing the hull"
      ]
    },
    "images": [
      "./data/images/temp_90_0034_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-banded pipe structure while avoiding the surrounding tangled yellow ropes and netting.",
    "output": {
      "reasoning": "The large green-banded pipe structure is directly ahead and slightly elevated in the image, serving as the primary target. Surrounding it are tangled yellow ropes and netting—especially dense on the left and bottom-left, with some extending toward the right side. The robot appears to be positioned below and slightly in front of the structure. To safely approach without entanglement: (1) First, ascend slightly to clear the lower netting; (2) Then, move forward cautiously while maintaining a central path between the left and right rope clusters; (3) Finally, close in on the pipe structure from its front face, avoiding lateral drift into the nets. No sharp turns are needed since the target is aligned with current heading, but fine lateral correction may be required mid-approach.",
      "action": [
        "Ascend slightly",
        "Go straight slowly",
        "Maintain center alignment relative to the pipe",
        "Approach the green-banded pipe structure head-on"
      ]
    },
    "images": [
      "./data/images/temp_90_0016_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red ribbed object on the seabed while avoiding the surrounding rocks and maintaining a safe distance from the yellow fish swimming nearby.",
    "output": {
      "reasoning": "The red ribbed object is clearly visible in the foreground, centered slightly to the right. Surrounding it are several dark rocks: one large rock to the left, another to the far right, and smaller ones scattered nearby. A yellow fish is swimming on the left side of the frame, near the left rock. To approach the red object safely: (1) First, maintain lateral clearance from the yellow fish by staying to its right; (2) Navigate between the left and right rocks—preferably along a central path where seabed is clearer; (3) Advance gradually while keeping the red object centered in view; (4) Avoid sudden turns near rocks; instead, make small course corrections as needed. The safest initial move is to go straight while slightly angling right to avoid the left rock and the fish.",
      "action": [
        "Go straight while slightly turning right",
        "Maintain center alignment with the red ribbed object",
        "Adjust leftward slightly if nearing the right rock",
        "Pause briefly if the yellow fish approaches within 1 meter",
        "Continue forward slowly until 0.5 meters from the red object"
      ]
    },
    "images": [
      "./data/images/0059.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral structure in the center while avoiding the smaller yellow and purple spherical corals on the seabed.",
    "output": {
      "reasoning": "The large green ridged coral structure is centrally located and clearly visible. In the foreground, there are smaller yellow and purple spherical corals on the seabed—specifically, one yellow and one purple sphere to the lower right of the central structure, and another yellow sphere to the lower left. To approach the central green coral while avoiding these obstacles, the robot should first move forward cautiously, then adjust laterally to steer clear of the spheres. Given their positions, a slight rightward correction after initial forward motion will help avoid the lower-right yellow and purple spheres, while maintaining safe distance from the left-side yellow sphere. The path should be straight but offset slightly right to stay clear of all small corals.",
      "action": [
        "Go straight slowly",
        "Turn slightly right",
        "Continue straight toward the green ridged coral"
      ]
    },
    "images": [
      "./data/images/temp_90_0039_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue cylindrical structure with yellow bands and inspect its midsection for attached sensors or biofouling.",
    "output": {
      "reasoning": "The blue cylindrical structure with yellow bands is directly in front of the robot and clearly visible. It stands vertically on the seabed, surrounded by coral formations. To inspect its midsection (approximately halfway up the cylinder), the robot must first approach it closely without colliding—maintaining a safe distance initially, then moving laterally or adjusting depth to align with the midsection. Given the current frontal view, the optimal sequence is: (1) advance straight toward the structure to reduce distance, (2) once near (~1–2 m), fine-tune position to center the midsection in the field of view, possibly with slight upward/downward adjustment if needed (though the structure appears vertically aligned and midsection is at camera height), and (3) hold position for inspection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the blue cylindrical structure",
        "Slow down and stop at ~1.5 meters distance",
        "Adjust position slightly to center the midsection in view",
        "Hold position for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0049_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical pipe-like structure emitting bubbles in the center of the underwater cave, while avoiding the algae-covered rocky walls on both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater cave with algae-covered rocky walls on both left and right sides. In the center, a vertical pipe-like structure is visible, emitting a stream of bubbles upward—this is the target. The robot is positioned at the cave entrance or mid-cave, facing the structure. To approach it safely: (1) Maintain central alignment to avoid contact with either wall; (2) Proceed forward steadily while monitoring lateral clearance; (3) The bubble trail provides a clear visual guide for centerline navigation. No sharp turns are needed—the path is straight but requires precise lateral control.",
      "action": [
        "Go straight while keeping centered between the left and right algae-covered walls",
        "Maintain steady forward motion toward the bubble-emitting vertical pipe",
        "Slightly adjust left/right as needed to preserve equal distance from both walls"
      ]
    },
    "images": [
      "./data/images/temp_90_0013_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the stone pillar structure and inspect its base for signs of marine growth or structural damage.",
    "output": {
      "reasoning": "The stone pillar structure is clearly visible in the center-right of the image, standing vertically on the seabed. Sunlight rays illuminate it from above and left, indicating good visibility. The robot appears to be positioned slightly to the left and at a moderate distance (~3–4 m) from the pillar, with open space in front. To inspect the base for marine growth or damage, I need to: (1) move forward toward the pillar while maintaining alignment, (2) get close enough (~1 m) to resolve fine details at the base, and (3) possibly adjust orientation to ensure full base visibility (e.g., slight yaw if needed). No immediate obstacles block the direct path. Given the current view, a straight approach followed by a minor lateral adjustment (if required upon closer inspection) is optimal.",
      "action": [
        "Go straight toward the stone pillar",
        "Slow down as you near the base",
        "Position yourself ~1 meter from the pillar base",
        "Slightly rotate right to view the full circumference of the base"
      ]
    },
    "images": [
      "./data/images/0081.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of green mushroom-shaped corals in the center foreground while avoiding the large blue-striped shell on the left.",
    "output": {
      "reasoning": "The image shows a cluster of green mushroom-shaped corals in the center foreground, which is the target. To the left, there is a large blue-striped shell (likely a giant clam or similar bivalve) that must be avoided. The robot is currently positioned such that the corals are directly ahead and slightly to the right of center, while the shell occupies the left mid-ground. To safely approach the corals without colliding with the shell, the robot should first steer slightly right to clear the shell’s position, then proceed straight toward the coral cluster. No immediate obstacles lie directly in the central path once the left-side shell is cleared.",
      "action": [
        "Turn slightly right",
        "Go straight toward the green mushroom-shaped corals"
      ]
    },
    "images": [
      "./data/images/0079.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the coiled yellow rope on the seabed while avoiding the large submerged pipe and netting overhead.",
    "output": {
      "reasoning": "The coiled yellow rope is visible on the seabed in the lower-left foreground. A large submerged pipe runs horizontally across the mid-to-background, and netting is draped overhead (especially above and behind the pipe). To approach the rope safely: (1) The robot must stay low to avoid the overhead netting; (2) It should navigate *under* or *around* the pipe—since the rope lies in front of the pipe from the current view, moving forward while staying left of the pipe’s axis avoids collision; (3) The rope is already relatively close, so a short forward movement with slight left correction will bring it into proximity. No sharp turns are needed; gentle steering suffices.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Maintain low altitude to avoid overhead netting"
      ]
    },
    "images": [
      "./data/images/0089.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large greenish sponge-like structure in the center-right while avoiding the coral formations on the left and bottom right.",
    "output": {
      "reasoning": "The large greenish sponge-like structure is prominent in the center-right of the image. To approach it, the robot must navigate from its current position (likely near the lower-left, given perspective) toward the center-right. There are coral formations on the left (a tall, dark, textured structure with green algae) and bottom right (branching yellowish corals), which must be avoided. The safest path appears to be: first move slightly right to clear the left coral, then proceed forward while maintaining a safe distance from the bottom-right coral, and finally adjust heading to center on the target sponge. Given the visual layout, a gentle right turn followed by straight advance, then minor fine-tuning, should suffice.",
      "action": [
        "Turn right slightly",
        "Go straight",
        "Adjust heading slightly right to center on the greenish sponge"
      ]
    },
    "images": [
      "./data/images/temp_90_0002_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front right side for damage or marine growth.",
    "output": {
      "reasoning": "The submerged car is visible in the lower-right portion of the image, oriented diagonally with its front facing roughly toward the upper-left. The front right side (from the car’s perspective) corresponds to the side closest to the viewer and slightly to the right — i.e., the right-front quarter panel and headlight area, which appears partially illuminated and covered in green marine growth. To inspect it closely, I need to: (1) move forward toward the car while maintaining a safe distance initially; (2) adjust laterally to position the robot on the car’s front-right side (i.e., approach from the car’s right-front quadrant); (3) get closer for detailed inspection without colliding. Given the car is stationary and no immediate obstacles block the path, a direct but controlled approach is feasible.",
      "action": [
        "Go straight toward the submerged car",
        "Turn slightly right to align with the car's front right side",
        "Move forward slowly to inspect the front right side closely"
      ]
    },
    "images": [
      "./data/images/0074.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the upright turquoise cylindrical structure with circular ports and inspect its base where it connects to the seabed pipe.",
    "output": {
      "reasoning": "The upright turquoise cylindrical structure is clearly visible in the center of the image, extending vertically from a seabed pipe. Its base—where it connects to the pipe—is partially visible but currently at a slight distance and slightly below the robot’s current viewpoint. To inspect the base closely, I need to: (1) move forward toward the structure to reduce distance, (2) descend slightly to align with the connection point (since the base is near the seabed), and (3) possibly adjust laterally if needed for optimal viewing angle. The surrounding area appears clear of immediate obstacles, though a diver and other equipment are present to the left—so maintain safe clearance. The lighting from above creates backscatter, but the target is well-lit and identifiable.",
      "action": [
        "Go straight toward the turquoise cylinder",
        "Descend slightly to align with the base connection",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0037_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged pipe structure and inspect the flexible blue corrugated hose connected to its end.",
    "output": {
      "reasoning": "The image shows a large submerged pipe structure lying horizontally on the seabed, with a flexible blue corrugated hose visibly connected to its right end (from the robot’s current perspective). The robot appears to be positioned slightly above and to the left of the structure, with clear visibility of the hose. To inspect the hose connection, the robot must first approach the structure laterally to align with the hose, then move closer for detailed inspection. Since the hose extends toward the foreground/right, the optimal path is: (1) move forward slightly to reduce distance, (2) adjust rightward to center on the hose connection point, (3) advance slowly to get within inspection range without disturbing sediment or risking collision.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/0075.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green structure in the center while avoiding the overhanging rock arch on the left.",
    "output": {
      "reasoning": "The glowing green structure is centrally located and clearly visible. An overhanging rock arch occupies the left side of the frame, extending inward from the left wall — posing a collision risk if the robot moves too far left. To safely approach the central structure, the robot should maintain a right-of-center trajectory. From the current viewpoint, the structure is directly ahead but slightly to the right; thus, initial slight right correction ensures clearance from the left arch, followed by steady forward motion toward the target.",
      "action": [
        "Turn slightly right",
        "Go straight toward the glowing green structure",
        "Maintain slight right bias to avoid the left rock arch"
      ]
    },
    "images": [
      "./data/images/temp_90_0025_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small dark cylindrical object on the seabed near the center of the frame, while avoiding the surrounding coral formations.",
    "output": {
      "reasoning": "From the image, a small dark cylindrical object is visible on the seabed near the center of the frame, slightly below the horizontal midline. It appears to be surrounded by coral formations: a large overhanging structure on the upper left, dense coral clusters on the lower left and right, and a prominent vertical coral formation on the far right. To approach the target safely, the robot should first move forward cautiously while staying centered to avoid lateral corals. As it gets closer, minor adjustments may be needed—slight left or right corrections—to navigate through the narrow gaps between coral structures. The path appears clearest directly ahead toward the center, so initial motion should be straight, then fine-tune as proximity increases.",
      "action": [
        "Go straight slowly",
        "Slightly adjust right to avoid left coral",
        "Continue forward toward the dark cylindrical object",
        "Fine-tune left if needed to clear right-side coral",
        "Stop when close to the object"
      ]
    },
    "images": [
      "./data/images/0069.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow ring-shaped object on the seabed to the right of the central pipe structure.",
    "output": {
      "reasoning": "The image shows a central vertical pipe structure. To its right, on the seabed, lies a yellow ring-shaped object (highlighted by a yellow oval in the image). The robot is currently facing the pipe, so to approach the yellow ring, it must first turn right to align its heading toward the ring, then move forward (go straight) to close the distance while maintaining orientation toward the target. No immediate obstacles block the path to the ring.",
      "action": [
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_90_0017_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large central column and inspect its surface for attached organisms or structural damage.",
    "output": {
      "reasoning": "The image shows a large central column dominating the scene, positioned directly ahead of the robot's current viewpoint. It is surrounded by coral formations on the left and smaller rocks to the right and rear. The column’s surface appears textured with possible biofouling (e.g., barnacles or algae) and irregularities that may indicate structural wear. To inspect it thoroughly, the robot must first approach closely while maintaining a safe distance from surrounding obstacles (coral on left, rocks on right). Given the column is centered and relatively unobstructed in front, the optimal path is to advance straight toward its midsection, then potentially circle or hover for detailed observation. For this initial phase, the priority is safe forward movement toward the column.",
      "action": [
        "Go straight toward the central column",
        "Maintain slight clearance from left-side coral formation",
        "Stop at ~2 meters from column base for initial visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0029_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submerged vehicle and inspect its side windows for damage or marine growth.",
    "output": {
      "reasoning": "The green submerged vehicle is clearly visible in the center-left of the image, resting on the seabed at a slight angle. Its side windows are facing roughly toward the camera’s current position, but the robot appears to be positioned slightly ahead and to the left of the vehicle’s midsection. To properly inspect the side windows (which run along the hull), the robot should first approach closer along the vehicle’s longitudinal axis, then align parallel to the side to enable a clear, close-range visual inspection. There are no immediate obstacles blocking the path; the orange vertical structures are behind the vehicle and not in the direct approach corridor. Therefore, the plan is: (1) move forward to reduce distance, (2) adjust laterally to align with the side windows, (3) proceed slowly alongside to inspect each window.",
      "action": [
        "Go straight toward the green vehicle",
        "Move right to align parallel with the side windows",
        "Slowly advance alongside the vehicle to inspect each side window"
      ]
    },
    "images": [
      "./data/images/temp_90_0009_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine and inspect its hull near the attached chain and netting.",
    "output": {
      "reasoning": "The submarine is visible in the center-right of the image, oriented horizontally. A thick chain and netting are attached above and slightly ahead of the submarine’s midsection. The robot is currently positioned below and to the left of the submarine, with coiled ropes in the foreground indicating proximity but requiring careful navigation. To inspect the hull near the chain and netting, the robot must first approach the submarine laterally from the left toward its midsection, then adjust upward and forward to reach the attachment point without entanglement. Given the lighting and depth, maintaining a steady, slow approach is critical for visual clarity and safety.",
      "action": [
        "Move right to align with the submarine's midsection",
        "Advance forward slowly toward the submarine",
        "Ascend slightly to position near the hull at chain attachment level",
        "Hover and rotate gently to inspect the hull near the chain and netting"
      ]
    },
    "images": [
      "./data/images/temp_90_0035_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-green coral formation in the foreground while maintaining a safe distance from the large textured rock structure on the left.",
    "output": {
      "reasoning": "The image shows the robot (submersible) positioned with a large textured rock structure on the left and a yellow-green coral formation in the foreground to the right. The coral is clearly visible and relatively close, but the robot must avoid drifting toward the left-side rock. To safely approach the coral, the robot should first slightly turn right to align its heading toward the coral while keeping clearance from the rock. Then, proceed forward cautiously, monitoring proximity to both the rock (left) and the coral (front/right) to maintain safe distance—especially since the coral is in the foreground and could be reached quickly. No immediate obstacles block the path to the coral once aligned.",
      "action": [
        "Turn slightly right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_90_0057_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater vehicle lying horizontally among the coral formations and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The large cylindrical underwater vehicle is lying horizontally on the seabed, partially surrounded by coral formations. It is positioned centrally in the image, slightly angled toward the right, with visible surface features (e.g., rivets, panels) suggesting potential areas for marine growth or damage inspection. The robot appears to be at a moderate distance (~2–3 m) and slightly above/forward of the vehicle. To approach safely without disturbing coral or risking collision, I will: (1) descend gently to align with the vehicle’s midsection height; (2) move forward slowly along its longitudinal axis; (3) maintain a safe lateral offset (~0.5 m) to avoid contact with coral; (4) once near the midsection, begin a slow lateral sweep to inspect surface details. No immediate obstacles block the direct path, but careful navigation is needed due to nearby coral.",
      "action": [
        "Descend slightly to match the vehicle's vertical level",
        "Move forward slowly toward the midsection of the cylindrical vehicle",
        "Maintain a safe distance (~0.5 m) from surrounding coral",
        "Pause near the midsection and begin a slow left-to-right scan of the surface"
      ]
    },
    "images": [
      "./data/images/8_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange cylindrical pipe lying on the seabed and inspect its open end for blockages or damage.",
    "output": {
      "reasoning": "The image shows a large orange cylindrical pipe lying horizontally on the seabed, with its open end facing roughly toward the camera's current position. The robot appears to be at a moderate distance (~2–3 m) and slightly elevated above the seabed. To inspect the open end for blockages or damage, the robot must: (1) approach straight toward the pipe’s open end while maintaining alignment, (2) reduce distance carefully to avoid disturbing sediment or colliding, and (3) position itself directly in front of the opening at close range (~0.5–1 m) for clear visual inspection. No immediate obstacles block the direct path; minor seabed irregularities are present but navigable. The pipe’s orientation suggests turning is unnecessary—only forward motion with slight depth adjustment is needed.",
      "action": [
        "Go straight toward the open end of the orange pipe",
        "Descend slightly to align with the pipe's centerline",
        "Continue forward slowly until ~0.5 meters from the pipe opening",
        "Hover steadily and orient camera directly into the pipe opening for inspection"
      ]
    },
    "images": [
      "./data/images/6_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the larger submerged vehicle with the glowing blue roof and inspect its front section for damage or marine growth.",
    "output": {
      "reasoning": "The image shows two submerged vehicles: a larger one on the left with a distinct glowing blue roof, and a smaller, more damaged one on the right. The target is the larger vehicle with the blue roof. From the robot’s current perspective (likely near center or slightly right), the larger vehicle is positioned to the left and slightly forward. To approach it, the robot should first move leftward to align laterally, then advance straight toward its front section. Once near, it should position itself directly in front for inspection—ensuring stable orientation and sufficient proximity to observe damage or marine growth on the front hull.",
      "action": [
        "Turn left",
        "Go straight toward the larger vehicle",
        "Stop in front of the vehicle",
        "Hover and inspect the front section"
      ]
    },
    "images": [
      "./data/images/10_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient stone column and inspect the glowing blue orb embedded at its top.",
    "output": {
      "reasoning": "The image shows a single ancient stone column centered in the frame, with a glowing blue orb clearly visible at its top. The robot appears to be positioned directly in front of the column at a moderate distance, with clear visibility and no obstacles between it and the target. To inspect the orb closely, the robot should first move forward toward the base of the column, then possibly adjust vertically or tilt upward to focus on the orb at the top. Since the orb is at the top and the column is tall, after reaching proximity, a slight upward orientation adjustment may be needed for optimal inspection.",
      "action": [
        "Go straight toward the stone column",
        "Stop near the base of the column",
        "Tilt camera upward to focus on the glowing blue orb"
      ]
    },
    "images": [
      "./data/images/7_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green core while maintaining a safe distance from the surrounding spiky green and yellow structures.",
    "output": {
      "reasoning": "The image shows a symmetrical, radial structure with a bright glowing green core at the center. Surrounding it are concentric layers of spiky green and yellow structures that appear hazardous. The robot is currently positioned directly facing the core (central perspective), suggesting forward motion is possible—but must be carefully controlled to avoid contact with the spikes. Since the spikes protrude radially outward, maintaining alignment with the central axis minimizes risk. A safe approach strategy is: (1) begin by moving straight toward the core while monitoring proximity to inner spike tips, (2) stop before entering the immediate spike zone (i.e., when the core fills ~30–40% of the frame), ensuring a buffer distance. No lateral correction is needed due to symmetry and centered framing.",
      "action": [
        "Go straight slowly",
        "Stop when the green core occupies about one-third of the image height"
      ]
    },
    "images": [
      "./data/images/4_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure on the left with the circular openings and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows an underwater scene with several green cylindrical structures featuring circular openings. The largest such structure is prominently positioned on the left side of the frame, partially illuminated by sun rays from above. It has a textured surface (possibly encrusting organisms) and visible circular apertures. To approach it for inspection, the robot must first navigate toward the left while maintaining a safe distance to avoid collision, then move closer to inspect its surface. Given the current perspective, the robot appears to be slightly in front and to the right of the target structure. Therefore, the plan is: (1) turn left to align with the structure, (2) advance straight toward it, (3) close in for detailed surface inspection (maintaining stable hover or slow forward motion).",
      "action": [
        "Turn left",
        "Go straight toward the large green cylindrical structure",
        "Approach closely and hover to inspect the surface"
      ]
    },
    "images": [
      "./data/images/1_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green mechanical structure on the seabed and inspect its central cylindrical component.",
    "output": {
      "reasoning": "The image shows a large green mechanical structure on the seabed, centrally located and clearly visible. It has a prominent central cylindrical component with radial supports and circular ports—this is the inspection target. The robot (submersible) is currently positioned above and slightly to the left of the structure, at a moderate distance. To approach and inspect the central cylinder, I need to: (1) descend vertically toward the seabed to reach the structure’s level; (2) move forward (straight) to close the horizontal distance; (3) fine-tune position to center on the cylindrical core; (4) possibly rotate slightly to align the camera for optimal inspection view. No immediate obstacles block the direct path, though caution is needed near the surrounding spiky organisms and debris. Lighting from above helps visibility.",
      "action": [
        "Descend vertically",
        "Go straight toward the green structure",
        "Move closer to the central cylindrical component",
        "Adjust position to center on the cylinder",
        "Rotate slightly to face the cylinder directly"
      ]
    },
    "images": [
      "./data/images/1_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submerged vehicle and inspect its side windows for damage or marine growth.",
    "output": {
      "reasoning": "The green submerged vehicle is clearly visible in the center-left of the image, lying horizontally on the seabed. Its side windows are facing upward and slightly toward the camera. The robot appears to be positioned at a moderate distance (~2–3 m) and slightly ahead of the vehicle’s midsection. To inspect the side windows, the robot must first approach closer (to ~1 m) for high-resolution visual inspection, then align parallel to the vehicle’s longitudinal axis to view all side windows systematically. There are no immediate obstacles blocking the path; orange buoys are anchored above but not interfering with horizontal movement near the seabed. Recommended sequence: (1) move forward to reduce distance, (2) adjust lateral position to align with the vehicle’s side, (3) proceed slowly alongside to inspect each window.",
      "action": [
        "Go straight toward the green vehicle",
        "Move slightly left to align with the vehicle's side",
        "Proceed slowly alongside the vehicle to inspect side windows"
      ]
    },
    "images": [
      "./data/images/6_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central vortex-like structure while maintaining a safe distance from its textured ridges.",
    "output": {
      "reasoning": "The image shows a central vortex-like structure with concentric, textured ridges radiating outward. The robot appears to be positioned directly in front of the vortex, aligned with its axis. To approach the center while maintaining a safe distance from the ridges, the robot should advance slowly along the central axis—where ridges are furthest apart and least likely to cause collision. The bright light source at the far right suggests a directional reference, but the primary goal is axial approach. Since the structure is symmetric and the current view is centered, no lateral correction is needed initially. A controlled forward motion will bring the robot closer while preserving clearance from the ridged periphery.",
      "action": [
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/4_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge located on the seabed near the cluster of dark rocks in the foreground.",
    "output": {
      "reasoning": "The yellow sponge is not clearly visible in the current frame, but the instruction specifies it is on the seabed near a cluster of dark rocks in the foreground. In the image, there is a prominent cluster of dark rocks at the bottom right and center foreground. The seabed is visible and relatively flat there. To approach the sponge, I need to first move toward that rock cluster. Since the robot appears to be positioned slightly left and above the target area (based on perspective), the most direct path is to go straight ahead while slightly adjusting right to align with the rock cluster. Once closer, fine positioning can occur—but for now, the atomic step is to advance toward the foreground rocks.",
      "action": [
        "Go straight",
        "Slightly turn right"
      ]
    },
    "images": [
      "./data/images/3_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its upper ornamental structure for signs of marine growth or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned slightly to the right of center and relatively close to the robot’s current position. The upper ornamental structure (capitals with volutes) is illuminated by overhead light, making it inspectable. To inspect for marine growth or damage, the robot must first approach the column closely enough for high-resolution observation—ideally within 1–2 meters—and then orient its camera upward to focus on the upper ornamental section. Given the open floor space and absence of immediate obstacles, a direct forward movement followed by a slight upward tilt is sufficient. No turning is needed since the column is already in the forward field of view.",
      "action": [
        "Go straight toward the central stone column",
        "Stop approximately 1.5 meters from the base",
        "Tilt camera upward to focus on the upper ornamental structure"
      ]
    },
    "images": [
      "./data/images/2_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-tipped coral formation in the foreground while maintaining clearance from the large dark structure above.",
    "output": {
      "reasoning": "The yellow-tipped coral formation is visible in the foreground, lower-right portion of the image (appearing as pale, branching corals with yellow tips). A large dark structure (likely a rock or overhang) dominates the upper part of the scene, casting shadows and posing a collision risk if the robot ascends. The robot (submarine-like vehicle) is currently positioned mid-water, slightly left of center, oriented horizontally. To approach the coral safely: (1) First, maintain current depth or slightly descend to increase vertical clearance from the overhead structure; (2) Then, move forward while gently steering right toward the coral cluster; (3) Avoid upward pitch to prevent encroaching on the dark structure. The coral is close enough that only short, controlled movements are needed.",
      "action": [
        "Descend slightly",
        "Go straight",
        "Turn right gently",
        "Approach the yellow-tipped coral"
      ]
    },
    "images": [
      "./data/images/8_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small green spherical organism floating near the center and inspect it closely.",
    "output": {
      "reasoning": "The small green spherical organism is visible near the center of the image, slightly above the seabed and suspended in the water column. It appears to be tethered by a thin filament. The robot is currently positioned below and somewhat behind it (from the camera perspective), with several large green cup-shaped organisms in the foreground and midground. To approach it closely for inspection, the robot should first move forward toward the center while maintaining depth to avoid colliding with foreground structures. Then, a slight upward adjustment may be needed to align with the organism’s vertical position. Finally, a slow advance to close proximity will enable detailed inspection.",
      "action": [
        "Go straight toward the center of the image",
        "Slightly ascend to match the organism's depth",
        "Slowly approach the green spherical organism"
      ]
    },
    "images": [
      "./data/images/5_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the underwater cave opening toward the sunlit area while avoiding the large rocks on the seabed.",
    "output": {
      "reasoning": "The image shows an underwater cave with a visible sunlit exit ahead, framed by the cave arch. Large rocks are scattered on the seabed, particularly in the lower half of the frame—some near the center and right. The robot is positioned inside the cave, facing the opening. To safely navigate toward the sunlit area: (1) First, maintain forward motion while staying slightly elevated to avoid seabed rocks; (2) Observe that the clearest path appears to be slightly left of center, where fewer rocks obstruct the way and light rays indicate open water; (3) Adjust heading gently left to steer clear of the central and right-side rock clusters; (4) Continue forward toward the brightest region (sunlit exit), monitoring for new obstacles as depth changes.",
      "action": [
        "Go straight while maintaining slight altitude",
        "Gently turn left to avoid central seabed rocks",
        "Proceed forward toward the sunlit opening",
        "Monitor for obstacles and adjust heading as needed"
      ]
    },
    "images": [
      "./data/images/3_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater passage between the left rock wall and the central coral-covered formation, heading toward the sunlit opening ahead.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage bounded on the left by a dark rock wall and on the right by a large, irregular coral-covered formation. Sunlight streams in from an opening ahead, indicating the goal direction. To safely navigate this constrained corridor, the robot must maintain central alignment—avoiding collision with either side—while progressing forward toward the light. Given the perspective, the passage appears to widen slightly ahead, so a steady forward motion with minor lateral corrections is appropriate. No immediate obstacles block the path, but precise heading control is critical due to the tight clearance.",
      "action": [
        "Go straight slowly while maintaining center alignment between left rock wall and right coral formation",
        "Slightly adjust left if nearing right coral, or right if nearing left rock wall",
        "Continue forward toward the sunlit opening ahead"
      ]
    },
    "images": [
      "./data/images/9_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate upward toward the illuminated surface opening while avoiding the overhead mesh structure.",
    "output": {
      "reasoning": "The robot is currently oriented upward, with a bright illuminated surface opening visible ahead and slightly above. An overhead mesh structure looms directly above the robot, partially obstructing the direct vertical path. To avoid collision, the robot must ascend while laterally shifting—preferably to the right (from its perspective), where the mesh appears less dense and more open space is visible. The yellow lights on the robot’s sides suggest maneuverability; turning slightly right before ascending ensures clearance from the mesh. Once clear of the mesh, continue straight upward toward the light source.",
      "action": [
        "Turn slightly right",
        "Ascend steadily",
        "Maintain slight rightward bias until clear of mesh",
        "Proceed straight upward toward illuminated opening"
      ]
    },
    "images": [
      "./data/images/10_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the textured blue coral-like structure resting on the circular rock base and inspect its surface details.",
    "output": {
      "reasoning": "The target is a textured blue coral-like structure sitting on a circular rock base, clearly visible in the center of the image. The robot appears to be positioned slightly in front and possibly elevated relative to the target. To inspect surface details, the robot must first approach closely without disturbing the structure, then position itself for optimal viewing—likely at a slight angle or directly in front, maintaining stable distance. Given the lighting from above-left, approaching from the front-right may minimize shadow interference. No immediate obstacles block the path. Step-by-step: (1) Move forward toward the structure to reduce distance; (2) Fine-tune position to center the structure in view; (3) Slightly descend or adjust pitch to get closer for surface inspection while avoiding contact.",
      "action": [
        "Go straight toward the blue coral-like structure",
        "Slow down and center the structure in the field of view",
        "Gently descend and move closer for detailed surface inspection"
      ]
    },
    "images": [
      "./data/images/7_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical pipe-like structure emitting bubbles in the center of the underwater cave while avoiding the surrounding rock formations.",
    "output": {
      "reasoning": "The image shows an underwater cave with a central vertical pipe-like structure emitting a stream of bubbles, clearly visible along the light ray from above. The structure is flanked by irregular rock formations on both left and right sides, forming a narrow passage. To approach it safely while avoiding collisions, the robot should first align itself centrally in the passage, then proceed forward cautiously. Since the pipe is directly ahead and centered in the field of view, no lateral correction is needed initially. However, due to potential drift or uneven terrain, small adjustments may be required mid-approach. The safest sequence is: (1) maintain centerline heading, (2) advance steadily toward the bubble column, (3) fine-tune position if rocks encroach laterally.",
      "action": [
        "Go straight",
        "Maintain center alignment",
        "Proceed forward toward bubble stream",
        "Slightly adjust left or right if nearing rock walls"
      ]
    },
    "images": [
      "./data/images/9_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green sponge structure on the right while avoiding the dark overhanging coral formation above.",
    "output": {
      "reasoning": "From the image, the large green sponge structure is located on the right side of the scene, appearing as a textured, porous green mass near the bottom. Above it and slightly to the left is a dark, overhanging coral formation that extends downward — this poses a collision risk if the robot ascends or moves too directly upward/rightward. The robot’s current position appears to be slightly left and below the target sponge, with open water in front and to the right at a lower depth. To safely approach: (1) First, move forward slightly to gain horizontal proximity while staying below the overhang; (2) Then, gently turn right to align toward the sponge; (3) Finally, advance carefully toward the sponge while maintaining depth to avoid the overhead coral. No upward motion is needed—staying low ensures clearance.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight toward the green sponge"
      ]
    },
    "images": [
      "./data/images/5_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green SUV and inspect its front right wheel for damage or entanglement.",
    "output": {
      "reasoning": "The green SUV is visible in the center-left of the image, lying on the seabed at a slight angle. Its front right wheel is partially visible and appears accessible from the current viewpoint. To inspect it closely, I need to approach the vehicle from the front-right side. First, I should move forward toward the SUV while maintaining a safe distance to avoid disturbing sediment. Then, I will reposition to the front-right quadrant of the vehicle to get a clear view of the target wheel. Finally, I will move closer for detailed inspection.",
      "action": [
        "Go straight toward the green SUV",
        "Turn right to position yourself at the front-right side of the SUV",
        "Move forward slowly to approach the front right wheel",
        "Hover and inspect the front right wheel for damage or entanglement"
      ]
    },
    "images": [
      "./data/images/2_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate between the two large stone columns toward the sunlit opening in the background.",
    "output": {
      "reasoning": "The image shows two large stone columns framing a central passage, with a sunlit opening visible in the background through the water. The robot is positioned in front of the columns, facing them. To navigate toward the sunlit opening, it must proceed straight through the gap between the two columns. There are no immediate obstacles in the passage, and the lighting suggests the opening is directly ahead. Therefore, the plan is: (1) align with the central gap, (2) move forward steadily while maintaining centerline, (3) continue until nearing the opening.",
      "action": [
        "Go straight toward the gap between the two stone columns",
        "Maintain center alignment between the columns",
        "Continue forward toward the sunlit opening"
      ]
    },
    "images": [
      "./data/images/0116.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large moss-covered stone pillar on the left and inspect its surface for embedded artifacts or markings.",
    "output": {
      "reasoning": "The image shows a large, moss-covered stone pillar on the left side of the frame, illuminated by light rays from above. It is the most prominent structure and matches the description in the instruction. The robot appears to be positioned slightly in front and to the right of the pillar, with clear space to maneuver toward it. To approach the pillar for inspection, the robot should first move leftward to align with its base, then advance straight toward it while maintaining proximity for surface inspection. No immediate obstacles block the path to the pillar’s left side.",
      "action": [
        "Turn left",
        "Move forward toward the moss-covered stone pillar",
        "Get closer to the pillar's surface for detailed inspection"
      ]
    },
    "images": [
      "./data/images/0302.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical pipe structure with attached sensors and inspect its lower junction with the green grid platform.",
    "output": {
      "reasoning": "The image shows a vertical pipe structure with attached sensors positioned centrally above a green grid platform. The lower junction—where the pipe meets the platform—is visible but slightly distant and angled. To inspect it closely, the robot must first approach the structure while maintaining alignment with the junction. Given the current perspective, moving straight forward will reduce distance to the pipe; then a slight downward tilt or descent may be needed to focus on the lower junction. No immediate obstacles block the direct path, so a straightforward approach is safe.",
      "action": [
        "Go straight toward the vertical pipe",
        "Descend slightly to align with the lower junction",
        "Hover and inspect the junction between pipe and green grid platform"
      ]
    },
    "images": [
      "./data/images/0100.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green segmented pipe lying on the seabed and inspect its surface for anomalies.",
    "output": {
      "reasoning": "The large green segmented pipe lies horizontally on the seabed, occupying the lower-right to center portion of the image. The robot is currently positioned above and slightly ahead of the pipe, facing it. To approach and inspect its surface, I need to: (1) descend vertically to reach seabed level near the pipe; (2) move forward slowly toward the nearest segment (front tip of pipe); (3) once close, perform a lateral sweep (e.g., move right or left along the pipe) to inspect surface anomalies. Given the pipe’s orientation (pointing left-to-right), approaching from the front and then moving right along its length is optimal for full visual coverage. No immediate obstacles block the path.",
      "action": [
        "Descend to seabed level",
        "Move forward toward the front tip of the green pipe",
        "Move right along the pipe while maintaining close proximity",
        "Inspect surface for anomalies during lateral movement"
      ]
    },
    "images": [
      "./data/images/temp_307_0045_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central gray rock with yellow bands and inspect its top surface for embedded sensors or markings.",
    "output": {
      "reasoning": "The image shows multiple rocks with yellow bands; the central gray rock with yellow bands is clearly visible in the foreground, slightly left of center, and appears to be the target. It has a distinct textured top surface with circular features that may be sensors or markings. To inspect its top surface, I need to first approach it closely (maintaining safe distance to avoid disturbance), then position directly above it for optimal overhead view. The seabed is relatively flat and clear around it, allowing straight-line approach. Since the robot is likely facing the scene from a slight angle, the first step is to move forward toward the rock, then fine-tune position to center it in the field of view before ascending slightly for top-down inspection.",
      "action": [
        "Go straight toward the central gray rock with yellow bands",
        "Stop when ~1 meter away",
        "Adjust position to center the rock in the camera view",
        "Slowly ascend to hover directly above the rock",
        "Inspect the top surface for embedded sensors or markings"
      ]
    },
    "images": [
      "./data/images/temp_307_0080_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine from below while maintaining a safe distance and aligning with its port side for inspection.",
    "output": {
      "reasoning": "The green submarine is positioned horizontally in the mid-water column, slightly above the seabed. From the current viewpoint (below and slightly behind), the robot needs to: (1) ascend gently to approach from below without colliding with the seabed or coral; (2) move forward while maintaining vertical clearance beneath the submarine; (3) adjust lateral position to align with the port (left) side of the submarine as viewed from its bow direction — since the submarine’s bow points rightward in the image, its port side is the left-facing hull; (4) hold position at a safe distance for inspection. The lighting suggests upward visibility is good, supporting controlled ascent and alignment.",
      "action": [
        "Ascend slowly to position directly below the submarine",
        "Move forward cautiously while staying beneath the submarine",
        "Drift left to align with the submarine's port side",
        "Maintain steady depth and distance for inspection"
      ]
    },
    "images": [
      "./data/images/0168.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge cluster on the seabed while maintaining a safe distance from the large dark coral structure above.",
    "output": {
      "reasoning": "From the image, the yellow sponge cluster is visible on the seabed in the lower-right foreground. A large dark coral structure looms above and slightly behind the robot’s current position (center-top of frame), posing a potential collision risk if the robot ascends or drifts upward. The robot is currently oriented horizontally, facing roughly forward-left. To approach the sponge cluster safely: (1) First, maintain depth to avoid the overhead coral—so no upward movement; (2) The sponge cluster lies to the right and slightly ahead; thus, a gentle right turn is needed to align heading toward it; (3) Then proceed forward at low altitude (just above seabed) to close distance while keeping clear of the coral overhead. No sharp maneuvers are required—the path appears open laterally and downward.",
      "action": [
        "Turn right slightly",
        "Go straight slowly while maintaining current depth"
      ]
    },
    "images": [
      "./data/images/temp_307_0011_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine positioned at the center of the cave opening, navigating carefully around the surrounding rocks and sediment.",
    "output": {
      "reasoning": "The submarine is clearly visible at the center of the cave opening, illuminated by light rays from above. The robot is currently outside the cave, facing the entrance. There are rocks and sediment on both left and right sides near the entrance, requiring careful lateral positioning before advancing. To safely approach: (1) First, align centrally with the submarine by moving slightly right to avoid the larger rock mass on the left foreground; (2) Then proceed straight forward through the cleared central corridor; (3) Finally, slow down and fine-tune position as it nears the submarine to avoid collision with sediment or hull features.",
      "action": [
        "Move slightly right to center alignment with the submarine",
        "Go straight toward the cave opening",
        "Advance slowly toward the submarine while maintaining central trajectory",
        "Hover gently in front of the submarine"
      ]
    },
    "images": [
      "./data/images/temp_307_0056_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater structure with green lighting and inspect its base where it connects to the seabed covered in green tubular organisms.",
    "output": {
      "reasoning": "The image shows a tall cylindrical underwater structure illuminated with green lights, positioned centrally. At its base, the seabed is densely covered with green tubular organisms (likely sponges or tunicates). The robot appears to be at a moderate distance and slightly below the structure’s midsection, facing it. To inspect the base where the structure meets the seabed, the robot must: (1) move forward toward the structure to reduce distance, (2) descend vertically to reach the seabed level, and (3) position itself close and stable for inspection—ideally slightly offset to avoid occlusion by the structure’s shadow or protrusions. Given the current perspective, a straight approach followed by a controlled descent is optimal; no immediate obstacles block the path.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Descend slowly to align with the seabed level",
        "Move slightly forward and adjust position to center the base in view",
        "Hold position for inspection of the connection zone"
      ]
    },
    "images": [
      "./data/images/0243.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange underwater robot and inspect its front sensors while avoiding the coral formations on the left.",
    "output": {
      "reasoning": "The orange underwater robot is directly ahead and slightly above the current viewpoint, with visible front sensors (two circular elements). Coral formations occupy the left side of the scene, requiring the robot to maintain a rightward bias during approach. To safely inspect the front sensors, I need to: (1) first steer right to clear the coral, (2) then advance straight toward the robot while maintaining safe lateral distance, and (3) finally position closely in front for sensor inspection without colliding. The robot appears stationary, so a controlled, steady approach is feasible.",
      "action": [
        "Turn right slightly",
        "Go straight toward the orange robot",
        "Move forward slowly to position in front of the robot",
        "Hover and inspect the front sensors"
      ]
    },
    "images": [
      "./data/images/0102.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its carved details while avoiding the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible in the image, standing upright near the center of the frame. Scattered debris (flat circular objects and broken blocks) lies on the seabed around its base, especially to the left and right front. To approach safely while avoiding debris, I should first align the robot directly toward the pillar from a safe distance, then advance cautiously along a central path where the seabed appears clearest (midway between left and right debris clusters). Once near, I’ll position for close inspection—slightly offset to the front-right to get optimal lighting and view of carved details without disturbing sediment. The debris does not block the direct frontal approach, but lateral deviations are needed during final approach to avoid contact.",
      "action": [
        "Turn slightly left to center the pillar in the field of view",
        "Go straight slowly toward the pillar, maintaining central alignment",
        "Adjust right by 5 degrees to avoid foreground circular debris",
        "Continue forward at reduced speed until 2 meters from base",
        "Hover and rotate slowly clockwise to inspect carved details on front-right face"
      ]
    },
    "images": [
      "./data/images/0285.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater device on the seabed while avoiding the red pipe in the foreground.",
    "output": {
      "reasoning": "The green underwater device is visible on the seabed, positioned slightly to the left and behind the red pipe in the foreground. The red pipe spans horizontally across the lower part of the image and blocks a direct forward path. To avoid collision, I must first move laterally away from the pipe—specifically, to the left (since the green device is left-of-center relative to the pipe’s obstruction), then proceed forward toward the device once clear. The device appears stationary and stable on the seabed, so a controlled approach is feasible after clearing the obstacle.",
      "action": [
        "Turn left",
        "Move forward slowly",
        "Adjust right slightly to align with green device",
        "Approach green underwater device"
      ]
    },
    "images": [
      "./data/images/0153.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large yellowish pipeline lying horizontally above the coral reef, maintaining a safe distance to inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The large yellowish pipeline is visible diagonally across the upper right portion of the image, lying horizontally above a coral reef. The robot appears to be positioned below and slightly to the left of the pipeline, with clear water ahead. To approach it safely while maintaining inspection distance, I should first move forward toward the pipeline’s midsection (which is most accessible), then adjust laterally to align parallel to it at a safe standoff distance (~1–2 m). Since the pipeline slopes slightly downward from left to right in the frame, approaching from the lower-left ensures minimal risk of collision with reef structures below. No immediate obstacles block the path forward.",
      "action": [
        "Go straight slowly",
        "Slightly ascend to match pipeline elevation",
        "Drift right to align parallel to pipeline at ~1.5 m distance",
        "Maintain steady forward motion along pipeline for inspection"
      ]
    },
    "images": [
      "./data/images/0205.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater device with coiled orange cables and inspect its connection point for entanglement or damage.",
    "output": {
      "reasoning": "The green cylindrical device with coiled orange cables is clearly visible in the image, positioned slightly to the right and ahead of the robot’s current viewpoint. Sunlight penetrates from above, illuminating the scene and revealing the connection point near the left end cap of the cylinder (closest to the robot). The cables appear tangled around the midsection, but the critical inspection zone is the connection interface—likely where the green cap meets the cylinder body. To inspect for entanglement or damage, I must first approach closely without disturbing the cables, then orient the camera to focus on the connection point. Given the current distance (~1.5–2 m estimated), a controlled forward movement is needed, followed by a slight leftward adjustment to center the connection point in view, and finally a minor pitch-up or hover to stabilize for visual inspection.",
      "action": [
        "Go straight slowly",
        "Turn slightly left",
        "Hover and adjust position to center the green device's left-end connection point in view",
        "Inspect the connection point for entanglement or damage"
      ]
    },
    "images": [
      "./data/images/0174.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-green submersible vehicle ahead while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The yellow-green submersible is directly ahead, centered in the corridor formed by rock formations on both sides. To approach it safely, I must maintain central alignment to avoid collisions with the left and right rocks. The current view shows sufficient clearance, so the first step is to move forward steadily while monitoring proximity to the rocks. As the vehicle gets closer, minor lateral corrections may be needed, but initially, a straight advance is optimal. No immediate obstacles block the direct path.",
      "action": [
        "Go straight",
        "Maintain center alignment",
        "Monitor distance to left and right rock formations"
      ]
    },
    "images": [
      "./data/images/0227.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue and orange underwater robot directly ahead while maintaining a safe distance from the surrounding rocky seabed and coral formations.",
    "output": {
      "reasoning": "The target—a blue and orange underwater robot—is directly ahead in the center of the frame, clearly visible under sunbeams. The seabed is uneven with rocky outcrops and coral formations on both left and right sides, requiring lateral clearance. To approach safely: (1) Maintain forward motion while keeping central alignment to avoid drifting toward obstacles; (2) Slightly elevate or hold depth to prevent contact with seabed features; (3) Monitor distance—stop before entering close proximity (<1 m) unless instructed otherwise. Since the robot is stationary and centered, a straight-forward approach with minor stabilization is sufficient.",
      "action": [
        "Go straight slowly",
        "Maintain central heading",
        "Hold depth slightly above seabed",
        "Stop at safe distance (~2 meters)"
      ]
    },
    "images": [
      "./data/images/0221.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green vertical structure while avoiding the nearby net and cables on the seafloor.",
    "output": {
      "reasoning": "The glowing green vertical structure is the primary target, centrally located and highly visible. To its left (from the robot’s perspective) lies a large net on the seafloor, and thin cables extend across the bottom near the base of the structure—especially visible to the right and front. To safely approach, the robot should first move forward cautiously while staying slightly to the right of center to avoid the net on the left. Then, as it nears the base, it must adjust laterally to steer clear of the cables extending from the right side and front. The final step is to position directly in front of the structure at a safe distance for observation or interaction.",
      "action": [
        "Go straight while slightly veering right to avoid the net on the left",
        "Continue forward, maintaining clearance from the cables on the seafloor ahead and to the right",
        "Adjust left slightly to center in front of the glowing green structure",
        "Stop at a safe distance in front of the structure"
      ]
    },
    "images": [
      "./data/images/0289.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe in the foreground while maintaining a safe distance from the overhead net structure.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe in the foreground, extending diagonally from bottom-left toward upper-right. An overhead net structure looms above and slightly ahead of the robot’s current position. To approach the pipe safely, the robot must avoid ascending too close to the net (which is directly above the pipe’s midsection). The pipe is accessible from its lower-left side, where there is clearer space. Therefore, the plan is: (1) move forward cautiously while staying low to avoid the net; (2) slightly adjust leftward to align with the pipe’s near end; (3) continue forward along the pipe’s side at a safe lateral distance (e.g., ~0.5–1 m) to maintain clearance from both the net above and potential contact with the pipe.",
      "action": [
        "Go straight slowly while keeping depth shallow",
        "Slightly turn left to align with the pipe's near end",
        "Proceed forward alongside the pipe, maintaining lateral clearance"
      ]
    },
    "images": [
      "./data/images/temp_307_0066_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral formation from the front while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The image shows a large, prominent green coral formation centered in the frame, directly ahead of the robot's current position. The coral is well-lit and occupies the mid-to-near field, indicating it is relatively close. To approach it from the front while maintaining a safe distance, the robot should proceed straight but with controlled forward motion—avoiding sudden acceleration or lateral drift. No immediate obstacles block the frontal path, but the seabed texture and nearby smaller corals suggest cautious navigation is needed. A single 'Go straight' step is appropriate for initial approach; subsequent steps (not included here) would involve fine-tuning distance based on real-time feedback.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0040_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central vertical structure wrapped in netting and inspect its upper spherical node.",
    "output": {
      "reasoning": "The image shows a tall vertical structure wrapped in netting, with a clear upper spherical node visible at the top. The robot appears to be positioned below and slightly to the side of the structure, looking upward. Sunlight rays indicate the camera is oriented upward, suggesting the robot is already near the base or mid-section. To approach the central vertical structure and inspect its upper spherical node, the robot should first move closer horizontally toward the centerline of the structure (since it's currently off-center), then ascend vertically while maintaining alignment with the central axis to reach the spherical node. Given the perspective, a slight forward-and-up trajectory is optimal to avoid netting entanglement and maintain visual contact.",
      "action": [
        "Move forward toward the central vertical structure",
        "Ascend vertically while centering on the structure",
        "Approach the upper spherical node for close inspection"
      ]
    },
    "images": [
      "./data/images/0278.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green structure and inspect its base where the tentacle-like arms meet the seabed.",
    "output": {
      "reasoning": "The central glowing green structure is clearly visible and dominates the scene. It has a vertical main body with multiple tentacle-like arms extending outward and downward, anchoring into the seabed. The base—where these arms meet the seabed—is illuminated and accessible from the front. From the current viewpoint, the robot is positioned slightly in front and above the structure, at moderate distance. To inspect the base closely, I need to: (1) move forward toward the structure while maintaining alignment with its center; (2) descend slightly to bring the camera level with the junction of arms and seabed; (3) possibly adjust laterally if needed for optimal view, but initial approach should be straight ahead to avoid disturbing sediment or missing the target. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the central green structure",
        "Descend slightly to align camera with the base of the tentacle arms",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/0192.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible in the image, positioned slightly ahead and to the right of the robot’s current viewpoint. It stands upright on a tiled seabed, with sunlight filtering from above creating shadows and highlights that reveal surface texture. Marine growth (e.g., algae, barnacles) appears present near its base and along vertical seams; structural damage such as cracks or erosion may be discernible upon closer inspection. To inspect effectively, the robot must first approach the pillar while maintaining a stable orientation, then circle or hover near its mid-section for detailed visual assessment. Given the presence of other pillars and debris nearby, careful navigation is required to avoid collisions. The optimal path is: (1) advance straight toward the pillar’s base, (2) adjust laterally to center on the pillar, (3) ascend slightly to inspect upper-mid surface, (4) maintain close proximity (~0.5–1 m) while slowly rotating around it for 360° coverage.",
      "action": [
        "Go straight toward the central stone pillar",
        "Adjust left slightly to center on the pillar",
        "Move closer until ~1 meter away",
        "Ascend gently to align camera with mid-height of pillar",
        "Rotate slowly clockwise around the pillar for full surface inspection"
      ]
    },
    "images": [
      "./data/images/0136.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green SUV and inspect its front right wheel and undercarriage for structural damage.",
    "output": {
      "reasoning": "The green SUV is visible in the center-left of the image, resting on the seabed. Its front right wheel and undercarriage are partially visible from the current angle (robot appears to be slightly behind and to the left of the vehicle). To inspect the front right wheel and undercarriage for structural damage, I need to: (1) approach the vehicle safely—moving forward and slightly right to align with its front-right quadrant; (2) get closer to the front right wheel area; (3) position the robot low and near the undercarriage for detailed visual inspection. Obstacles include scattered debris on the seabed and a column to the right—must avoid collision. Lighting is dim but sufficient for visual inspection once close enough.",
      "action": [
        "Move forward and slightly right toward the green SUV",
        "Approach the front right wheel area while maintaining safe distance from seabed debris",
        "Descend slightly and position robot parallel to undercarriage near front right wheel",
        "Inspect front right wheel and adjacent undercarriage for cracks, deformation, or detachment"
      ]
    },
    "images": [
      "./data/images/temp_307_0006_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green spherical object with circular indentations in the center of the sandy floor.",
    "output": {
      "reasoning": "The green spherical object with circular indentations is clearly visible in the center of the sandy floor, slightly ahead and to the left of the robot’s current position. The path appears unobstructed—no red obstacles or significant terrain irregularities block direct access. To approach it efficiently: first, align the robot toward the object by turning slightly left; then proceed straight forward until close enough for interaction (e.g., grasping or inspection). Given the perspective, a small left turn (~15–20°) followed by a short forward movement should center the target in the field of view and reduce distance safely.",
      "action": [
        "Turn left slightly",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0028_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible device suspended among the vertical pipes and inspect its lower orange component.",
    "output": {
      "reasoning": "The green cylindrical submersible device is visible in the center-right of the image, suspended among vertical pipes. It has a distinct lower orange component (likely a sensor or connector) hanging below it. The robot appears to be positioned slightly below and to the left of the device, with rocky seabed and algae-covered structures in the foreground. To approach and inspect the lower orange component, the robot must: (1) move forward and slightly right to align with the device’s central axis, (2) ascend gently to reach the device’s mid-height, then (3) move closer while maintaining stable orientation to inspect the orange part from a frontal/slight-bottom angle. Obstacles (pipes and rocks) are present but there is a clear path from current position toward the device.",
      "action": [
        "Move forward and slightly right",
        "Ascend gently",
        "Approach the green cylindrical device",
        "Position for close inspection of the lower orange component"
      ]
    },
    "images": [
      "./data/images/temp_307_0085_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater instrument suspended above the coral reef and inspect its lower sensor array.",
    "output": {
      "reasoning": "The red cylindrical instrument is directly in front of the robot, suspended vertically above the reef, with its lower sensor array (dark circular housing) visible at the bottom end. The robot is currently positioned at a moderate distance and slightly below the instrument’s midsection. To inspect the lower sensor array, I need to: (1) move forward to reduce distance, (2) adjust vertical position upward slightly to align with the lower array (since the array is at the bottom of the cylinder and the robot appears slightly below center), and (3) possibly fine-tune lateral position to center on the array. Given the clear frontal view and minimal lateral offset, a straight-forward approach followed by a slight upward correction should suffice. No obstacles block the direct path.",
      "action": [
        "Go straight",
        "Move up slightly",
        "Center on the lower sensor array"
      ]
    },
    "images": [
      "./data/images/temp_307_0078_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue corrugated pipe on the seabed and inspect its surface for damage or marine growth, while avoiding the floating net above it.",
    "output": {
      "reasoning": "The image shows a large blue corrugated pipe lying horizontally on the seabed. A floating net is suspended above it, partially draped over the pipe’s left section. To inspect the pipe’s surface while avoiding the net, the robot must approach from a safe angle—likely from the front or side where the net is not directly overhead. The pipe extends diagonally from lower-left to upper-right in the frame; the clearest access appears from the front (toward the camera’s current position), then moving laterally along its length. Since the net hangs loosely and casts shadows, maintaining a slight upward clearance (but below the net) or staying close to the seabed (where the net doesn’t touch) is safer. Step-by-step: (1) Advance straight toward the pipe’s near end (front), staying low to avoid net contact; (2) Once adjacent, move parallel to the pipe (rightward, following its orientation) to inspect surface; (3) Continue inspection along the visible segment, adjusting depth if needed to keep net above.",
      "action": [
        "Go straight toward the near end of the blue pipe",
        "Move right parallel to the pipe while maintaining close proximity to seabed",
        "Continue inspecting the pipe surface along its length"
      ]
    },
    "images": [
      "./data/images/temp_307_0065_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central bioluminescent coral structure and inspect its glowing polyps for signs of activity or damage.",
    "output": {
      "reasoning": "The central bioluminescent coral structure is clearly visible and dominates the image, positioned directly ahead of the robot’s current viewpoint. Its glowing polyps are active (emitting light), suggesting biological activity, but close inspection is needed to assess for subtle signs of damage (e.g., discoloration, retraction, or irregular luminescence). The surrounding environment appears stable with no immediate obstacles blocking a direct approach. To inspect effectively, the robot should first move forward to reduce distance, then stabilize and possibly adjust orientation slightly to center the structure in view for detailed observation.",
      "action": [
        "Go straight toward the central bioluminescent coral",
        "Slow down and hover at ~0.5 m distance",
        "Adjust pitch slightly upward to align camera with polyp layer",
        "Inspect glowing polyps for uniformity, brightness, and structural integrity"
      ]
    },
    "images": [
      "./data/images/0092.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the bright central light source while maintaining a safe distance from the textured, undulating walls of the tunnel.",
    "output": {
      "reasoning": "The image shows a tunnel-like structure with a bright central light source directly ahead, indicating the target direction. The walls are textured and undulating (rippled green-blue patterns), suggesting potential collision risk if the robot drifts laterally. To maintain safety, the robot must stay centered while advancing. Since the light is aligned with the current heading, no initial turning is needed—only forward motion with minor lateral corrections if drift is detected. Given the symmetry and clear central axis, the safest strategy is to proceed straight while monitoring wall proximity.",
      "action": [
        "Go straight",
        "Maintain center alignment with the light source",
        "Adjust slightly left or right only if wall proximity decreases"
      ]
    },
    "images": [
      "./data/images/0109.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the left while avoiding the large dark rock formation on the right.",
    "output": {
      "reasoning": "The yellow sponge is visible on the left side of the image, near the bottom-left corner. A large dark rock formation occupies the right side, extending vertically and partially blocking the right path. To approach the sponge while avoiding the rock, the robot should first steer left to increase lateral clearance from the rock, then proceed forward toward the sponge. Since the sponge appears relatively close and directly accessible once aligned, a two-step plan suffices: (1) turn left to orient toward the sponge and away from the rock, (2) go straight to reach it. No upward/downward adjustment is needed—the sponge is at similar depth as the robot’s current position.",
      "action": [
        "Turn left",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0077_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the textured green column in the center while avoiding the overgrown structure to the left.",
    "output": {
      "reasoning": "The image shows two prominent vertical structures: on the left, a heavily overgrown, irregular column with dense marine growth; in the center, a smoother, textured green column with horizontal ridges—this matches the target 'textured green column in the center'. The robot appears to be positioned slightly left of center, facing forward. To approach the central column while avoiding the overgrown structure on the left, the robot should first shift rightward to create clearance, then proceed straight toward the central column. A slight right turn followed by forward motion ensures safe navigation without colliding with the left obstacle.",
      "action": [
        "Turn right slightly",
        "Go straight toward the central green column"
      ]
    },
    "images": [
      "./data/images/temp_307_0063_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged green pipe and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The large submerged green pipe is clearly visible in the center-right of the image, lying horizontally on the seabed. The robot appears to be positioned slightly to the left and front of the pipe’s near end (open circular opening). To approach and inspect its surface, the robot should first move forward toward the pipe’s open end to get within close inspection range (~1–2 m), then proceed along its length while maintaining a parallel offset to visually scan for marine growth or damage. Since the pipe extends diagonally away from the camera, a slight rightward correction may be needed to align with its axis before moving alongside.",
      "action": [
        "Go straight toward the open end of the green pipe",
        "Turn slightly right to align parallel to the pipe",
        "Move alongside the pipe from near end to far end, maintaining ~0.5 m distance",
        "Pause briefly at intervals to inspect surface for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/0199.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central dark columnar structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The central dark columnar structure is clearly visible and dominates the scene, positioned directly ahead of the robot. It appears to be within reachable distance, with no immediate obstacles blocking a direct approach. To inspect its surface for marine growth or damage, I need to first move closer to reduce distance, then position the robot parallel to the structure for optimal visual inspection. Given the lighting and visibility, approaching straight ahead is safe and efficient. After closing in, a slight lateral adjustment may be needed to align for detailed observation.",
      "action": [
        "Go straight toward the central columnar structure",
        "Stop at approximately 1 meter distance",
        "Adjust position to align parallel to the column's surface",
        "Inspect the surface visually for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0030_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small cylindrical device with red tether floating mid-water and inspect its status.",
    "output": {
      "reasoning": "The image shows a small cylindrical device floating mid-water, with a visible red tether attached—this matches the target described. The robot (likely the large fish-shaped AUV in the upper part of the frame) is currently positioned above and slightly behind the target. To approach and inspect, the robot should first descend slightly to align vertically with the device, then move forward (straight) toward it, and finally stabilize at close range for inspection. No immediate obstacles block the direct path. The lighting from above suggests good visibility along the descent and approach trajectory.",
      "action": [
        "Descend slightly",
        "Go straight toward the cylindrical device",
        "Hover in front of the device for inspection"
      ]
    },
    "images": [
      "./data/images/0274.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large brain coral in the center while maintaining a safe distance from the surrounding rocky outcrops and smaller coral formations.",
    "output": {
      "reasoning": "The large brain coral is centrally located and clearly visible. Surrounding it are smaller coral formations and rocky outcrops—especially to the left, right, and bottom of the frame. To approach safely, I must avoid lateral drift toward obstacles. The coral appears directly ahead at moderate distance, so a controlled forward advance with slight central alignment is appropriate. No immediate collision risk is present in the direct forward path, but I should maintain steady heading and avoid sharp turns. Given the depth and lighting, small adjustments may be needed mid-approach, but initial steps can be straightforward.",
      "action": [
        "Go straight slowly",
        "Maintain central heading toward the brain coral",
        "Slightly adjust left if nearing right-side outcrop",
        "Slightly adjust right if nearing left-side outcrop",
        "Stop at ~1 meter distance from the brain coral"
      ]
    },
    "images": [
      "./data/images/0176.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submersible vehicle positioned near the center-left of the frame, maintaining a safe distance while observing its surroundings.",
    "output": {
      "reasoning": "The green submersible vehicle is clearly visible in the center-left of the frame, suspended mid-water near a rocky outcrop. It is oriented horizontally, facing roughly toward the right. The robot appears to be positioned slightly behind and below it, with open water ahead and some coral/rock formations nearby but not directly obstructing a direct approach from the current position. To approach safely while observing its surroundings, I should first move forward cautiously to reduce distance without getting too close, then adjust laterally if needed to maintain optimal observation angle (e.g., slightly to the right to avoid occlusion by foreground rocks). Given the lighting and visibility, a slow, straight advance is safest initially.",
      "action": [
        "Go straight slowly",
        "Maintain slight rightward offset to keep vehicle in view",
        "Pause and observe surroundings"
      ]
    },
    "images": [
      "./data/images/0187.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green pipeline along the seabed while maintaining a safe distance from the adjacent yellow-banded cable.",
    "output": {
      "reasoning": "The image shows a green pipeline running along the seabed on the left side of the frame, and a yellow-banded cable (likely a power or communication cable) running parallel to it on the right, elevated slightly on a ladder-like support structure. The robot is currently positioned such that both are visible in the forward view, with the green pipeline closer to the seabed and the yellow-banded cable to the right and slightly above. To follow the green pipeline while maintaining a safe distance from the yellow-banded cable, the robot should stay centered relative to the green pipeline—i.e., keep it on the left but not too close—and avoid drifting right toward the yellow cable. Given the current alignment, the immediate action is to proceed straight while subtly adjusting leftward to maintain optimal lateral offset (~0.5–1 m) from the green pipeline’s edge, ensuring the yellow cable remains clearly to the right without encroachment.",
      "action": [
        "Go straight",
        "Slightly adjust left to keep green pipeline on left side at safe distance",
        "Maintain heading parallel to green pipeline"
      ]
    },
    "images": [
      "./data/images/temp_307_0090_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ornate, tower-like structure in the center while navigating between the large coral formations on the left and right.",
    "output": {
      "reasoning": "The ornate, tower-like structure is centrally located in the image, illuminated by sunbeams from above. Large coral formations flank it: a prominent dark coral mass on the left (foreground/mid-left) and a textured, rounded coral formation on the right (mid-right). The robot appears to be positioned slightly behind and below the structure, with a clear navigable corridor between the two corals. To approach the structure safely, the robot should first align itself centrally—i.e., move forward while maintaining equal clearance from left and right corals. Minor course corrections may be needed: since the left coral appears closer and more protruding, a slight rightward bias might help avoid collision, but the primary action is forward motion toward the center. No immediate turns are required; the path is relatively straight ahead through the gap.",
      "action": [
        "Go straight",
        "Maintain central alignment between left and right coral formations",
        "Proceed toward the tower-like structure"
      ]
    },
    "images": [
      "./data/images/0283.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green cylindrical structure with circular ports and inspect its front opening for signs of marine growth or damage.",
    "output": {
      "reasoning": "The image shows a submerged green cylindrical structure with circular ports lying horizontally on the seabed, oriented roughly left-to-right from the robot’s current perspective. Its front opening (largest circular port) faces toward the right side of the frame and is partially illuminated by sun rays. The robot appears to be positioned slightly left and forward of the structure, at a moderate distance (~3–5 m). To inspect the front opening closely, the robot must: (1) move forward to reduce distance, (2) adjust lateral position to align directly with the front opening (which is on the right end of the cylinder), and (3) approach carefully to get a clear view without disturbing sediment. No immediate obstacles block the path, though coral is present on the left—so lateral movement should favor the right side to avoid contact.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Go straight",
        "Move closer to the front opening",
        "Hover and inspect the front opening"
      ]
    },
    "images": [
      "./data/images/0269.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe structure on the left and inspect the circular end cap marked with the number '1'.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe structure on the left side of the frame, with multiple circular end caps visible. The end cap marked with the number '1' is clearly visible in the lower-left foreground, slightly angled but facing the camera. The robot is currently positioned at a moderate distance and slightly above the seabed. To approach and inspect the target end cap, the robot should first move forward toward the structure while maintaining alignment with the left-side pipe. As it gets closer, a slight downward and leftward adjustment may be needed to center the '1'-marked cap in the field of view for inspection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the left-side cylindrical pipe",
        "Slightly turn left to align with the end cap marked '1'",
        "Move forward slowly to get closer to the end cap",
        "Hover and stabilize in front of the end cap marked '1' for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0070_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central green pipeline structure and inspect the red indicator light near its midsection.",
    "output": {
      "reasoning": "The image shows a large central green pipeline structure extending vertically, with a visible red indicator light near its midsection (slightly below the center of the structure in the frame). The robot is currently positioned below and slightly in front of the structure, looking upward. To approach and inspect the red light, the robot must first move upward along the structure while maintaining alignment to avoid drifting laterally. Once near the midsection, a slight forward movement will bring the red light into close inspection range. No immediate obstacles block the path; however, due to the upward perspective and lighting from above, careful vertical control is needed to avoid overshooting.",
      "action": [
        "Move upward slowly while keeping the green pipeline centered in view",
        "Adjust position slightly forward when the red indicator light becomes clearly visible",
        "Hover steadily at the red indicator light for inspection"
      ]
    },
    "images": [
      "./data/images/0237.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the prominent white branching coral in the center-right while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The prominent white branching coral is clearly visible in the center-right of the image, illuminated by sun rays and standing out against darker rock formations and other corals. To approach it safely, I must avoid the surrounding rocks: there is a large dark rock formation to the left, and textured rocky structures below and to the right of the target coral. The robot appears to be positioned slightly left and somewhat distant from the coral. A safe approach path would involve moving forward while gently steering right—keeping enough lateral clearance from the left-side rock—and maintaining altitude to avoid bottom obstacles. Since the coral is at mid-depth and well-lit, a straight-forward + slight right correction should bring the robot into proximity without collision.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight",
        "Maintain steady depth and distance"
      ]
    },
    "images": [
      "./data/images/0299.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green mechanical structure on the seabed and inspect its central cylindrical component.",
    "output": {
      "reasoning": "The large green mechanical structure is clearly visible in the center-right of the image, resting on the seabed. Its central cylindrical component (the vertical, ribbed cylinder with circular ports) is the inspection target. The robot is currently positioned at a moderate distance and slightly elevated (as seen from the downward-looking perspective). To approach and inspect the central cylinder, the robot should first descend gently to seabed level, then move forward toward the structure, and finally position itself directly in front of or slightly below the central cylinder for close inspection. No immediate obstacles block the direct path, though care should be taken around nearby sea urchins and coral-like formations.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward toward the green mechanical structure",
        "Approach the central cylindrical component head-on",
        "Hover steadily in front of the central cylinder for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0003_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible and dominates the scene, positioned directly ahead of the robot's current viewpoint. Sunlight rays illuminate it from above, enhancing visibility. To inspect its surface for inscriptions or damage, I need to approach it closely while maintaining a stable orientation. Given its central position and lack of immediate obstacles in the forward path, the optimal plan is: (1) move straight toward the base of the column to reduce distance, (2) circle slightly around it (e.g., clockwise) to view multiple sides, and (3) get closer for detailed inspection—ideally positioning the camera parallel to the surface at mid-height where inscriptions are likely. No red obstacles are present; the surrounding smaller rocks are distant and non-blocking.",
      "action": [
        "Go straight toward the central stone column",
        "Circle clockwise around the column at close range",
        "Move closer to the column surface for detailed inspection"
      ]
    },
    "images": [
      "./data/images/0292.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular metal structure on the left while avoiding the coral clusters on the seabed.",
    "output": {
      "reasoning": "The large circular metal structure is prominently visible on the left side of the image, extending vertically and partially embedded in the seabed. Coral clusters are scattered on the seabed in front and slightly to the right of the robot’s current position. To approach the structure while avoiding corals, the robot should first steer left to align with the structure’s axis, then proceed forward cautiously, maintaining clearance from the seabed corals by staying slightly elevated or hugging the left wall of the structure where corals are less dense. The path should avoid the central seabed region where coral density appears highest.",
      "action": [
        "Turn left slightly",
        "Move forward slowly while keeping the metal structure on the left",
        "Maintain altitude to avoid seabed coral clusters",
        "Continue approaching until close to the structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0048_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large dark rock formation on the right while maintaining a safe distance from the overhanging ledge above.",
    "output": {
      "reasoning": "The image shows a large dark rock formation on the right side of the frame, with an overhanging ledge above it casting shadow and potentially posing a collision risk. The robot is currently positioned to the left and slightly below the formation. To approach the rock safely: (1) First, move forward cautiously to close distance while staying clear of the overhead ledge—this requires maintaining depth (not ascending) and aligning laterally toward the rock’s base. (2) Then, gently turn right to align parallel to the rock face, keeping a buffer zone beneath the overhang. (3) Finally, proceed straight alongside the rock at a consistent safe distance (e.g., ~1–2 m), monitoring proximity to both the rock and the ledge. No immediate obstacles block the path, but lighting gradients suggest the ledge extends inward—so avoid upward motion.",
      "action": [
        "Go straight slowly",
        "Turn right slightly",
        "Go straight while maintaining lateral distance from the rock and avoiding ascent"
      ]
    },
    "images": [
      "./data/images/temp_307_0021_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green bioluminescent structure and inspect its base for signs of marine growth or damage.",
    "output": {
      "reasoning": "The glowing green bioluminescent structure is centrally located in the image, standing vertically on the seabed with a distinct base featuring multiple lobes. The robot is currently at a moderate distance, facing the structure head-on. To inspect the base for marine growth or damage, I need to: (1) approach the structure carefully to reduce distance without disturbing sediment; (2) position the robot close enough to resolve fine details at the base (likely requiring ~1–2 m range); (3) possibly adjust orientation to ensure optimal lighting and field-of-view of the base—since the glow originates from the upper part, the base may be shadowed, so a slight downward tilt or lateral shift may help. No immediate obstacles block the direct path. Therefore, the plan is: go straight toward the structure, then fine-tune position near the base, and finally stabilize for inspection.",
      "action": [
        "Go straight toward the glowing green structure",
        "Slow down as you near the base (within 2 meters)",
        "Slightly descend and/or tilt downward to focus on the base",
        "Hover steadily to inspect the base for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/0111.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater structure and inspect its base for attachment points or damage.",
    "output": {
      "reasoning": "The red cylindrical underwater structure is clearly visible in the upper center of the image, suspended vertically with its base extending downward toward the seabed. The robot appears to be positioned slightly below and in front of the structure, with coral and marine life surrounding it. To approach the base for inspection, the robot should first move upward (toward the structure) and slightly forward to align with the cylinder’s axis, then descend carefully along its side to reach the base without disturbing sediment or obstacles. Since the base is likely near the seabed (partially obscured by coral), a controlled descent after alignment is necessary. No immediate obstacles block the direct path upward toward the cylinder.",
      "action": [
        "Move upward toward the red cylindrical structure",
        "Align horizontally with the cylinder's midsection",
        "Descend slowly along the cylinder's side toward its base",
        "Hover near the base and inspect for attachment points or damage"
      ]
    },
    "images": [
      "./data/images/0181.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the weathered stone column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The weathered stone column is clearly visible in the center-right of the image, standing vertically on a submerged platform. It appears to be the primary target. The robot is currently at a moderate distance and slightly to the left-front of the column. To approach and inspect its surface, I need to: (1) move forward toward the column while maintaining alignment, (2) close the distance to allow detailed visual inspection (within ~1–2 meters), and (3) possibly adjust orientation to scan the full circumference or focus on areas with visible marine growth (e.g., green algae patches near base and mid-section). No immediate obstacles block the direct path, though some scattered debris lies to the left—avoidance is not urgent but should be monitored. Given the column’s height and vertical structure, a straight-in approach followed by a slight circling or lateral drift will enable comprehensive inspection.",
      "action": [
        "Go straight toward the stone column",
        "Slow down as you near the base (within 2 meters)",
        "Circle slightly around the column clockwise to inspect all sides",
        "Pause briefly at each quadrant to observe surface details"
      ]
    },
    "images": [
      "./data/images/0230.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central bioluminescent structure with the green spiral tendril and pink ring, avoiding the surrounding red-lit nodes and fibrous filaments.",
    "output": {
      "reasoning": "The central bioluminescent structure is clearly visible: a dark spherical core with a pink ring and a prominent green spiral tendril extending upward. Surrounding it are red-lit nodes (two visible on the lower left and right of the core) and numerous fibrous green filaments radiating outward, forming a web-like obstacle field. The robot appears to be positioned slightly below and in front of the structure, with open water above and some filament clearance directly ahead. To approach safely: (1) First, ascend gently to align with the mid-height of the core and avoid entanglement with lower filaments; (2) Then move forward slowly along the central axis—where the green tendril provides a visual corridor—to minimize proximity to red nodes; (3) Finally, fine-tune lateral position to stay centered between the left and right red nodes, maintaining distance from all filaments. No sharp turns are needed since the target is directly ahead and vertically aligned.",
      "action": [
        "Ascend slightly",
        "Move forward slowly along the central axis",
        "Maintain center alignment between left and right red nodes"
      ]
    },
    "images": [
      "./data/images/0152.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the arched rock opening toward the sunlit area ahead, avoiding the large boulders on the seabed.",
    "output": {
      "reasoning": "The image shows an arched rock opening directly ahead, with strong sunlit rays visible beyond it—indicating the target destination. The seabed contains several large boulders (especially on the left and right near the arch base) that must be avoided. To safely navigate through the arch: (1) First, align the robot centrally with the arch opening to ensure clearance; (2) Then proceed forward steadily while maintaining a safe distance from the boulders on both sides; (3) As the arch narrows slightly mid-passage, slight lateral adjustments may be needed—based on visual feedback—but the initial path should be straight ahead toward the brightest light. No sharp turns are required yet; the primary risk is collision with nearby boulders, so forward motion with centered alignment is optimal.",
      "action": [
        "Align center with the arched rock opening",
        "Go straight forward slowly",
        "Maintain central trajectory through the arch",
        "Proceed toward the sunlit area beyond"
      ]
    },
    "images": [
      "./data/images/temp_307_0008_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle on the seabed and inspect its front illuminated porthole.",
    "output": {
      "reasoning": "The image shows a submarine-like vehicle resting on the seabed, oriented horizontally with its front (illuminated porthole) facing roughly toward the right side of the frame. The robot appears to be positioned slightly left and behind the vehicle, with rocky obstacles in the immediate foreground. To approach the front illuminated porthole, the robot must first navigate past the left-side rocks, then move forward and slightly right to align with the vehicle’s front. The porthole is clearly visible and glowing, serving as the inspection target. A safe path exists along the open seabed to the right of the nearest rock. Therefore, the plan is: (1) turn right to clear the left obstacle and face the vehicle’s direction, (2) advance straight toward the vehicle while maintaining alignment with the front, (3) fine-tune position to center on the illuminated porthole for inspection.",
      "action": [
        "Turn right",
        "Go straight",
        "Move slightly right and forward to center on the illuminated porthole"
      ]
    },
    "images": [
      "./data/images/temp_307_0083_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral structure with the spiral interior and inspect its central cavity.",
    "output": {
      "reasoning": "The image shows a large green coral structure with a prominent spiral interior and a visible central cavity. The robot is currently positioned to the left and slightly in front of the structure, with the cavity facing roughly toward the robot's right side. To approach and inspect the central cavity, the robot should first move closer to the structure while aligning its heading toward the cavity entrance. Given the current perspective, a slight right turn will better orient the robot toward the cavity, followed by forward motion to get within inspection range. Once near, a minor adjustment (e.g., slight left or right) may be needed to center the cavity in view, but the primary steps are: turn right to face the cavity, then go straight toward it.",
      "action": [
        "Turn right",
        "Go straight",
        "Move closer to the central cavity"
      ]
    },
    "images": [
      "./data/images/0257.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall perforated cylindrical structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The tall perforated cylindrical structure is clearly visible in the center-right of the image, standing vertically on the seabed. The robot appears to be positioned slightly left and at a moderate distance (~2–3 m) from it, with clear visibility and no immediate obstacles blocking the path. To approach and inspect its surface, I need to: (1) move forward toward the structure, (2) adjust lateral position to align parallel to its side for optimal inspection, and (3) perform a slow circumferential or close-range pass to examine surface features (e.g., marine growth, cracks, corrosion). Given the lighting and clarity, a direct forward motion followed by a slight right turn to get alongside is safe and efficient.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Turn slightly right to align parallel to the structure's side",
        "Move slowly alongside the structure to inspect its surface"
      ]
    },
    "images": [
      "./data/images/0156.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater structure with yellow and blue bands and inspect its top opening.",
    "output": {
      "reasoning": "The cylindrical structure with yellow and blue bands is clearly visible in the center-left of the image, upright on the seabed. Its top opening (with a blue cap and yellow ridged ring) is facing upward and partially illuminated by surface light. The robot appears to be positioned slightly below and to the side of the structure. To approach and inspect the top opening, the robot should first move forward toward the structure while maintaining a safe distance, then adjust laterally if needed to center itself beneath the opening, and finally ascend vertically to get a clear overhead view of the top opening.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Slightly adjust left/right to center under the structure",
        "Ascend vertically to position above the top opening",
        "Hover and inspect the top opening"
      ]
    },
    "images": [
      "./data/images/0281.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical vehicle emitting blue light and inspect its front hatch for signs of damage or open access.",
    "output": {
      "reasoning": "The submerged cylindrical vehicle is centered in the image, emitting blue light from its rear (bottom end), and oriented vertically with its front hatch likely at the top (nose) end, facing upward toward the camera's perspective. The robot appears to be positioned slightly below and in front of the vehicle, viewing it through an opening in a rocky structure. To inspect the front hatch, the robot must first close the distance while maintaining a stable approach angle. Since the vehicle is directly ahead but elevated, the optimal path is to move forward and slightly upward (or pitch up) to align with the front hatch. No immediate obstacles block the direct path, though surrounding rocks require careful lateral positioning to avoid collision. Given the monocular view, depth estimation suggests moderate proximity—so a controlled forward advance followed by fine positioning is appropriate.",
      "action": [
        "Go straight slowly",
        "Pitch up slightly",
        "Approach the front hatch of the cylindrical vehicle",
        "Hover and inspect the front hatch for damage or open access"
      ]
    },
    "images": [
      "./data/images/0249.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car between the two large columns and inspect its front left wheel area for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a submerged car positioned between two large, column-like structures—one on the left (a multi-columned ruin) and one on the right (a single thick column). The car is angled slightly, with its front facing roughly toward the camera but tilted leftward; the front left wheel area is partially visible but not yet close enough for detailed inspection. To approach safely and directly, the robot should first move forward while maintaining central alignment between the columns to avoid collision. Once closer (~2–3 m), a slight leftward adjustment will position the robot optimally to inspect the front left wheel region. No immediate obstacles block the path, but marine growth on columns suggests caution near surfaces.",
      "action": [
        "Go straight forward between the two columns",
        "Approach the submerged car steadily",
        "Slightly turn left to align with the front left wheel area",
        "Hover and inspect the front left wheel for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0054_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the left while maintaining a safe distance from the smaller glowing corals in the foreground.",
    "output": {
      "reasoning": "The image shows a large textured coral formation on the left side of the frame, which is the target. In the foreground, there are several smaller corals emitting green glow—these are the obstacles to avoid. The robot is currently positioned such that the large coral is to its left and slightly ahead, while the glowing corals lie directly in front and slightly to the right. To approach the large coral safely: (1) First, move forward cautiously but steer slightly left to begin aligning toward the large coral while keeping distance from the foreground glowing corals; (2) Then, continue moving leftward (or turn left gently) to position the robot parallel to the large coral’s side, ensuring the glowing corals remain behind or to the right; (3) Finally, advance straight along the left side of the large coral at a safe lateral offset (e.g., ~1–2 m) to maintain proximity without collision.",
      "action": [
        "Move forward while gently turning left",
        "Continue turning left until aligned with the left side of the large coral",
        "Proceed straight alongside the large coral at a safe distance"
      ]
    },
    "images": [
      "./data/images/0271.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing blue-green circular structure on the seabed while avoiding the larger bioluminescent organism above it.",
    "output": {
      "reasoning": "The image shows a glowing blue-green circular structure on the seabed (foreground, lower right), which is the target. Above it, there is a larger bioluminescent organism with multiple glowing green spots and a bulbous, textured body (mid-to-upper center), which must be avoided. The robot appears to be positioned slightly left and possibly elevated relative to the target. To approach the circular structure safely: (1) First, move forward cautiously while maintaining a safe vertical distance below the large organism; (2) Since the target is to the right of current view, a slight rightward correction is needed; (3) Then proceed straight toward the structure, keeping depth shallow enough to stay under the obstacle but high enough to avoid seabed contact. No sharp turns are needed—gentle steering suffices due to clear lateral separation.",
      "action": [
        "Move forward slowly",
        "Turn slightly right",
        "Continue forward toward the glowing blue-green circular structure on the seabed",
        "Maintain depth below the large bioluminescent organism"
      ]
    },
    "images": [
      "./data/images/0272.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green dome-shaped device emitting a green light and inspect its top cylindrical component.",
    "output": {
      "reasoning": "The green dome-shaped device emitting a green light is clearly visible in the foreground, slightly to the right of center. Its top cylindrical component (a blue-tinted cap) is upright and accessible. The robot appears to be positioned at a moderate distance (~1.5–2 m) and slightly elevated relative to the device. To inspect the top cylindrical component, the robot must first approach it directly while maintaining stable depth and orientation, then position itself directly above or slightly in front to get a clear view of the top. No immediate obstacles block the direct path; the surrounding rocks are to the left and rear. Therefore, the plan is: (1) move straight toward the device, (2) fine-tune position to center on the device, (3) ascend slightly if needed to align视线 with the top component, (4) hold position for inspection.",
      "action": [
        "Go straight toward the green dome-shaped device",
        "Slow down and center the device in the field of view",
        "Ascend slightly to align with the top cylindrical component",
        "Hold position and inspect the top cylindrical component"
      ]
    },
    "images": [
      "./data/images/0263.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange and green submersible device resting on the seabed near the rocky outcrop.",
    "output": {
      "reasoning": "The orange and green submersible device is clearly visible on the seabed, positioned slightly left of center in the image, near a large rocky outcrop to its right. The robot appears to be facing it at a moderate distance (~2–3 m), with clear visibility and no immediate obstacles in the direct path. To approach it safely, the robot should first move straight forward toward the device, then possibly adjust laterally if needed to center itself relative to the target. Since the device is stationary and well-lit, no avoidance maneuvers are required at this stage.",
      "action": [
        "Go straight",
        "Approach the orange and green submersible device"
      ]
    },
    "images": [
      "./data/images/0234.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the carved stone column at the center of the cave while avoiding the rocky outcrops on the left and right.",
    "output": {
      "reasoning": "The carved stone column is centrally located and illuminated by light from above, making it the clear target. Rocky outcrops are visible on both left and right sides—left side shows a large irregular formation, right side has a cluster of boulders near the column’s base. To avoid collisions, the robot must maintain a central trajectory. Given the current perspective (robot appears to be at cave entrance or mid-chamber), the safest path is to proceed straight while slightly favoring the centerline, adjusting as needed to keep equal clearance from both sides. No immediate turn is required since the column is directly ahead and unobstructed in the central corridor.",
      "action": [
        "Go straight",
        "Maintain central alignment",
        "Proceed toward the illuminated column"
      ]
    },
    "images": [
      "./data/images/0276.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the right while avoiding the large coral formations on the left and center.",
    "output": {
      "reasoning": "The robot is currently oriented forward, with large coral formations occupying the left and central regions of the scene. A yellow sponge is visible on the right side, partially illuminated by light rays. To approach it safely, the robot must first steer right to avoid the corals on the left and center, then proceed forward toward the sponge. Since the sponge appears at a moderate distance and slightly ahead-right, a controlled right turn followed by straight motion should align the robot with the target while maintaining clearance from obstacles.",
      "action": [
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0050_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green pipe while avoiding the surrounding coral formations and small fish.",
    "output": {
      "reasoning": "The illuminated green pipe is vertically oriented and centrally located in the image, extending from near the bottom toward the top. It is surrounded by coral formations on both left and right sides (e.g., large fan-like coral on left, dense branching coral on lower-left, and dark reef structure on right), and numerous small fish are scattered throughout the water column—especially near the pipe’s midsection and around the corals. To approach safely: (1) The robot should first align directly with the pipe’s axis to minimize lateral deviation; (2) Since the pipe is well-lit and prominent, maintaining forward motion while keeping a safe clearance (~0.5–1 m) from visible coral edges is feasible; (3) Small fish are highly mobile and unlikely to obstruct if the robot moves steadily without sudden maneuvers; (4) Given the pipe extends upward, approaching from below (i.e., moving upward along its length) is natural and avoids entanglement with seabed corals. Therefore, the plan is: center on the pipe, move straight ahead (upward in the image frame), and slightly adjust laterally only if needed to maintain clearance—but initial alignment suggests no immediate turn is required.",
      "action": [
        "Center on the illuminated green pipe",
        "Move straight forward (upward) along the pipe's axis",
        "Maintain slight leftward offset to avoid right-side coral",
        "Proceed steadily while monitoring for fish proximity"
      ]
    },
    "images": [
      "./data/images/0150.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central green cylindrical structure with multiple circular openings and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The central green cylindrical structure with multiple circular openings is clearly visible in the image, positioned slightly ahead and centered in the field of view. It is surrounded by netting and other debris, but a clear approach path exists from the current robot position. To inspect its surface for marine growth or damage, I need to: (1) move forward toward the structure while maintaining alignment with its center; (2) close in to a safe inspection distance (~1–2 m) without colliding with surrounding nets; (3) possibly adjust orientation to ensure full surface visibility (e.g., slight yaw if needed). Given the current perspective, the most direct action is to advance straight while monitoring proximity to obstacles.",
      "action": [
        "Go straight toward the central green cylindrical structure",
        "Slow down as you near the structure (within 2 meters)",
        "Hover and stabilize at ~1.5 meters distance",
        "Perform slow clockwise rotation to inspect the full surface"
      ]
    },
    "images": [
      "./data/images/0231.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object with green circular feature on the seabed and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The red dome-shaped object with a green circular feature is clearly visible on the seabed in the foreground, slightly to the right of center. The robot appears to be positioned at a moderate distance (~1–2 m) and slightly elevated. To inspect its surface for markings or damage, I need to: (1) approach it carefully to reduce distance without disturbing sediment; (2) position the robot directly in front or slightly above for optimal visual coverage; (3) stabilize and possibly adjust orientation to ensure the green feature and surface details are fully in view. No immediate obstacles block the path, but surrounding rocks suggest cautious forward motion.",
      "action": [
        "Go straight slowly toward the red dome-shaped object",
        "Move slightly left to center the object in the field of view",
        "Descend gently to ~0.5 m above the seabed",
        "Hover and stabilize in front of the object",
        "Rotate slightly clockwise to inspect the green circular feature and adjacent surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0060_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater robot with the red fin and inspect its front sensor array.",
    "output": {
      "reasoning": "The green underwater robot with a red fin is visible in the center-left of the image, oriented roughly toward the camera. Its front sensor array (a cylindrical protrusion with a white tip) is facing forward and slightly downward. The robot is positioned on the seabed among rocks, with clear space in front of it. To inspect its front sensor array, I need to approach from the front—i.e., move straight toward it while maintaining a safe distance, then possibly adjust position to align directly with the sensor for close inspection. No immediate obstacles block the direct path; the main challenge is avoiding contact with surrounding rocks. Given the current perspective, the robot appears ~2–3 meters ahead and slightly to the left. So first, I’ll correct slight lateral offset by moving right, then go straight toward it, and finally fine-tune position to face the sensor array head-on.",
      "action": [
        "Move slightly right",
        "Go straight toward the green robot",
        "Stop at ~1 meter distance in front of the robot",
        "Adjust position to center the front sensor array in view"
      ]
    },
    "images": [
      "./data/images/0194.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated submarine at the cave entrance while avoiding the rocky outcrops on the seafloor.",
    "output": {
      "reasoning": "The illuminated submarine is visible at the cave entrance, centered in the image and slightly ahead. Rocky outcrops are present on the seafloor—particularly to the left and right of the current path—and also along the cave walls. To safely approach, the robot must maintain a central trajectory, avoiding lateral deviations toward the rocks. The submarine appears at moderate distance; thus, a sequence of forward movements with minor course corrections is needed. Since the cave opening frames the target, staying aligned with the light beam (which originates from the submarine) helps ensure safe passage. No immediate sharp turns are required—just steady forward motion with slight adjustments to stay clear of near-field rocks.",
      "action": [
        "Go straight slowly",
        "Slightly adjust right to avoid left-side rocks",
        "Continue straight toward the submarine's lights",
        "Maintain center alignment with the cave opening",
        "Approach the submarine steadily"
      ]
    },
    "images": [
      "./data/images/0108.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submarine from behind while maintaining a safe distance and aligning with its central axis.",
    "output": {
      "reasoning": "The image shows a large submarine oriented vertically, with its stern (rear) facing the camera. A smaller autonomous underwater vehicle (AUV) is positioned in front of and slightly below the submarine, angled toward it. To approach from behind while maintaining a safe distance and aligning with the central axis, the robot must first reposition to the rear hemisphere of the submarine. Since the current view shows the submarine’s bow (front), the robot is currently in front — thus, it needs to maneuver around to the back. Given the submarine’s vertical orientation and the presence of seabed terrain on the right, the safest path is to ascend slightly, then move laterally to the left (port side) and circle behind, avoiding the rock formation on the right. Once behind, fine adjustments are needed to center on the central axis and maintain safe standoff distance. The immediate next step is to turn left and ascend gently to begin repositioning.",
      "action": [
        "Turn left",
        "Ascend slightly",
        "Move backward while maintaining left offset",
        "Re-center behind the submarine",
        "Maintain safe distance along central axis"
      ]
    },
    "images": [
      "./data/images/0241.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient stone column and inspect its surface for inscriptions or damage, avoiding the surrounding coral formations.",
    "output": {
      "reasoning": "The image shows a tall ancient stone column centered in the frame, with coral formations on both left and right sides near its base. The robot appears to be positioned slightly in front and below the column, with clear water ahead but potential obstruction from corals if approaching directly laterally. To safely approach while avoiding corals: (1) First, move forward cautiously along the central axis—this path appears clearest based on lighting and absence of nearby coral in the direct frontal view. (2) Once closer (within ~2 m), adjust laterally—slightly to the left or right—to align for inspection, choosing the side with less dense coral (left side has smaller, more spaced corals; right side has denser clusters). (3) Then perform a slow circular or lateral scan at close range to inspect surface details without contact. Given the instruction emphasizes *avoiding* coral, the initial approach must stay centered, then fine-tune position before inspection.",
      "action": [
        "Go straight toward the column center",
        "Slightly turn left to maintain safe distance from right-side coral",
        "Slowly advance until 1 meter from column base",
        "Orbit clockwise around column at 1-meter radius to inspect surface",
        "Pause and zoom in visually on areas with texture anomalies"
      ]
    },
    "images": [
      "./data/images/temp_307_0088_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged concrete structure and inspect its underside for marine growth or damage.",
    "output": {
      "reasoning": "The submerged concrete structure is directly in front of the robot, elevated above the seabed on supports. Its underside is partially visible but not fully accessible from the current position. To inspect the underside for marine growth or damage, the robot must first move closer to reduce distance and improve resolution, then maneuver underneath—likely by moving forward while slightly descending or adjusting pitch. Given the open space beneath and no immediate obstacles blocking access, a safe approach path exists. The plan: (1) advance toward the structure to get within inspection range, (2) position directly beneath the overhang, (3) stabilize and orient camera upward for detailed visual inspection.",
      "action": [
        "Go straight toward the concrete structure",
        "Descend slightly while continuing forward to pass under the structure",
        "Position centrally beneath the structure's underside",
        "Tilt camera upward and hold position for inspection"
      ]
    },
    "images": [
      "./data/images/0183.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure covered in marine growth and inspect its central opening.",
    "output": {
      "reasoning": "The large green cylindrical structure is centrally located in the image, with its central opening clearly visible and facing the camera. The robot appears to be at a moderate distance, slightly below and in front of the structure. To approach and inspect the central opening, the robot should first move forward to reduce distance, then fine-tune position to center the opening in the field of view. No immediate obstacles block the direct path; marine growth is on the structure itself, not obstructing access. Given the lighting and visibility, a straight approach is safe and optimal.",
      "action": [
        "Go straight toward the green cylindrical structure",
        "Slow down as you near the central opening",
        "Position yourself directly in front of the central opening for inspection"
      ]
    },
    "images": [
      "./data/images/0189.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the right while avoiding the dark pipe-like structure above and the spiky coral formations below.",
    "output": {
      "reasoning": "From the image, the yellow sponge is visible on the upper right side of the frame, partially illuminated by sun rays. A dark pipe-like structure (likely a cable or tube) runs diagonally across the upper portion of the scene, above the robot’s current position. Spiky coral formations are present along the bottom right and near the center-right, below the robot. The robot is currently oriented roughly forward-left, with its nose pointing toward the lower-left quadrant. To approach the yellow sponge on the right while avoiding both the overhead pipe and the bottom corals, the robot must: (1) first ascend slightly to clear the spiky corals below, (2) then turn right to align toward the sponge, (3) proceed forward cautiously while maintaining vertical clearance from the pipe above and horizontal clearance from corals below. Given the sponge is elevated and to the right, a gentle upward-right trajectory is optimal.",
      "action": [
        "Ascend slightly",
        "Turn right",
        "Go forward while maintaining safe altitude above corals and below pipe"
      ]
    },
    "images": [
      "./data/images/0138.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall dark sponge structure rising from the seabed while avoiding the surrounding yellow cup corals and rocky outcrops.",
    "output": {
      "reasoning": "The image shows a tall, dark, vertically oriented sponge structure centered in the frame, rising from the seabed. Surrounding it are yellow cup corals (especially to the right and lower right) and various rocky outcrops (left and background). The robot appears to be positioned slightly below and in front of the sponge, with clear water ahead but obstacles laterally. To approach safely: (1) Maintain forward motion toward the sponge’s base while keeping central alignment; (2) Slightly adjust left or right as needed to avoid encroaching on yellow cup corals—given their concentration on the right, a slight leftward bias may help; (3) Monitor proximity to avoid collision with nearby rocks. Since the sponge is directly ahead and dominant in view, the primary action is controlled forward advance with minor lateral correction.",
      "action": [
        "Go straight toward the dark sponge",
        "Slightly veer left to avoid yellow cup corals on the right",
        "Continue approaching until close to the sponge base"
      ]
    },
    "images": [
      "./data/images/0218.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green plate coral in the center while avoiding the surrounding purple branching corals.",
    "output": {
      "reasoning": "The large green plate coral is clearly visible in the center of the image, characterized by its flat, layered, circular structure and bright green color. Surrounding it are several purple branching corals (e.g., upper right and lower left), which must be avoided. The robot appears to be positioned slightly below and in front of the target coral, with open water space directly ahead toward the central plate coral. To approach safely: (1) Maintain forward motion along a central trajectory to minimize proximity to purple corals on either side; (2) Slightly adjust heading to keep equidistant from left and right purple obstacles; (3) As the coral grows larger in view, reduce speed and fine-tune position to center on the plate coral without contacting adjacent structures. No immediate turn is needed—initial movement should be straight with minor lateral correction if drift is detected.",
      "action": [
        "Go straight",
        "Slightly adjust left to maintain clearance from right-side purple coral",
        "Continue straight while centering on green plate coral",
        "Slow down as coral fills more of the field of view"
      ]
    },
    "images": [
      "./data/images/0121.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical structure on the left while avoiding the suspended green vegetation and the small debris on the seabed.",
    "output": {
      "reasoning": "From the image, the red cylindrical structure is on the left side of the frame, partially visible and lying on the seabed. Suspended green vegetation hangs vertically near the center, slightly right of the red structure. Small debris (e.g., rectangular object and fragments) is scattered on the seabed in front and to the right of the robot’s current position. To approach the red structure safely: (1) First, move forward cautiously while staying left of the suspended vegetation to avoid entanglement; (2) Then, adjust slightly leftward to align with the red cylinder; (3) Finally, proceed straight toward it, ensuring the path avoids the seabed debris—especially the rectangular object ahead and to the right. The robot should maintain a low altitude to minimize disturbance but stay above the debris field.",
      "action": [
        "Go straight while keeping left of the hanging green vegetation",
        "Slightly turn left to align with the red cylindrical structure",
        "Proceed straight toward the red cylindrical structure, avoiding seabed debris"
      ]
    },
    "images": [
      "./data/images/0291.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central cluster of three glowing green sea anemones inside the circular metal frame structure.",
    "output": {
      "reasoning": "The image shows a circular metal frame structure enclosing three glowing green sea anemones arranged centrally within it. The robot is positioned outside the frame, facing the opening. Sunlight penetrates from above, illuminating the scene and highlighting the anemones at the center. To approach the central cluster, the robot must first enter the circular frame through the visible archway (front-facing opening), then proceed straight toward the illuminated anemones, which are clearly centered and slightly forward inside the structure. No immediate obstacles block the direct path; minor adjustment may be needed to stay centered as it advances.",
      "action": [
        "Enter the circular metal frame through the front opening",
        "Go straight toward the glowing green anemones",
        "Maintain center alignment while advancing"
      ]
    },
    "images": [
      "./data/images/0232.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The submerged stone column is directly in front of the robot, centered in the field of view, and appears accessible without immediate obstacles. Sunlight rays indicate relatively clear water ahead. To inspect its surface, the robot must first approach closely (within ~1–2 m) to resolve fine details like marine growth or cracks. Given its current distance (estimated >3 m based on scale), a straight-forward advance is appropriate. Once near, a slow circular or lateral movement will allow full surface inspection—but the initial step is to close the distance safely.",
      "action": [
        "Go straight toward the stone column",
        "Slow down as you near the column (within 2 meters)",
        "Orbit slowly around the column at close range to inspect surface"
      ]
    },
    "images": [
      "./data/images/0247.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle positioned centrally between the rocky outcrops and inspect its front hatch.",
    "output": {
      "reasoning": "The image shows a submarine-like vehicle centered between two rocky outcrops, oriented vertically with its front (hatch side) facing the camera. The robot is currently at a moderate distance and slightly below the vehicle’s midline. To approach and inspect the front hatch, I need to: (1) move forward toward the vehicle while maintaining alignment with its central axis; (2) adjust vertical position upward slightly to align with the hatch (which appears near the upper front section); (3) stop at close range for inspection. No immediate obstacles block the direct path, but careful forward motion is needed to avoid contact with rocks or the vehicle itself.",
      "action": [
        "Go straight toward the submarine-like vehicle",
        "Ascend slightly to align with the front hatch",
        "Stop at close proximity to inspect the front hatch"
      ]
    },
    "images": [
      "./data/images/0148.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and white striped buoy anchored on the seabed, maintaining a safe distance from the surrounding coral and rocks.",
    "output": {
      "reasoning": "The red and white striped buoy is centrally located in the image, anchored on the seabed. Coral formations are visible to the left and right of the buoy, and a large rock is on the right side. To approach safely, the robot should first align directly toward the buoy while maintaining a central path to avoid the nearby coral and rock. Given the clear frontal view and sufficient clearance ahead, the safest initial action is to move straight forward at a moderate pace, monitoring proximity to obstacles. As it gets closer, fine adjustments can be made—but for now, the primary step is forward motion along the open corridor.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0071_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green and blue segmented underwater vehicle resting near the coral formation and inspect its front sensor housing.",
    "output": {
      "reasoning": "The green and blue segmented underwater vehicle is visible in the center-right of the image, resting near coral formations. Its front sensor housing (dark circular feature) is on the leftmost end of the vehicle, facing roughly toward the camera but slightly angled downward. The robot is currently at a moderate distance and slightly above/behind the vehicle. To approach and inspect the front sensor housing, I need to: (1) move forward cautiously to reduce distance while maintaining clear view; (2) adjust heading slightly left to align with the vehicle’s longitudinal axis; (3) fine-tune position to bring the front sensor housing into close inspection range without disturbing sediment or coral. No immediate obstacles block the path, but care must be taken near the coral.",
      "action": [
        "Go straight slowly",
        "Turn slightly left",
        "Approach the front of the vehicle",
        "Hover in front of the sensor housing for inspection"
      ]
    },
    "images": [
      "./data/images/0178.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, segmented green structure in the center while avoiding the surrounding brain coral formations.",
    "output": {
      "reasoning": "The image shows a tall, segmented green structure (likely a coral or artificial pillar) centered in the frame. Surrounding it are brain coral formations—bulbous, grooved structures—on both left and right sides, posing potential collision risks. The robot is currently positioned at a moderate distance directly facing the target. To safely approach, I should first ensure alignment with the central structure, then advance while maintaining clearance from the flanking corals. Since the path appears relatively clear straight ahead (no immediate obstacles in the central corridor), a direct forward movement is feasible, but must be cautious and incremental to avoid drifting into side corals. No need to turn initially—the structure is already centered.",
      "action": [
        "Go straight slowly",
        "Maintain center alignment with the green structure",
        "Monitor left and right for encroaching brain coral"
      ]
    },
    "images": [
      "./data/images/0175.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green netted structure on the left and inspect its attachment point to the seabed.",
    "output": {
      "reasoning": "The large green netted structure is clearly visible on the left side of the image, partially anchored to the seabed. Its attachment point appears near the lower-left portion of the frame, where the net converges toward a cylindrical base resting on the seafloor. To inspect the attachment point, I need to: (1) move closer to the structure while maintaining a safe distance to avoid entanglement; (2) reposition to get a clear view of the base and its connection to the seabed; (3) possibly descend slightly to align the camera with the attachment zone. The current perspective shows the structure at an angle, so a slight leftward approach followed by forward motion and minor descent will optimize visibility.",
      "action": [
        "Turn left slightly",
        "Go straight toward the green netted structure",
        "Descend gently to align with the base of the structure",
        "Move closer to inspect the attachment point on the seabed"
      ]
    },
    "images": [
      "./data/images/0251.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing coral-like structure mounted on the illuminated base and inspect its surface texture and embedded red lights.",
    "output": {
      "reasoning": "The glowing coral-like structure is centrally located in the image, mounted on a bright cyan illuminated base. It is clearly visible and within reasonable proximity for approach. The robot appears to be facing it directly, with no immediate obstacles in the forward path. To inspect surface texture and embedded red lights, the robot must first move closer (approach), then possibly adjust orientation for optimal viewing—ideally maintaining a stable position at close range (~0.5–1 m) to resolve fine details. Given the lighting and clarity, a straight-forward approach followed by a slight hover/position stabilization is sufficient; no turning is needed initially since the target is centered.",
      "action": [
        "Go straight toward the coral structure",
        "Slow down as you near the base",
        "Hover steadily at ~0.6 meters distance",
        "Tilt camera slightly downward to inspect upper surface texture",
        "Pan slowly around the structure to view embedded red lights"
      ]
    },
    "images": [
      "./data/images/0300.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red submersible vehicle ahead while maintaining a safe distance from the large coral formation on the right.",
    "output": {
      "reasoning": "The red submersible vehicle is directly ahead and slightly elevated in the water column. A large coral formation occupies the right side of the scene, posing a collision risk if the robot drifts right. To approach safely, the robot should maintain central or slight-left heading to keep clearance from the coral. Since the target is ahead, the primary motion is forward, but minor leftward correction may be needed initially to center the path relative to the coral. No immediate turn is required—just straight advance with awareness of right-side obstacle.",
      "action": [
        "Go straight",
        "Slightly adjust left to maintain distance from coral"
      ]
    },
    "images": [
      "./data/images/0154.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green cylindrical device suspended by cables and inspect its lower attachment points.",
    "output": {
      "reasoning": "The illuminated green cylindrical device is centrally located in the upper portion of the image, suspended by multiple thin cables extending downward. The robot appears to be positioned below and slightly to the left of the device, with a complex cable/net structure in the foreground obstructing direct access. To inspect the lower attachment points (where cables connect to the device’s base), I must: (1) move forward cautiously to close distance while avoiding entanglement with foreground cables; (2) adjust lateral position to center under the device for optimal viewing angle; (3) ascend slightly to bring the lower attachment region into clear view; (4) stabilize and hover to enable detailed inspection. The green illumination helps visibility, but turbidity and cable density require precise, incremental movements.",
      "action": [
        "Go straight slowly",
        "Move right slightly",
        "Ascend gently",
        "Hover and inspect lower attachment points"
      ]
    },
    "images": [
      "./data/images/0288.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-and-black striped coral structure with white nodules, maintaining a safe distance to avoid contact while observing its texture and surrounding marine life.",
    "output": {
      "reasoning": "The target is the large green-and-black striped coral structure with prominent white nodules, centrally located in the image and illuminated from above. It is surrounded by other corals (blue branching, yellow, and beige), but no immediate obstacles block a direct approach. To observe texture and surrounding marine life safely, I should first align the robot directly toward the structure, then advance slowly while maintaining a safe standoff distance—ensuring the structure remains centered in view and not too close to cause contact. Given its size and central position, a straight-forward approach with minor fine-tuning is optimal. No turning is needed initially since the structure is already in front.",
      "action": [
        "Go straight slowly",
        "Maintain center alignment with the coral structure",
        "Pause briefly to observe texture and surrounding marine life"
      ]
    },
    "images": [
      "./data/images/0207.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange sea anemone in the foreground while maintaining a safe distance from the surrounding coral formations.",
    "output": {
      "reasoning": "The large orange sea anemone is clearly visible in the foreground, centered slightly to the lower-left of the image. The robot (submersible) is positioned above and behind it, with several coral formations (green ridged corals and brown spherical ones) surrounding the anemone—especially to its right and rear. To approach safely, the robot should first descend gently to align horizontally with the anemone’s depth, then move forward while slightly adjusting left to stay centered on the target and avoid the nearby green coral to the right. Maintaining a safe distance means avoiding lateral drift into adjacent corals; thus, small corrective turns are needed before advancing straight.",
      "action": [
        "Descend slightly",
        "Turn left slightly",
        "Go straight slowly",
        "Maintain steady heading toward the orange anemone"
      ]
    },
    "images": [
      "./data/images/0164.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the left green columnar structure and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows two prominent green columnar structures; the left one is closer and more clearly visible, with circular openings and surface texture suggesting possible biofouling or damage. The robot is positioned slightly in front and to the right of the left column, with clear water path toward it. To approach and inspect its surface, I need to: (1) move leftward to align with the column’s midline, (2) advance forward to get within close inspection range (~1–2 m), and (3) possibly rotate slightly to scan the full circumference—though initial inspection can start from current frontal angle. No immediate obstacles block the path. Lighting from above creates shadows, so maintaining stable heading is important.",
      "action": [
        "Move left",
        "Go straight",
        "Hover and slowly rotate right to inspect surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0064_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the two ancient stone columns and inspect their structural integrity, focusing on visible cracks and marine growth on the right column.",
    "output": {
      "reasoning": "The image shows two ancient stone columns standing side by side, with the right column visibly covered in marine growth (algae/barnacles) and exhibiting apparent cracks—especially in its middle and lower sections. The robot is currently positioned at a moderate distance, facing both columns head-on. To inspect structural integrity as instructed, I must first approach the pair to get within close inspection range (~1–2 m), then reposition to focus on the right column’s surface details. Since the right column is the priority for crack and biofouling assessment, after initial approach, a slight rightward lateral movement will align the camera parallel to that column for optimal visual access. No immediate obstacles are present; the seabed appears clear.",
      "action": [
        "Go straight toward the two columns",
        "Stop when ~1.5 meters away",
        "Move slightly right to position alongside the right column",
        "Inspect the right column’s surface for cracks and marine growth"
      ]
    },
    "images": [
      "./data/images/0210.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red conical buoy floating above the coral reef and inspect its underside for attached sensors or fouling organisms.",
    "output": {
      "reasoning": "The red conical buoy is visible in the upper center of the image, floating above a coral reef. It is positioned slightly ahead and above the robot’s current depth. To inspect its underside, the robot must first ascend vertically to reach the buoy’s level, then approach horizontally from below to position itself directly beneath it. The coral structures are dense but there appears to be a clear vertical corridor near the center-left; thus, ascending straight while maintaining slight leftward offset avoids immediate collision with nearby coral. Once at buoy height, a short forward movement will bring the robot under the buoy, enabling visual inspection of its base (where sensors or fouling are likely attached).",
      "action": [
        "Ascend vertically while maintaining slight leftward offset to avoid nearby coral",
        "Move forward slowly toward the red conical buoy",
        "Position directly beneath the buoy’s base",
        "Hover and inspect the underside for sensors or fouling organisms"
      ]
    },
    "images": [
      "./data/images/0142.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange-banded sea pen on the right while avoiding the spiky sea urchins in the foreground.",
    "output": {
      "reasoning": "The large orange-banded sea pen is clearly visible on the right side of the image, standing vertically and illuminated by sun rays. In the foreground, several spiky sea urchins (with radial spines) are clustered near the seabed, particularly on the left and center-bottom. To approach the sea pen safely, the robot must avoid these urchins. Since the urchins occupy the lower-left and central foreground, the safest path is to move forward while slightly veering right—keeping distance from the urchins—and then proceed directly toward the sea pen once clear of obstacles. The sea pen is relatively close, so only a few controlled steps are needed.",
      "action": [
        "Turn slightly right",
        "Move forward slowly",
        "Approach the orange-banded sea pen"
      ]
    },
    "images": [
      "./data/images/0225.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car between the two stone pillars and inspect its front right side for damage or debris.",
    "output": {
      "reasoning": "The green car is positioned centrally between two stone pillars, slightly angled with its front right side facing the viewer. From the current perspective, the robot appears to be at a moderate distance in front of the car, with clear visibility of the target area. To inspect the front right side, the robot must first approach the car directly, then maneuver to position itself closer to that specific quadrant—ideally from the front-right diagonal to get an optimal inspection angle without occlusion. No immediate obstacles block the path, though scattered rocks on the seabed require cautious forward motion. The light beam from above illuminates the scene well, aiding navigation.",
      "action": [
        "Go straight toward the green car",
        "Turn slightly right to align with the front right side of the car",
        "Move forward slowly to get closer to the front right wheel and bumper area",
        "Hover and inspect the front right side for damage or debris"
      ]
    },
    "images": [
      "./data/images/temp_307_0073_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object on the seabed while avoiding the large rocks on either side.",
    "output": {
      "reasoning": "The red dome-shaped object is clearly visible on the seabed, centered in the lower part of the image. Two large rocks flank it: one on the left (larger, textured) and one on the right (partially visible, also large). The robot appears to be positioned slightly above and behind the target, with a clear forward path toward the red dome if it stays centered between the two rocks. To avoid collision, the robot should maintain a central trajectory—slightly adjusting left or right only as needed to preserve safe clearance. Since the dome is directly ahead and unobstructed in the midline, the optimal plan is: (1) descend gently toward the seabed while aligning with the dome’s center, (2) proceed straight forward, keeping equal distance from both side rocks, (3) stop upon reaching close proximity to the dome. No sharp turns are needed; minor course corrections may be applied during descent if drift occurs.",
      "action": [
        "Descend slowly while centering on the red dome",
        "Go straight forward, maintaining equal distance from left and right rocks",
        "Stop when near the red dome-shaped object"
      ]
    },
    "images": [
      "./data/images/temp_307_0042_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spherical coral structure and inspect its surface texture and openings.",
    "output": {
      "reasoning": "The large spherical coral structure is clearly visible in the center-right of the image, resting on the sandy seabed. It is relatively close, but to properly inspect its surface texture and openings (e.g., the prominent cavity on its left side), the robot needs to approach closer and position itself for optimal viewing. Given the current perspective, moving forward slightly will reduce distance, then a slight leftward adjustment will align the robot with the main opening and textured surface for detailed inspection. No immediate obstacles block the path.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0031_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the large green pipeline along the seabed while maintaining a safe distance from the nearby yellow buoy and structural support.",
    "output": {
      "reasoning": "The image shows a large green pipeline extending diagonally from the lower-left toward the upper-right, lying on the seabed. A yellow buoy is visible near the top-center, slightly above and to the right of the pipeline’s near section. A structural support (likely a platform base) is in the background left. To follow the pipeline while maintaining safe distance: (1) The robot should initially align parallel to the pipeline’s direction—i.e., move forward while keeping the pipeline on its right or left side consistently; (2) The yellow buoy is above and ahead—so as the robot advances, it must avoid drifting upward/right toward the buoy; (3) The structural support is behind and to the left, not an immediate hazard. Given the pipeline curves gently upward/right, the safest strategy is to stay slightly left of the pipeline (keeping it on the robot’s right), moving forward while subtly adjusting rightward to track the curve, but avoiding encroaching on the buoy’s vicinity. No sharp turns needed—just steady forward motion with minor course corrections.",
      "action": [
        "Go straight while keeping the green pipeline on your right",
        "Slightly turn right to follow the pipeline's gentle curve",
        "Maintain distance from the yellow buoy above and ahead",
        "Continue following the pipeline along the seabed"
      ]
    },
    "images": [
      "./data/images/0307.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the leaking pipe joint emitting blue-tinted bubbles and inspect the seal for damage.",
    "output": {
      "reasoning": "The image shows a large underwater pipe with a visible joint emitting a plume of blue-tinted bubbles—indicating a leak. The leak is located toward the left-middle of the frame, slightly elevated and upstream along the pipe axis. The robot appears to be positioned downstream (right side of image), facing the pipe. To approach the leaking joint for inspection, the robot should first move forward along the pipe’s axis, then adjust laterally if needed to center on the joint. Given the clear visibility and unobstructed path, a direct forward motion followed by fine positioning is optimal. The red-banded joints are reference points; the leaking one is the second joint from the left, distinguished by the bubble plume.",
      "action": [
        "Go straight toward the pipe",
        "Move closer to the leaking joint emitting blue bubbles",
        "Position directly in front of the joint for close inspection"
      ]
    },
    "images": [
      "./data/images/0120.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the right while avoiding the dark rocky debris on the left.",
    "output": {
      "reasoning": "The image shows a large, textured coral formation on the right side of the frame, with distinct ridged patterns and occupying a significant portion of the right half. On the left, there is dark, irregular rocky debris scattered near the seabed. The robot is currently facing forward, with the coral slightly ahead and to the right, and the debris to the left and slightly behind. To approach the coral while avoiding the debris, the robot should first steer right to align toward the coral, then proceed forward cautiously, maintaining distance from the left-side debris. A slight right turn followed by straight movement is sufficient given the current orientation and spacing.",
      "action": [
        "Turn right slightly",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0295.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible lying on the seabed and inspect its side portholes for damage or marine growth.",
    "output": {
      "reasoning": "The green cylindrical submersible is visible on the seabed, oriented horizontally, with its side portholes facing the camera. The robot appears to be positioned slightly in front and above it, with some rocky obstacles nearby but a clear approach path from the current viewpoint. To inspect the side portholes, the robot must first move closer laterally (to align parallel to the submersible’s side) and then proceed along its length at close range. Given the current perspective, the optimal first step is to move forward slightly to reduce distance, then adjust left/right to position alongside the mid-section of the cylinder where portholes are visible. Since the submersible lies horizontally and the portholes are on its side, a 'go straight' followed by 'turn right' or 'left' may be needed depending on relative offset — but from the image, the robot is slightly to the right of the submersible’s centerline, so moving forward and then gently turning left will align it parallel to the side. After alignment, proceed slowly along the side for inspection.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight alongside the submersible",
        "Pause and inspect side portholes"
      ]
    },
    "images": [
      "./data/images/0147.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large rusted cylindrical pipe structure and inspect its interior opening for obstructions or marine growth.",
    "output": {
      "reasoning": "The large rusted cylindrical pipe is directly in front of the robot, angled slightly to the right. Its open end faces the camera, and the interior is partially visible but obscured by shadow and distance. To inspect the interior opening for obstructions or marine growth, I need to: (1) move forward cautiously to reduce distance while maintaining alignment with the pipe’s axis; (2) adjust orientation slightly if needed to center the opening in the field of view; (3) get close enough for high-resolution inspection—ideally within 1–2 meters—without colliding with the rim or protruding bolts. The seabed appears stable, and no immediate obstacles block the approach path.",
      "action": [
        "Go straight slowly toward the pipe opening",
        "Slightly adjust left to center the pipe opening in view",
        "Continue forward until ~1.5 meters from the pipe entrance",
        "Hover and stabilize for visual inspection of interior"
      ]
    },
    "images": [
      "./data/images/0262.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue pipeline and inspect the yellow-banded joint near the coral formation.",
    "output": {
      "reasoning": "The image shows a large blue pipeline extending diagonally from the lower right toward the upper left, resting on the seabed near coral formations. A yellow-banded joint is visible mid-pipeline, adjacent to a prominent coral cluster (left foreground). The robot appears to be positioned slightly in front and to the left of the pipeline, with clear visibility of the target joint. To approach the yellow-banded joint for inspection, the robot should first move forward toward the pipeline, then adjust laterally (right) to align with the joint, and finally close in for a detailed view without disturbing the coral.",
      "action": [
        "Go straight toward the blue pipeline",
        "Turn slightly right to align with the yellow-banded joint",
        "Move forward slowly to inspect the joint closely"
      ]
    },
    "images": [
      "./data/images/0306.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue pickup truck and inspect its front bumper for damage or attached objects.",
    "output": {
      "reasoning": "The blue pickup truck is visible in the lower-left portion of the image, resting on the seabed. It is oriented with its front facing roughly toward the camera (slightly angled right). Sunlight penetrates from above, illuminating the scene and highlighting the truck’s front bumper area. To inspect the front bumper for damage or attached objects, I need to approach it directly while maintaining a stable, close-range viewing position. The path appears clear of immediate obstacles between the robot’s current position (assumed near center-top due to fisheye distortion) and the truck. First, I should descend slightly and move forward toward the truck; then, fine-tune positioning to align with the front bumper at close range for detailed inspection.",
      "action": [
        "Descend 1 meter",
        "Move forward toward the blue pickup truck",
        "Approach until 2 meters from the front bumper",
        "Hover and stabilize facing the front bumper",
        "Inspect the front bumper for damage or attached objects"
      ]
    },
    "images": [
      "./data/images/0201.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue cylindrical device with red top and yellow panels, then inspect its front-facing sensors while avoiding the nearby vertical textured structures.",
    "output": {
      "reasoning": "The blue cylindrical device with a red top and yellow panels is centrally located in the image, illuminated by a light source above. It is flanked on both sides by tall vertical textured structures (likely obstacles to avoid). The robot appears to be positioned slightly below and in front of the device. To approach it safely while avoiding the vertical structures, the robot should first move forward cautiously to close distance, then adjust laterally (slightly left or right) only if needed to maintain clearance—however, since the central path appears open and the device is directly ahead, a straight approach is feasible. Once near, the robot must orient to face the front of the device to inspect its sensors; this likely requires a slight upward tilt and frontal alignment. No immediate collision risk is visible in the direct forward path, so the priority is controlled forward motion followed by precise positioning for inspection.",
      "action": [
        "Go straight slowly toward the blue cylindrical device",
        "Maintain central alignment to avoid left and right vertical structures",
        "Stop at ~1 meter from the device",
        "Tilt upward slightly and center camera on front-facing sensors",
        "Hold position for sensor inspection"
      ]
    },
    "images": [
      "./data/images/0222.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car parked beside the moss-covered concrete pillar and inspect its driver-side door for damage.",
    "output": {
      "reasoning": "The green car is visible to the right of the moss-covered concrete pillar, parked parallel to it. The driver-side door (left side of the car from our perspective, since the car faces roughly toward the camera) is partially visible but not yet close enough for detailed inspection. To approach it safely and align for inspection, I should first move forward toward the car while staying slightly left of the pillar to avoid collision. Once near the car’s midsection, I will reposition laterally to face the driver-side door directly, then move closer for inspection.",
      "action": [
        "Go straight toward the green car, staying left of the concrete pillar",
        "Turn slightly left to align with the driver-side door",
        "Move forward slowly to inspect the driver-side door"
      ]
    },
    "images": [
      "./data/images/temp_307_0082_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-lit underwater vehicle positioned centrally among the coral formations.",
    "output": {
      "reasoning": "The yellow-lit underwater vehicle is clearly visible and centered in the image, surrounded by coral formations on both sides and slightly ahead. Sunlight rays indicate the vehicle is at moderate depth and well-illuminated. The robot appears to be facing it directly, with a clear forward path—no immediate obstacles block the direct approach. To safely approach, the robot should maintain central alignment and advance steadily while monitoring proximity to nearby corals.",
      "action": [
        "Go straight",
        "Maintain center alignment",
        "Approach slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0046_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical underwater vehicle marked with the number 9 and inspect its side panel for damage or debris.",
    "output": {
      "reasoning": "The large green cylindrical underwater vehicle marked with '9' is visible in the center-right of the image, oriented horizontally. The robot appears to be positioned slightly behind and to the left of the vehicle, with clear water ahead. To approach and inspect its side panel (likely the port or starboard side facing the camera), I need to: (1) move forward toward the vehicle while maintaining safe distance, (2) adjust lateral position to align parallel to the side panel, and (3) close in for detailed inspection. Given the vehicle’s orientation and current perspective, moving straight ahead first brings us closer; then a slight right turn positions us alongside the mid-section of the vehicle where the side panel is accessible. No immediate obstacles block the path.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Go straight slowly",
        "Hover parallel to the side panel of the green vehicle"
      ]
    },
    "images": [
      "./data/images/0117.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large yellow-green striped coral formation on the left while maintaining a safe distance from the surrounding smaller corals and fish.",
    "output": {
      "reasoning": "The large yellow-green striped coral formation is prominently visible on the left side of the image, with its vertical ridges and bright coloration distinguishing it from surrounding elements. Smaller corals (e.g., purple and green bushy types) and fish are scattered nearby, especially in the mid- and foreground, requiring careful navigation. To approach safely: first, orient toward the left while avoiding immediate foreground obstacles (e.g., the large curved yellow-green structure in the lower-left corner); then proceed forward at a moderate pace, maintaining lateral clearance from smaller corals; finally, adjust heading slightly to center the target formation in view without closing too quickly. The robot should avoid sudden turns or forward surges to prevent disturbing fish or contacting fragile corals.",
      "action": [
        "Turn left gently by ~15 degrees",
        "Move forward slowly for 2 meters",
        "Maintain slight leftward bias to keep target coral centered",
        "Pause briefly to assess proximity to smaller corals",
        "Continue forward at reduced speed until 1.5 meters from target"
      ]
    },
    "images": [
      "./data/images/0161.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone structure with columns and inspect the central opening for potential entry or artifacts.",
    "output": {
      "reasoning": "The image shows a submerged stone structure with two prominent columns and a central opening, located ahead and slightly to the right of the robot's current position. A blue vehicle is visible on the left, but it is not relevant to the task. The goal is to approach the structure and inspect the central opening. Given the clear path forward and slight rightward offset of the structure, the optimal plan is: (1) move forward toward the structure while maintaining alignment with the central opening, (2) adjust slightly right if needed to center the opening in view, (3) continue forward until close enough for detailed inspection (e.g., within 2–3 meters), and (4) stabilize and orient camera toward the opening for inspection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the stone structure",
        "Slightly turn right to align with the central opening",
        "Continue forward until near the entrance",
        "Hover and focus camera on the central opening for inspection"
      ]
    },
    "images": [
      "./data/images/0185.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged vertical structure with the cross-shaped top while avoiding the large green kelp fronds in the foreground.",
    "output": {
      "reasoning": "The target is the submerged vertical structure with a distinct cross-shaped top, located centrally in the background. Large green kelp fronds dominate the foreground and left side, obstructing direct forward movement. To avoid them, I must first steer right to clear the dense kelp on the left and center, then proceed straight toward the structure once a clear path is established. The structure appears reachable in ~3–4 meters; after clearing kelp, a slight left correction may be needed to center on it, but initial priority is rightward evasion.",
      "action": [
        "Turn right",
        "Go straight",
        "Go straight",
        "Slightly turn left",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0268.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red spherical object suspended on a vertical pole and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The red spherical object is clearly visible, suspended centrally on a vertical pole, with sunlight rays highlighting its position. The robot (visible as a dark silhouette in the lower-left foreground) is currently at a moderate distance and slightly to the left and below the target. To approach for inspection, the robot should first move forward toward the object while maintaining alignment with the pole. As it gets closer, a slight rightward correction may be needed to center the sphere in the field of view, followed by a slow upward movement to bring the camera face-on to the sphere’s surface for detailed inspection. No immediate obstacles block the direct path, though the large red structure on the right suggests caution to avoid drifting too far right.",
      "action": [
        "Go straight toward the red spherical object",
        "Slightly turn right to center the sphere in view",
        "Move upward slowly to align camera with the sphere's equator",
        "Hold position and inspect the surface"
      ]
    },
    "images": [
      "./data/images/0273.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central bioluminescent coral structure and gently inspect its upper frilled layers while avoiding contact with the surrounding yellow-tipped polyps.",
    "output": {
      "reasoning": "The image shows a central bioluminescent coral structure with intricate upper frilled layers (pink-green ridges) and surrounding yellow-tipped polyps arranged radially at the base. The robot is currently positioned at a moderate distance, facing the structure head-on. To safely approach and inspect the upper frills without contacting the yellow-tipped polyps: (1) First, move forward slowly to reduce distance while maintaining central alignment; (2) Then, ascend slightly to position the camera/viewpoint level with the upper frilled layers; (3) Finally, perform a gentle lateral drift (e.g., slight left or right) to get an oblique view of the frills—ensuring the robot’s body remains clear of the protruding yellow polyps below. Given symmetry, a small rightward drift is safe and provides optimal viewing angle of the frills’ texture.",
      "action": [
        "Go straight slowly",
        "Ascend slightly",
        "Drift right gently"
      ]
    },
    "images": [
      "./data/images/0091.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue and white mesh-covered vertical pipe structure and inspect its base where it connects to the seafloor.",
    "output": {
      "reasoning": "The blue and white mesh-covered vertical pipe structure is clearly visible in the center-right of the image, standing upright with its base resting on the seafloor (green seabed visible at bottom). The robot appears to be positioned slightly left and front of the structure, at a moderate distance (~2–3 m). To inspect the base where it connects to the seafloor, the robot must: (1) move forward toward the structure, (2) adjust laterally (slightly right) to align with the base’s centerline, and (3) descend gently to get a close-up view of the junction. No immediate obstacles block the path; the horizontal pipe behind is at a safe distance. Lighting from above creates glare but does not obscure the target.",
      "action": [
        "Go straight toward the vertical pipe structure",
        "Turn slightly right to center on the base",
        "Descend slowly to inspect the pipe-seafloor connection"
      ]
    },
    "images": [
      "./data/images/0129.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical structure with the vertical pipe and inspect its base near the orange crate-like objects.",
    "output": {
      "reasoning": "The green cylindrical structure with a vertical pipe is clearly visible in the center-right of the image. Orange crate-like objects are positioned to its left and slightly in front (from the robot’s perspective). The robot appears to be at a moderate distance, with a clear path toward the structure. To inspect the base near the orange crates, I need to: (1) move forward toward the green structure, (2) adjust laterally (left) to align with the base region adjacent to the orange crates, and (3) descend slightly if needed to get a closer view of the base—though current depth seems adequate. No immediate obstacles block the path. The lighting is sufficient for visual inspection.",
      "action": [
        "Go straight toward the green cylindrical structure",
        "Turn left slightly to align with the base near the orange crates",
        "Move forward slowly to inspect the base area"
      ]
    },
    "images": [
      "./data/images/0159.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral formation while maintaining a safe distance from the yellow fish swimming above it.",
    "output": {
      "reasoning": "The large green ridged coral formation is the primary target, centrally located and prominent in the image. A yellow fish is swimming directly above it, slightly to the right of the coral’s peak. To approach safely, I must avoid ascending toward the fish and instead maintain a lower trajectory—likely slightly below or beside the coral’s midsection. The robot appears to be positioned at a moderate distance in front of the coral. Given the current perspective, the safest approach path is to move forward while slightly angling left (away from the fish’s position) and keeping depth stable or slightly descending to stay beneath the fish’s swim path.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Maintain current depth or descend gently"
      ]
    },
    "images": [
      "./data/images/0134.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green orb on the seabed while avoiding the large carved stone structures on either side.",
    "output": {
      "reasoning": "The glowing green orb is visible on the seabed in the lower-right portion of the image, illuminated by a beam of light. Two large carved stone structures flank the central path: one on the left (closer, with facial features) and one on the right (taller, partially in shadow). The orb lies between them but slightly right-of-center. To approach it safely while avoiding collision, the robot should first center itself in the open corridor between the stones, then proceed forward while maintaining clearance—slightly favoring the left side to avoid the right stone’s protrusions, as the orb is closer to the right but still accessible from a central-left trajectory. The seabed appears relatively flat ahead, so no vertical adjustment is needed yet.",
      "action": [
        "Turn slightly left to align with the central passage between the two stone structures",
        "Go straight slowly while keeping equal distance from both stones",
        "Adjust right slightly as the orb comes into closer view to position directly in front of it",
        "Descend gently toward the seabed to reach the orb"
      ]
    },
    "images": [
      "./data/images/0275.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large central sponge-like structure and inspect its central cavity for signs of marine life or sediment accumulation.",
    "output": {
      "reasoning": "The image shows a large, central sponge-like structure with a prominent central cavity directly ahead of the robot's current viewpoint. The structure is well-lit and centered in the frame, indicating it is within close proximity and aligned with the robot’s forward direction. To inspect the central cavity for marine life or sediment, the robot must first approach closer to ensure high-resolution observation, then position itself to peer into the cavity—likely requiring a slight upward or forward tilt and stabilization. No immediate obstacles block the direct path, though surrounding rock formations suggest cautious forward motion is needed to avoid collision. Step-by-step: (1) Advance slowly toward the structure to reduce distance; (2) Fine-tune position to center the cavity in view; (3) Hover and stabilize for inspection.",
      "action": [
        "Go straight slowly",
        "Adjust position to center the cavity in view",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/0132.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange submersible vehicle resting on the seabed and inspect its front viewport and surrounding structure for damage or marine growth.",
    "output": {
      "reasoning": "The orange submersible vehicle is directly in front of the robot, resting on the seabed. Its front viewport is clearly visible and centered in the frame. To inspect the front viewport and surrounding structure, I need to approach carefully without disturbing sediment. Given its proximity and frontal orientation, the optimal sequence is: (1) move forward slowly to get closer for detailed inspection; (2) once near, perform a slight upward tilt or hover to align the camera with the viewport center; (3) optionally execute a slow clockwise rotation to survey the surrounding hull structure (left side first, then front-right). No obstacles block the direct path, but bubbles above suggest recent movement—so proceed steadily to avoid turbulence.",
      "action": [
        "Go straight slowly",
        "Hover and adjust pitch slightly upward",
        "Rotate clockwise 30 degrees while maintaining position",
        "Inspect front viewport and hull for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/0204.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged pickup truck and inspect its front right wheel area for damage or entanglement.",
    "output": {
      "reasoning": "The submerged pickup truck is clearly visible in the center of the image, oriented diagonally with its front facing slightly left and its right side more exposed to the camera. The front right wheel area is partially visible but not yet close enough for detailed inspection—it appears to be near the foreground yellow cylindrical structures (possibly pipes or buoys), which may pose proximity constraints. To inspect the front right wheel, I need to: (1) approach the vehicle from the front-right quadrant to minimize obstruction; (2) maintain safe distance from surrounding objects; (3) position the robot directly in front of and slightly to the right of the truck to get a clear view of the wheel well and tire. Given the current perspective, moving forward and slightly right will align the robot with the target zone.",
      "action": [
        "Move forward slowly",
        "Turn slightly right",
        "Approach the front right wheel area of the truck",
        "Hover steadily in front of the front right wheel for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0058_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-encrusted cylindrical structure on the left and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a large green-encrusted cylindrical structure prominently on the left side of the frame, consistent with the target described. It appears to be a vertical piling or column covered in algae/biofouling, with visible surface irregularities (holes, erosion) suggesting potential marine growth or damage. The robot is currently at a moderate distance and slightly angled away from it. To inspect its surface closely, I need to: (1) reorient toward the structure (it's on the left, so turn left), (2) advance straight toward it while maintaining safe proximity, and (3) position for close visual inspection—likely requiring a slight upward or lateral adjustment to scan the surface systematically. Given the clear visibility and lack of immediate obstacles, a direct approach is feasible.",
      "action": [
        "Turn left",
        "Go straight toward the green cylindrical structure",
        "Move closer until within 1 meter of the structure",
        "Hover and slowly ascend while scanning the surface"
      ]
    },
    "images": [
      "./data/images/0303.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted green car and inspect its open driver-side door for potential artifacts or structural damage.",
    "output": {
      "reasoning": "The rusted green car is clearly visible in the center-left of the image, resting on the seabed with its driver-side door open. The robot appears to be positioned slightly in front and to the right of the vehicle. To approach the open driver-side door for inspection, the robot should first move forward toward the car, then adjust leftward to align with the open door. Given the relatively clear path and absence of immediate obstacles near the target area, a direct approach is feasible. After reaching proximity, a slight turn may be needed to orient the camera toward the interior of the door frame for detailed inspection.",
      "action": [
        "Go straight toward the car",
        "Turn left slightly to align with the open driver-side door",
        "Move forward slowly to get closer to the door opening",
        "Hover and inspect the interior edge and frame of the open door"
      ]
    },
    "images": [
      "./data/images/temp_307_0020_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral structure from the front-left while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The large green coral structure is centrally located in the image, with its front-left side visible and accessible. The robot appears to be positioned slightly to the right and behind the coral (based on shadow direction and perspective). To approach from the front-left while maintaining a safe distance, I need to: (1) first reposition leftward to align with the front-left quadrant, (2) then advance cautiously toward it without drifting into contact—keeping a buffer of ~0.5–1 m (visually estimated from scale of surrounding seabed texture), and (3) possibly fine-tune orientation to face the target squarely upon arrival. No immediate obstacles block the path; the seabed is clear and flat.",
      "action": [
        "Move left",
        "Go forward slowly",
        "Adjust heading slightly left to face the front-left side of the coral",
        "Maintain steady distance while approaching"
      ]
    },
    "images": [
      "./data/images/0098.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-eyed robotic fish in the center while maintaining a safe distance from the surrounding coral formations.",
    "output": {
      "reasoning": "The yellow-eyed robotic fish is clearly centered in the image, slightly above the seabed and illuminated by light rays from above. Coral formations are present on both left and right sides, as well as in the foreground (bottom), indicating constrained lateral maneuverability. To approach safely: (1) First, ensure forward motion is aligned with the fish’s position—no immediate turn needed since it's already centered. (2) Maintain a moderate distance to avoid contact with nearby corals; given the fish is elevated, staying slightly below and centered minimizes collision risk. (3) Proceed slowly and steadily forward while monitoring proximity to corals on either side. No sharp turns required at this stage.",
      "action": [
        "Go straight slowly",
        "Maintain center alignment with the robotic fish",
        "Monitor left and right coral proximity and adjust laterally if needed"
      ]
    },
    "images": [
      "./data/images/temp_307_0055_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall ribbed cylindrical structure in the center while avoiding the surrounding coral formations and scattered debris.",
    "output": {
      "reasoning": "The tall ribbed cylindrical structure is clearly centered in the image, illuminated by overhead light rays. It is surrounded by coral formations (to the right and lower-left) and scattered debris (e.g., disc-shaped object in lower-left foreground). To approach safely: (1) First, move forward cautiously to close distance while maintaining central alignment with the cylinder; (2) Observe that the left side has a large disc-shaped debris and dense coral, while the right side has coral but appears slightly more open near the base—however, since the cylinder is centered and the path directly ahead is relatively clear (sunlight beams suggest open water column), the safest initial maneuver is straight advance; (3) As we get closer, minor lateral adjustments may be needed, but the first atomic step is to go straight toward the center target. No immediate turn is required because the direct path is unobstructed at current range.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0191.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the bright central light source while maintaining alignment with the concentric green structural rings.",
    "output": {
      "reasoning": "The image shows a bright central light source surrounded by concentric green structural rings, forming a tunnel-like structure. The robot (visible on the left) is currently offset to the left and slightly angled relative to the central axis. To approach the light while maintaining alignment with the rings, the robot must first correct its lateral position to center itself within the ring structure, then proceed straight toward the light. Since the rings are symmetric and visually guide the central axis, minor rightward correction followed by forward motion will preserve alignment. No obstacles block the path directly ahead.",
      "action": [
        "Turn slightly right to center within the green rings",
        "Go straight toward the bright central light"
      ]
    },
    "images": [
      "./data/images/0149.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green segmented cylindrical object lying on the seabed and inspect its front end for markings or damage.",
    "output": {
      "reasoning": "The large green segmented cylindrical object is clearly visible on the seabed, oriented horizontally with its front end (narrower, capped end) facing toward the right side of the image. The robot appears to be positioned slightly left and rearward relative to the object. To approach and inspect the front end, the robot should first move forward to reduce distance, then adjust laterally (right) to align with the front axis, and finally move closer for detailed inspection. No immediate obstacles block the path; coral formations are present but leave a clear corridor to the object’s front. Lighting from above creates shadows, but the front end is well-illuminated and accessible.",
      "action": [
        "Go straight toward the green cylindrical object",
        "Turn right to align with the front end of the object",
        "Move forward slowly to inspect the front end for markings or damage"
      ]
    },
    "images": [
      "./data/images/0097.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, textured vertical rock formation in the center while avoiding the surrounding coral clusters and maintaining a safe distance from the yellow fish on the right.",
    "output": {
      "reasoning": "The target is the tall, textured vertical rock formation centered in the image. Coral clusters are visible to the left and right of the rock, requiring lateral clearance. A yellow fish is present on the right side, near the coral, so the robot should avoid approaching too closely from that direction. The safest approach path is slightly from the left-front quadrant—maintaining distance from both the right-side coral and the yellow fish—while moving straight toward the rock's base. Initial small correction: slight left turn to align away from the right-side hazards, then proceed forward steadily.",
      "action": [
        "Turn slightly left",
        "Go straight toward the central rock formation",
        "Maintain steady forward motion while keeping the yellow fish and right-side coral on the right edge of view"
      ]
    },
    "images": [
      "./data/images/0096.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green net structure ahead while avoiding entanglement with the surrounding suspended fishing nets.",
    "output": {
      "reasoning": "The image shows a complex underwater scene dominated by suspended fishing nets. A brightly illuminated green net structure is visible directly ahead, slightly elevated and centered in the field of view. Surrounding it are numerous loose, tangled nets forming potential entanglement hazards—especially on the left and right flanks, and some strands crossing the foreground. To safely approach the target, the robot must maintain a central, slightly elevated trajectory to avoid lower hanging strands, while carefully monitoring lateral clearance. The illumination suggests the green structure is the intended goal. Given the density of nets, small, controlled movements are essential: first stabilize heading toward the center of the green structure, then advance cautiously while adjusting pitch to stay above lower net layers.",
      "action": [
        "Stabilize heading toward the center of the illuminated green net structure",
        "Gently ascend 0.5 meters to clear lower net strands",
        "Advance straight forward slowly (0.3 m/s) for 2 meters",
        "Scan left and right for net proximity; if within 0.8 m, adjust laterally outward by 0.2 m",
        "Continue advancing toward the green structure while maintaining 1.0 m clearance from surrounding nets"
      ]
    },
    "images": [
      "./data/images/0252.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the circular metallic object suspended in the water column while avoiding the coral formations on the seabed.",
    "output": {
      "reasoning": "The circular metallic object is suspended in the water column, centered and slightly above mid-frame, illuminated by sun rays. Coral formations are prominent on the seabed (bottom-left and bottom-right), posing a risk if the robot descends too low or drifts laterally toward them. To approach safely: (1) Maintain neutral buoyancy and ascend slightly to clear the coral canopy; (2) Navigate forward while keeping the target centered; (3) Adjust laterally only if needed to stay clear of corals—currently, the central path appears open. The safest initial action is to go straight while maintaining depth just above the highest coral peaks.",
      "action": [
        "Ascend slightly",
        "Go straight toward the circular metallic object",
        "Maintain center alignment and monitor coral proximity"
      ]
    },
    "images": [
      "./data/images/0162.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large ribbed coral structure in the center while avoiding the spiky green coral to its left.",
    "output": {
      "reasoning": "The image shows a large ribbed coral structure in the center (target), with a spiky green coral to its left (obstacle). The robot is positioned slightly in front and below the target. To approach the central ribbed coral while avoiding the spiky green coral on the left, the robot should first move slightly right to clear the obstacle’s lateral proximity, then proceed straight toward the center. A small rightward correction ensures safe clearance before advancing.",
      "action": [
        "Turn slightly right",
        "Go straight toward the large ribbed coral"
      ]
    },
    "images": [
      "./data/images/0127.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall central column covered in green algae while avoiding the surrounding rocky outcrops.",
    "output": {
      "reasoning": "The image shows a tall central column covered in green algae, clearly the target. It is surrounded by rocky outcrops on the left, right, and rear—especially prominent on the left (large yellow-green rock) and right (dark vertical rock). The robot appears to be positioned slightly in front and possibly offset to the left of the column. To approach safely while avoiding obstacles: (1) First, reposition slightly rightward to clear the left-side rock and align with the column’s centerline; (2) Then proceed straight forward toward the column, maintaining central alignment; (3) As it nears, fine-tune to stay centered and avoid any protrusions on the column base or nearby smaller rocks. No immediate upward/downward motion is needed—the column is fully visible at current depth.",
      "action": [
        "Turn right slightly",
        "Go straight toward the central column",
        "Adjust position to stay centered on the column while approaching"
      ]
    },
    "images": [
      "./data/images/0095.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe with green end caps and inspect its surface for damage or biofouling.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe with green end caps positioned diagonally across the field of view, extending from left-mid to right-rear. The robot appears to be slightly below and to the left of the pipe’s main body. To approach for inspection, the robot should first move forward toward the nearest visible section (left green cap), then adjust laterally to align parallel to the pipe axis, and finally proceed along its length for surface inspection. Given the pipe’s orientation and proximity, the initial action is to go straight toward the left green end cap while maintaining safe distance to avoid collision with surrounding structures (e.g., the vertical pipe junction on the right).",
      "action": [
        "Go straight toward the left green end cap",
        "Adjust right slightly to align parallel to the pipe axis",
        "Move forward slowly along the pipe surface",
        "Inspect the pipe surface for damage or biofouling"
      ]
    },
    "images": [
      "./data/images/0130.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of glowing green and orange cup-shaped corals while maintaining a safe distance from the surrounding dark rock formations.",
    "output": {
      "reasoning": "The image shows a dense cluster of glowing green and orange cup-shaped corals centered in the frame, with dark rock formations surrounding them—particularly on the left and right peripheries. The robot appears to be positioned slightly above and in front of the coral cluster, with clear visibility toward the target. To approach safely: (1) First, descend slightly to align horizontally with the coral cluster (they appear slightly below current viewpoint due to lighting and perspective). (2) Then, move forward slowly while monitoring proximity to dark rocks—since rocks are lateral, maintaining central heading should keep safe distance. (3) Stop when within optimal observation range (~1–2 m), avoiding contact with any coral or rock edges. No sharp turns needed; straight-line approach is feasible given open frontal space.",
      "action": [
        "Descend slightly",
        "Move forward slowly",
        "Maintain central heading",
        "Stop at safe observation distance"
      ]
    },
    "images": [
      "./data/images/0122.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure on the left with the circular openings and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows an underwater scene with several green cylindrical structures featuring circular openings and textured surfaces. The largest such structure is positioned on the left side of the frame, partially illuminated by sun rays from above. It appears to be the target described in the instruction. To approach it, the robot should first move toward the left while maintaining forward progress, then close in for inspection. Since the structure is already visible and relatively nearby, a short sequence of directional movements suffices: turn left to align, go straight toward it, and then move closer for surface inspection.",
      "action": [
        "Turn left",
        "Go straight",
        "Move closer to the large green cylindrical structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0004_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red ribbed lantern on the seabed while avoiding the nearby striped sea cucumbers.",
    "output": {
      "reasoning": "The red ribbed lantern is the primary target, located centrally on the seabed and well-lit by sun rays. Two striped sea cucumbers are visible: one to the left of the lantern (partially in frame), and another to the right and slightly behind the lantern. To approach safely while avoiding both, I should first move forward cautiously toward the lantern’s general direction, then adjust laterally—preferably to the right side of the lantern (since the right-side cucumber is farther back and more visible, allowing better clearance) or left if the left cucumber is less obstructive. Given the lantern is directly ahead and the left cucumber is closer to the robot’s current position (left edge of image), a safer initial path is to advance straight, then veer slightly right to pass between the lantern and the right cucumber, keeping distance from both. The seabed appears relatively flat and clear in the central corridor, supporting this path.",
      "action": [
        "Go straight slowly",
        "Turn slightly right",
        "Advance toward the red lantern",
        "Maintain distance from striped sea cucumbers on both sides"
      ]
    },
    "images": [
      "./data/images/temp_307_0023_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large purple coral formation on the left while maintaining a safe distance from the submarine above.",
    "output": {
      "reasoning": "The image shows a submarine positioned above and slightly to the right of the robot’s current view. A large purple coral formation is visible on the left side of the frame, with other corals nearby. To approach the purple coral while avoiding the submarine overhead, the robot should first move leftward toward the coral, then adjust forward motion while maintaining lateral clearance from the submarine. Since the submarine is above, vertical avoidance is less critical than horizontal positioning—so we prioritize lateral movement first, then controlled forward advance. The coral is already in view, so no rotation is needed initially; a gentle left turn followed by straight motion will bring the robot closer safely.",
      "action": [
        "Turn left slightly",
        "Go straight slowly",
        "Maintain leftward offset to keep distance from submarine"
      ]
    },
    "images": [
      "./data/images/0118.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine-like robot emerging from the cave entrance, keeping a safe distance to avoid collision while observing its orientation and lighting.",
    "output": {
      "reasoning": "The green submarine-like robot is visible emerging from the cave entrance, centered in the frame and slightly to the left of the cave’s opening. It faces forward (toward the camera), with its lights on, illuminating the water ahead. The cave walls form a narrow corridor; to approach safely, I must avoid the rocky edges on both sides. Given its position and orientation, the safest path is to advance slowly along the central axis of the cave opening while maintaining lateral clearance—approximately 1–2 meters—to prevent collision. Since it’s stationary or moving slowly (no motion blur), I can proceed cautiously straight ahead, then adjust laterally if needed based on real-time observation. No immediate obstacles block the direct path, but I should monitor for drift or sudden movement.",
      "action": [
        "Go straight slowly toward the cave entrance",
        "Maintain center alignment with the submarine’s longitudinal axis",
        "Hold distance at ~1.5 meters from the submarine’s hull",
        "Observe its orientation and light pattern continuously"
      ]
    },
    "images": [
      "./data/images/0188.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical pipe assembly on the left and inspect its flange connection for signs of corrosion or leakage.",
    "output": {
      "reasoning": "The green cylindrical pipe assembly is clearly visible on the left side of the image, occupying the foreground. Its flange connection (the circular bolted joint with visible bolts and gasket area) is exposed and accessible. The robot is currently positioned at a moderate distance and slightly angled—likely from the front-right relative to the pipe. To inspect the flange for corrosion or leakage, the robot must: (1) move closer to reduce distance for high-resolution visual inspection, (2) adjust orientation to face the flange directly (i.e., align camera normal to the flange plane), and (3) stabilize at optimal inspection distance (~0.5–1 m). Given the current view, the most efficient path is to advance straight while slightly turning left to center the flange in the field of view.",
      "action": [
        "Turn left slightly",
        "Go straight toward the green pipe assembly",
        "Stop when 1 meter away from the flange",
        "Adjust orientation to face the flange directly"
      ]
    },
    "images": [
      "./data/images/temp_307_0033_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar while avoiding the green net-like structures overhead and the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and illuminated in the center of the image, serving as the primary target. Overhead, green net-like structures span across the top of the frame, requiring the robot to maintain a safe vertical clearance—likely by staying centered and not drifting upward. On the seabed, scattered debris (dark rectangular objects) are present near the base of the pillar and to the left/right; these must be avoided laterally during final approach. The robot appears to be positioned directly in front of the pillar at moderate distance. To safely approach: (1) proceed straight toward the pillar while maintaining neutral heading and depth; (2) monitor for lateral drift toward debris; (3) as proximity increases, fine-tune position to avoid seabed obstacles. No immediate turn is needed—the path is clear ahead.",
      "action": [
        "Go straight toward the central stone pillar",
        "Maintain neutral depth to avoid overhead green nets",
        "Slightly adjust left/right as needed to avoid seabed debris during final approach"
      ]
    },
    "images": [
      "./data/images/temp_307_0044_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green brain coral structure from the front while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The image shows a large green brain coral structure directly in front of the robot, occupying most of the frame. It is well-lit and clearly visible, with its textured surface indicating proximity. Since the instruction specifies approaching *from the front* while maintaining a *safe distance*, the robot should not move forward aggressively. Instead, it should first ensure alignment with the coral’s central axis (i.e., confirm it's facing it squarely), then proceed with small, controlled forward movements while monitoring distance—ideally using visual cues like relative size change or shadow depth. Given the current perspective, the robot appears already aligned frontally; thus, the immediate action is to advance cautiously without lateral deviation.",
      "action": [
        "Maintain current heading (face directly toward coral)",
        "Move forward slowly by 0.5 meters",
        "Pause and assess distance (ensure >0.3 m clearance)",
        "If still safe, move forward another 0.3 meters"
      ]
    },
    "images": [
      "./data/images/0258.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine from the left while maintaining a safe distance, and inspect its hull for visible damage or marine growth.",
    "output": {
      "reasoning": "The green submarine is positioned diagonally in the scene, oriented from lower-left to upper-right, with its bow pointing toward the upper right. The robot appears to be currently located slightly ahead and to the right of the submarine’s midsection (based on perspective and shadow). To approach from the left while maintaining a safe distance, the robot should first reposition laterally to the submarine’s left side—i.e., move left relative to current heading. Then, it should advance slowly along the left flank, keeping parallel alignment to inspect the hull. The coral formation on the left foreground suggests potential obstacle; thus, initial movement must avoid collision by adjusting depth or lateral offset if needed. Given lighting from above and clear visibility, a controlled leftward translation followed by forward motion along the hull is feasible.",
      "action": [
        "Move left to position yourself on the submarine's left side",
        "Advance slowly parallel to the submarine's hull",
        "Maintain steady distance (~1–2 m) while scanning the hull surface",
        "Pause briefly at midsection and stern to inspect for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/0167.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spotted rock in the foreground and inspect the orange cone and yellow spherical object resting on its surface.",
    "output": {
      "reasoning": "The large spotted rock is clearly visible in the foreground, centered and slightly to the right. On its surface rest two objects: an orange cone (left side of the rock) and a yellow spherical object (right side of the rock). The robot appears to be positioned at a moderate distance (~1–2 m) from the rock, with clear visibility and no immediate obstacles. To inspect both objects closely, the robot should first approach the rock directly, then position itself to view both items—ideally hovering near the center of the rock’s top surface. Since the instruction specifies 'approach' and 'inspect', the sequence should prioritize closing distance, then fine positioning for visual inspection.",
      "action": [
        "Go straight toward the large spotted rock",
        "Slow down as you near the rock surface",
        "Hover steadily above the rock, centered between the orange cone and yellow sphere",
        "Tilt camera downward slightly to focus on both objects"
      ]
    },
    "images": [
      "./data/images/0193.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central corroded metallic pipe structure while avoiding the surrounding porous rock formations and green coral-like growths.",
    "output": {
      "reasoning": "The central corroded metallic pipe structure is clearly visible in the image, positioned vertically and centered in the frame. Surrounding it are porous rock formations on the left and right, and green coral-like growths at the bottom-right and lower-left. The robot appears to be slightly below and in front of the pipe, with a clear upward-forward path toward its midsection. To avoid obstacles: (1) the left-side rock is close, so slight rightward correction is needed; (2) the green coral at bottom-right suggests avoiding low-right trajectories; (3) the pipe’s upper section is illuminated, indicating it's accessible from current depth. A safe approach path is: first ascend slightly while moving forward, then gently correct right to stay clear of left rock, maintaining central alignment toward the pipe’s midsection.",
      "action": [
        "Go straight and slightly ascend",
        "Turn right slightly",
        "Continue straight toward the central pipe"
      ]
    },
    "images": [
      "./data/images/0284.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green coral-topped structure and inspect its base for attached sensors or biofouling.",
    "output": {
      "reasoning": "The glowing green coral-topped structure is centrally located in the image and clearly visible. It stands vertically on the seabed, with its base near the sediment surface. The robot appears to be positioned slightly in front and possibly a bit to the left of the structure (based on perspective and lighting). To inspect the base for sensors or biofouling, the robot must first close the distance safely—avoiding nearby netting on the right and coiled cable on the left—and then maneuver to a position directly facing or slightly below the base for optimal visual inspection. Given the current view, the most efficient path is to advance straight toward the structure, then fine-tune positioning once closer.",
      "action": [
        "Go straight toward the glowing green structure",
        "Slow down as you approach the base",
        "Hover and adjust position to face the base directly",
        "Inspect the base area for attached sensors or biofouling"
      ]
    },
    "images": [
      "./data/images/0112.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical structure with yellow markings and inspect its surface for damage or fouling.",
    "output": {
      "reasoning": "The image shows a large submerged cylindrical structure with visible yellow markings (likely bands or stripes) running along its length. It is positioned diagonally across the frame, extending from the lower-left toward the upper-right, and appears to be suspended or resting near a netted barrier. The robot is currently at a moderate distance and slightly below/forward of the structure. To approach and inspect its surface, I need to: (1) move forward to reduce distance while maintaining safe clearance from the net on the left; (2) adjust heading to align parallel to the cylinder’s axis for optimal inspection; (3) close in laterally to bring the camera within inspection range (~1–2 m); (4) perform a slow lateral pass along the cylinder to visually scan for damage or fouling. Given lighting and turbidity, maintaining steady motion and orientation is critical.",
      "action": [
        "Go straight slowly",
        "Turn slightly right to align with the cylinder's axis",
        "Move forward closer to the cylinder",
        "Drift left gently to position alongside the cylinder",
        "Perform slow lateral inspection pass along the yellow-marked surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0036_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green-camouflaged underwater vehicle and inspect its front sensor array for damage or debris.",
    "output": {
      "reasoning": "The green-camouflaged underwater vehicle is visible in the center-left of the image, resting on a green base. Its front sensor array (a large circular aperture with a lens/light assembly) faces toward the camera. The robot is currently at a moderate distance and slightly to the right of the vehicle’s frontal axis. To inspect the front sensor array closely, I need to: (1) move forward to reduce distance, (2) adjust leftward to align directly with the front sensor, and (3) stop at close range for visual inspection. No immediate obstacles block the path; the rocky terrain is to the right and rear, so a direct approach is safe.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight slowly",
        "Stop"
      ]
    },
    "images": [
      "./data/images/0123.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater passage between the left rock wall and the central coral-covered ridge, maintaining clearance from both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage bounded by a dark rock wall on the left and a coral-covered ridge in the center/right. Sunlight penetrates from above, illuminating the sandy floor and highlighting the passage’s corridor-like shape. The robot appears to be positioned at the entrance, facing into the passage. To navigate safely while maintaining clearance from both sides, the robot should first align centrally within the passage, then proceed forward with slight adjustments to avoid drifting toward either wall. Given the passage’s orientation (slightly curving rightward based on the ridge’s shape), a gentle rightward bias may help stay centered without hitting the left wall. No immediate obstacles are visible ahead, so forward motion is safe if controlled.",
      "action": [
        "Turn slightly right to center in the passage",
        "Go straight slowly",
        "Maintain slight rightward correction to avoid left wall",
        "Continue forward while monitoring distance to both walls"
      ]
    },
    "images": [
      "./data/images/temp_307_0014_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of dark, textured coral formations on the right while maintaining a safe distance from the rising bubble plume in the center.",
    "output": {
      "reasoning": "The image shows a rising bubble plume in the center, which must be avoided for safety. On the right side, there is a cluster of dark, textured coral formations—the target. The robot is currently positioned such that the coral cluster is to its right and slightly ahead, while the bubble plume lies directly ahead and slightly left of center. To approach the coral safely: (1) First, move slightly right to lateralize away from the bubble plume’s trajectory; (2) Then proceed forward while maintaining rightward bias to stay clear of the plume; (3) Finally, fine-tune position to get closer to the coral without encroaching on the plume’s proximity zone (estimated ~1–1.5 m radius based on bubble dispersion).",
      "action": [
        "Turn right slightly",
        "Go forward while staying right of the bubble plume",
        "Approach the dark coral cluster on the right cautiously"
      ]
    },
    "images": [
      "./data/images/0170.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater structure with three illuminated circular ports, maintaining a safe distance while aligning with its front face for inspection.",
    "output": {
      "reasoning": "The image shows a large cylindrical underwater structure with three illuminated circular ports arranged horizontally on its front face. The robot is currently positioned slightly below and to the left of the structure, with the front face clearly visible and oriented toward the camera. To approach while maintaining a safe distance and aligning with the front face, the robot should first correct its lateral position to center itself relative to the structure’s symmetry axis, then advance straight while monitoring distance to avoid collision. Given the current angle, a slight rightward correction is needed (since the structure appears left-of-center in the frame), followed by forward motion while keeping the three ports symmetrically framed.",
      "action": [
        "Turn slightly right",
        "Go straight slowly",
        "Maintain alignment so the three illuminated ports remain horizontally centered in view"
      ]
    },
    "images": [
      "./data/images/0172.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged metallic structure along its length while maintaining a safe distance to inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The image shows a large submerged metallic structure (likely a shipwreck or pipeline) extending diagonally from the lower-left toward the upper-right, with visible rivets, panels, and marine growth. The robot is currently positioned near the mid-section of the structure, slightly to its left side, at a moderate distance. To inspect along its length while maintaining a safe distance, the optimal path is to parallel-track alongside it—first moving forward (along the structure’s orientation) while keeping lateral offset constant. Given the structure’s diagonal alignment, 'forward' for the robot should align with the structure’s longitudinal axis. The current view suggests the structure continues ahead and upward; thus, the first step is to advance while gently correcting heading to stay parallel. No immediate obstacles are present in the immediate vicinity, so safe progression is feasible.",
      "action": [
        "Turn slightly right to align parallel with the structure's longitudinal axis",
        "Go straight forward while maintaining ~1.5 m lateral distance from the structure",
        "Continue forward, monitoring surface features for damage or biofouling"
      ]
    },
    "images": [
      "./data/images/0270.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater structure lying on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large cylindrical structure is visible lying horizontally on the seabed, slightly angled with its open end facing toward the upper right of the frame. The robot appears to be positioned above and slightly behind it (relative to the cylinder’s axis). To approach for inspection, the robot should first descend toward the seabed to align with the structure’s height, then move forward along its length—ideally from the closed end toward the open end—to ensure full surface coverage. Given the lighting and nearby coral, we must avoid sudden movements; a controlled descent followed by steady forward motion is safest. No immediate obstacles block the direct path to the rear (closed) end of the cylinder.",
      "action": [
        "Descend vertically to reach seabed level",
        "Move forward slowly toward the rear (closed) end of the cylinder",
        "Align parallel to the cylinder’s long axis",
        "Translate laterally left or right to maintain ~1m standoff distance",
        "Proceed forward along the cylinder’s length while inspecting surface"
      ]
    },
    "images": [
      "./data/images/0202.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red conical object hanging beneath the cylindrical underwater vehicle and inspect it closely.",
    "output": {
      "reasoning": "The image shows a cylindrical underwater vehicle suspended above the seabed, with a red conical object hanging directly beneath it (attached via a short tether or rod). The robot is currently positioned at a moderate distance and slightly below/forward of the vehicle. To approach and inspect the red conical object closely, the robot must: (1) move upward to align vertically with the object (since it hangs below the vehicle), (2) then move forward toward the vehicle’s underside where the cone is located, and (3) finally fine-tune position to get close without colliding. The seabed structures are to the sides and not obstructing the direct path upward and forward. Given the perspective, the cone is centered in the field of view under the vehicle, so a straight-up + forward trajectory is optimal.",
      "action": [
        "Go up",
        "Move forward toward the cylindrical vehicle",
        "Approach the red conical object hanging beneath the vehicle",
        "Hover closely in front of the red conical object for inspection"
      ]
    },
    "images": [
      "./data/images/0282.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green coral structure from the right side while maintaining a safe distance from its textured ridges.",
    "output": {
      "reasoning": "The image shows a large, illuminated green coral structure with prominent textured ridges extending outward. The light source is from the upper right, highlighting the right side of the structure and casting shadows on the left. To approach from the right side while avoiding the ridges, I must first position myself to the right of the structure’s main body. Since the robot appears to be currently facing the front-left portion (based on perspective), I need to reorient rightward. Then, I will advance cautiously along the right flank, keeping lateral clearance from the nearest ridge—visible as the outermost undulating edge on the right—to maintain safety. The goal is not to touch or enter the concave interior but to parallel the right-side contour at a safe offset.",
      "action": [
        "Turn right 15 degrees",
        "Move forward slowly for 2 meters",
        "Maintain slight rightward drift to stay aligned with the right flank",
        "Pause and adjust distance to keep ≥0.5m from nearest ridge"
      ]
    },
    "images": [
      "./data/images/0128.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate forward through the central illuminated corridor formed by the green and yellow striped tubular structures.",
    "output": {
      "reasoning": "The image shows a symmetrical underwater corridor formed by green-and-yellow striped tubular structures converging toward a bright central light source. The robot is positioned at the entrance, facing directly down the corridor. The central illuminated path is clearly defined and unobstructed. To follow the instruction, the robot should maintain forward motion along this central axis, avoiding deviation toward either side where the tubular structures are closer. No turns or lateral adjustments are needed initially since the corridor is aligned with the current heading.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0287.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and blue patterned object on the seabed while avoiding the yellow-ringed sea anemone above.",
    "output": {
      "reasoning": "The red and blue patterned object lies on the seabed in the lower-right foreground. A yellow-ringed sea anemone (hazardous) is suspended above and to the left, near the center of the image. To approach the target safely, I must avoid ascending or moving left into the anemone’s vicinity. The safest path is to maintain depth near the seabed and move rightward toward the target, possibly with a slight forward advance once aligned. Since the robot appears already oriented toward the target (right side), the first step is to turn slightly right to align directly, then proceed straight along the seabed while keeping distance from the upper-left anemone.",
      "action": [
        "Turn slightly right",
        "Go straight along the seabed",
        "Approach the red and blue patterned object"
      ]
    },
    "images": [
      "./data/images/temp_307_0072_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green pipeline with yellow markings and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The image shows a large green pipeline with yellow markings extending diagonally from the lower right toward the upper left. The robot (foreground submersible) is positioned to the left and slightly below the pipeline, facing roughly toward it. To approach the pipeline for inspection, the robot should first move forward to reduce distance, then adjust laterally (rightward) to align parallel to the pipeline. Once aligned, it can proceed alongside the pipeline while maintaining close proximity for surface inspection. No immediate obstacles block the direct path, though another submersible is above—likely not interfering. The goal is to get within inspection range (~1–2 m) and travel along the pipe.",
      "action": [
        "Go straight forward",
        "Turn right slightly",
        "Approach the green pipeline until ~1 meter away",
        "Move parallel to the pipeline along its length",
        "Inspect surface for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/0179.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate between the two large vertical stone pillars toward the blue coral cluster in the mid-ground.",
    "output": {
      "reasoning": "The image shows two large vertical stone pillars framing a central passage. The blue coral cluster is visible in the mid-ground, slightly to the right of center, beyond the gap between the pillars. To navigate toward it, the robot must first align itself with the gap between the two prominent pillars (left and right). Given the current perspective, the robot appears slightly left of center—so a slight rightward correction is needed to enter the passage. Once aligned, proceed straight through the gap, then adjust slightly right to approach the blue coral cluster, which lies off-center to the right in the mid-ground.",
      "action": [
        "Turn slightly right to align with the gap between the two large stone pillars",
        "Go straight through the gap between the pillars",
        "Turn slightly right to face the blue coral cluster",
        "Advance toward the blue coral cluster"
      ]
    },
    "images": [
      "./data/images/0293.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow cylindrical object resting on the seabed near the large dark structure and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The yellow cylindrical object is visible on the seabed, positioned to the right of the large dark structure (likely a wreck or rock formation). It lies slightly ahead and to the right from the robot’s current perspective. To approach it for inspection, the robot should first move forward toward the general area, then adjust laterally to align with the object. Given its proximity to the dark structure, care must be taken to avoid collision—so a slight right turn followed by forward motion is safer than a direct straight path. Once near, a slow final approach and slight orbit/pan will enable surface inspection for markings or damage.",
      "action": [
        "Turn right slightly",
        "Go forward slowly",
        "Adjust position to center the yellow cylinder in view",
        "Move closer until within 1 meter of the object",
        "Orbit slowly around the cylinder while maintaining close distance to inspect surface"
      ]
    },
    "images": [
      "./data/images/0124.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the illuminated opening in the cave wall while avoiding the rocky outcrops on the seafloor.",
    "output": {
      "reasoning": "The illuminated opening is visible in the upper-left portion of the cave wall, serving as the target destination. Rocky outcrops are scattered across the seafloor—particularly prominent on the right and lower-right—and must be avoided. The robot is currently positioned inside the cave, facing roughly toward the opening but slightly offset to the right. To safely approach the opening: (1) First, move slightly left to align with the central axis of the opening and clear the nearest right-side rocks; (2) Then proceed straight forward toward the light, maintaining a safe vertical clearance above the seafloor rocks; (3) As the opening nears, make a minor right correction if needed to center the entry path, since the opening appears slightly right-of-center from current alignment due to perspective. No sharp turns are needed—gentle lateral adjustments suffice given the relatively wide passage.",
      "action": [
        "Move left slightly",
        "Go straight toward the illuminated opening",
        "Adjust right slightly as approaching the opening"
      ]
    },
    "images": [
      "./data/images/temp_307_0022_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the bright green sponge in the foreground while avoiding the surrounding coral structures.",
    "output": {
      "reasoning": "The bright green sponge is clearly visible in the foreground, slightly right of center. Surrounding it are dark, irregular coral structures on the left, right, and behind. To approach safely, I must navigate through a relatively open corridor—likely between the left coral mass and the central sponge. The sponge appears reachable by moving forward while maintaining slight rightward bias to avoid the left coral. No immediate obstacles block the direct path, but careful lateral control is needed to prevent contact with nearby corals.",
      "action": [
        "Go straight slowly",
        "Slightly turn right",
        "Continue forward while keeping the sponge centered in view",
        "Stop when within 0.5 meters of the sponge"
      ]
    },
    "images": [
      "./data/images/0143.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large yellow sponge on the left while avoiding the black-spotted coral structure in the foreground.",
    "output": {
      "reasoning": "From the image, the large yellow sponge is visible on the left side of the frame, partially in the foreground. A black-spotted coral structure (likely the dark, porous, spotted object near center-bottom) lies directly in front of the robot’s current position and slightly to the right—blocking a direct straight path. To approach the yellow sponge safely, I must first move laterally left to bypass the coral, then proceed forward toward the sponge. The pipe structure overhead is not an immediate obstacle but indicates orientation; the sponge is clearly leftward relative to the robot’s heading. Therefore, the plan is: (1) turn left to align with the sponge while clearing the coral, (2) advance straight once clear of the coral, (3) fine-tune position to approach the sponge closely.",
      "action": [
        "Turn left",
        "Go straight",
        "Turn left slightly",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/0206.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green laser beam straight ahead through the tunnel while maintaining a safe distance from the textured walls on both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater tunnel with textured, irregular walls on both left and right sides. A bright green laser beam projects straight ahead along the central axis of the tunnel, serving as a clear visual guidance path. The robot is currently aligned with the laser, which originates near the center of the field of view and extends into the distance. To follow the instruction, the robot must maintain forward motion while ensuring lateral deviation is minimized to avoid contact with the textured walls—suggesting small corrective adjustments may be needed if drift occurs. Since the laser is centered and the tunnel appears straight in the visible segment, the primary action is to proceed straight, with implicit micro-adjustments assumed in 'maintaining safe distance'. No immediate turn or stop is required.",
      "action": [
        "Go straight",
        "Maintain center alignment with the green laser beam",
        "Slightly adjust left/right as needed to keep equal distance from both textured walls"
      ]
    },
    "images": [
      "./data/images/0215.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater robot resting on the large moss-covered rock and inspect its status.",
    "output": {
      "reasoning": "The green underwater robot is clearly visible, resting on top of a large moss-covered rock in the center-right of the image. Sunlight streams from above, illuminating the scene and confirming good visibility. The robot is at moderate distance and slightly elevated relative to the current viewpoint (which appears to be lower and leftward). To approach it safely and for inspection, I should first move forward toward the rock, then adjust laterally if needed to align directly with the robot, and finally close in for a detailed status inspection (e.g., observing lights, sensors, or body integrity). No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the large moss-covered rock",
        "Move slightly right to align with the green robot",
        "Approach closely to inspect the green robot's status"
      ]
    },
    "images": [
      "./data/images/0264.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine-like object resting on the seabed and inspect its hull for attached marine growth or damage.",
    "output": {
      "reasoning": "The green submarine-like object is visible in the mid-to-upper portion of the image, resting on a flat seabed surface. It is oriented diagonally, with its bow pointing toward the upper-left and stern toward the lower-right. The robot appears to be positioned slightly below and in front of the object (based on perspective and lighting). To approach and inspect the hull, I need to: (1) move forward to reduce distance, (2) adjust lateral position to align parallel to the hull (likely shift left to get a side view), and (3) maintain stable altitude near the seabed to enable close inspection. No immediate obstacles block the path; coral formations are present but to the right and bottom—safe to navigate around by staying centered. Since the hull is long and horizontal, a side-parallel approach is optimal for full visual coverage.",
      "action": [
        "Go straight forward slowly",
        "Shift left slightly to align parallel to the submarine's hull",
        "Maintain steady depth and continue approaching until ~1 meter away",
        "Pan camera right to scan the hull from bow to stern"
      ]
    },
    "images": [
      "./data/images/0267.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged turquoise car and inspect its front right side for damage or attached marine growth.",
    "output": {
      "reasoning": "The turquoise car is visible to the right of the central stone column, resting on the seabed. Its front right side (i.e., the passenger-side front corner) is partially visible but not yet close enough for detailed inspection. To approach it safely and position for inspection: (1) First, move forward slightly to clear the column’s immediate vicinity; (2) Then turn right to align with the car’s front-right quadrant; (3) Finally, advance slowly toward that specific area to enable close visual inspection of damage or marine growth.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/0248.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submersible vehicle and inspect the red netting wrapped around its front section.",
    "output": {
      "reasoning": "The green submersible vehicle is clearly visible in the center-right of the image, with red netting wrapped around its front (nose) section. The robot appears to be positioned slightly below and to the left of the target, with some green netting (possibly debris or part of the environment) in the immediate foreground on the left. To approach and inspect the red netting, the robot should first move forward toward the vehicle while maintaining a safe lateral offset to avoid collision with foreground objects. Once closer, a slight rightward adjustment may be needed to align with the front section. Finally, a small forward movement will bring the robot into optimal inspection range of the red netting.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight slowly",
        "Hover and inspect the red netting"
      ]
    },
    "images": [
      "./data/images/0180.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its upper ornamental structure for signs of marine growth or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned slightly to the right of center and at a moderate distance from the robot's current location. The upper ornamental structure (capital with volutes) is illuminated by overhead light, making it suitable for inspection. To approach it safely and get a clear view of the upper section, I should first move forward toward the column, then position the robot closer and slightly below the ornamental part to inspect for marine growth or damage. No immediate obstacles block the direct path, though the environment is dim and turbid—so controlled forward motion is advised.",
      "action": [
        "Go straight toward the central stone column",
        "Move closer until near the base of the column",
        "Ascend slightly and position in front of the upper ornamental structure",
        "Hover and inspect the capital for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0005_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rightmost column to inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "From the image, three columns are visible: left, center, and rightmost. The rightmost column is clearly identifiable by its position on the far right of the frame and its distinct architectural features (e.g., arched openings, textured surface). The robot appears to be positioned slightly left-of-center, facing roughly forward with the columns ahead. To approach the rightmost column, the robot must first reorient toward the right, then move forward while maintaining alignment with that column. Given the underwater environment and potential for drift or current, small corrective adjustments may be needed. No immediate obstacles block the path to the rightmost column, though the seabed has some debris—so a moderate altitude should be maintained. The goal is to get close enough for visual inspection of surface details (marine growth, cracks, erosion), which likely requires being within ~1–2 meters.",
      "action": [
        "Turn right 30 degrees",
        "Go straight 2 meters",
        "Turn right 15 degrees",
        "Go straight 1.5 meters",
        "Hover and inspect the rightmost column surface"
      ]
    },
    "images": [
      "./data/images/0135.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient green-encrusted column and inspect its surface for inscriptions or structural damage.",
    "output": {
      "reasoning": "The ancient column is directly in front of the robot, centered in the field of view, and appears to be within close proximity (a few meters away). The seabed is relatively flat with minor undulations and no immediate obstacles between the robot and the column. To inspect its surface for inscriptions or damage, the robot must first approach it closely—ideally within 1–2 meters—while maintaining a stable orientation to allow high-resolution visual inspection. A straight forward movement is optimal; once near, a slight orbit or lateral shift may be needed for full surface coverage, but the initial step is straightforward approach.",
      "action": [
        "Go straight toward the column"
      ]
    },
    "images": [
      "./data/images/0266.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the illuminated green pipeline along the seabed while maintaining a safe distance from the rocky left wall.",
    "output": {
      "reasoning": "The image shows a brightly illuminated green pipeline running diagonally from lower-left to upper-right, parallel to the seabed. A rocky wall is visible on the left side of the frame, close to the pipeline. To follow the pipeline while maintaining a safe distance from the left wall, the robot should position itself slightly right of the pipeline’s centerline—ensuring it stays clear of the rocks but still tracks the pipeline visually. The pipeline appears to curve gently upward/rightward, so the robot must adjust heading gradually to stay aligned. Initial action: move forward while slightly correcting right to establish safe lateral offset; then continue forward with minor right adjustments as the pipeline curves.",
      "action": [
        "Go straight while slightly turning right",
        "Maintain forward motion, keeping the green pipeline centered in view and the rocky wall visible but at a safe distance on the left",
        "Gently turn right as the pipeline curves upward-right"
      ]
    },
    "images": [
      "./data/images/0216.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green ribbed cylindrical object with the red fin-like attachment and inspect its surface for biofouling or damage.",
    "output": {
      "reasoning": "The green ribbed cylindrical object with a red fin-like attachment is clearly visible on the right side of the image, resting on or near a spotted rock formation. It is relatively close to the robot’s current position (within ~1–2 m), and the path to it appears unobstructed—no immediate obstacles block direct approach from the current heading. To inspect its surface for biofouling or damage, the robot must first move closer to achieve high-resolution visual coverage. Given the object is to the right and slightly forward, the optimal initial action is to advance straight while maintaining orientation toward the object. Once within ~0.5 m, a slight turn may be needed to align parallel to the cylinder for full surface inspection, but the first step is straightforward forward motion.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0086_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green striped buoy with the orange cone on top and inspect its surface for attached marine growth.",
    "output": {
      "reasoning": "The green striped buoy with an orange cone on top is clearly visible in the center-left of the image, resting on the seabed. It is the primary target. The robot (camera platform) is positioned above and slightly behind it, with a clear line of sight. To approach and inspect its surface, I need to: (1) move forward toward the buoy while maintaining depth; (2) position closely alongside or slightly in front to enable detailed visual inspection; (3) possibly adjust orientation (e.g., slight yaw or pitch) to scan the surface for marine growth. No immediate obstacles block the direct path, though the rusted disc and green dome are nearby—so lateral clearance must be maintained. Since the buoy is stationary and prominent, a straightforward approach is feasible.",
      "action": [
        "Go straight toward the green striped buoy",
        "Move closer until within 0.5 meters of the buoy",
        "Hover and slowly circle around the buoy to inspect all sides",
        "Tilt camera downward slightly to examine the lower surface for marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0062_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and positioned directly ahead of the robot’s current viewpoint. Sunlight rays indicate relatively clear water and good visibility. The pillar stands on a square base, with no immediate obstacles in the direct forward path. To inspect its surface for inscriptions or damage, the robot must first close the distance to within visual inspection range (≈1–2 m), then possibly circle or tilt to examine multiple sides. Given the current alignment, the optimal first step is to move straight toward the pillar’s base, maintaining centering to avoid drifting.",
      "action": [
        "Go straight toward the central stone pillar",
        "Slow down as you approach the base (within 2 meters)",
        "Hover and rotate slowly clockwise to inspect the front-right surface",
        "Rotate counterclockwise to inspect the front-left surface",
        "Tilt camera upward slightly to examine upper sections"
      ]
    },
    "images": [
      "./data/images/0115.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front grille for marine growth or damage.",
    "output": {
      "reasoning": "The submerged car is clearly visible in the center-right of the image, resting on the seabed. Its front grille faces the camera, slightly angled to the left. The robot appears to be positioned at a moderate distance (~3–5 m) and slightly elevated (sunlight rays indicate surface above). To inspect the front grille closely, the robot must first approach straight toward the car while maintaining alignment with the grille. Given no immediate obstacles in the direct path, a straightforward advance is safe. Once near (~1–2 m), a slight forward lean or descent may be needed to get a close-up view of the grille’s condition (marine growth/damage). No red obstacles are present per instruction constraints.",
      "action": [
        "Go straight toward the car",
        "Slow down as you near the front grille",
        "Hover steadily 1 meter in front of the grille",
        "Tilt camera downward slightly for detailed inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0052_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green cylindrical device suspended above the twisted green cable structure and inspect its base connection.",
    "output": {
      "reasoning": "The glowing green cylindrical device is centrally located and suspended directly above the twisted green cable structure. From the current perspective, the robot appears to be slightly below and at a distance from the device, with clear visibility of both the device and its connection point to the cable below. To inspect the base connection, the robot must first move upward to align horizontally with the base of the cylinder, then close in while maintaining stable orientation to avoid disturbing the delicate cable structure. The path appears unobstructed, so a direct approach is feasible: ascend, then advance forward toward the base, possibly with slight lateral adjustment if needed upon closer inspection.",
      "action": [
        "Ascend vertically",
        "Move forward toward the glowing green cylinder",
        "Adjust position to center on the base connection",
        "Hover and inspect the base connection"
      ]
    },
    "images": [
      "./data/images/0151.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical object on the seabed and inspect its top valve assembly.",
    "output": {
      "reasoning": "The image shows a red cylindrical object upright on the seabed, located to the left-center of the frame. The robot appears to be positioned slightly above and to the right of it, with another large metallic structure dominating the right side of the view. To approach the red cylinder and inspect its top valve assembly, the robot must first move laterally leftward to align with the cylinder, then descend gently to get closer to its top (which is likely near seabed level, given its upright posture). Once in position above or beside the top, a slight upward tilt or hover may be needed to clearly view the valve assembly. No immediate obstacles block the path to the red cylinder, though care should be taken to avoid the nearby large structure on the right.",
      "action": [
        "Turn left",
        "Move forward toward the red cylindrical object",
        "Descend slightly to match its height",
        "Hover above the top of the red cylinder",
        "Tilt camera upward to inspect the top valve assembly"
      ]
    },
    "images": [
      "./data/images/0219.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge located on the seabed near the cluster of dark rocks in the foreground.",
    "output": {
      "reasoning": "The yellow sponge is not clearly visible in the current frame, but the instruction specifies it lies on the seabed near a cluster of dark rocks in the foreground. In the image, there is a prominent cluster of dark rocks at the bottom right and center foreground. The seabed is visible and relatively flat there. To approach the sponge, I must first navigate toward that rock cluster. Since the robot appears to be positioned slightly left and above the target area (based on perspective and lighting), the logical first step is to move forward and slightly right to align with the rock cluster. Once closer, I can fine-tune positioning to locate and approach the yellow sponge on the seabed.",
      "action": [
        "Move forward",
        "Turn slightly right",
        "Descend gently toward seabed"
      ]
    },
    "images": [
      "./data/images/temp_307_0007_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large segmented pipeline on the seabed and inspect the yellow-banded joint section for signs of corrosion or damage.",
    "output": {
      "reasoning": "The image shows a large segmented pipeline lying on the seabed, extending diagonally from the lower-left toward the upper-right. A prominent yellow-banded joint section is visible near the center-right of the frame — this is the target for inspection. The robot appears to be positioned slightly to the left and in front of the pipeline, with the yellow joint ahead and slightly to the right. To approach it safely and align for close inspection, the robot should first move forward along the pipeline’s axis, then adjust laterally (rightward) to center on the yellow-banded joint. Given the pipeline’s orientation and the robot’s current pose, a sequence of 'Go straight' followed by 'Turn right' and another 'Go straight' will bring the robot into optimal proximity and alignment for visual inspection.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0238.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large bioluminescent spherical organism with red-orange polyps and inspect its surface texture while maintaining a safe distance.",
    "output": {
      "reasoning": "The image shows a large bioluminescent spherical organism with red-orange polyps (likely the target) centered in the frame, resting on a purple base. The robot (submersible with red lights) is currently positioned above and slightly behind the organism, at a moderate distance. To approach while maintaining a safe distance, the robot should first descend vertically to align horizontally with the organism’s equator, then move forward slowly toward it—stopping before entering the immediate vicinity of the polyps (to avoid contact or disturbance). Given the organism’s size and prominence, a controlled forward advance after leveling is sufficient; no lateral correction is needed. Inspection of surface texture can occur once stable at ~1–2 m distance.",
      "action": [
        "Descend vertically to align with the organism's midsection",
        "Move forward slowly toward the organism",
        "Stop at a safe distance (~1.5 meters) for inspection"
      ]
    },
    "images": [
      "./data/images/0301.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated yellow net structure while avoiding the overhead cables and the fish swimming nearby.",
    "output": {
      "reasoning": "The illuminated yellow net structure is the primary target, located centrally and slightly below the current viewpoint. Overhead cables (thick black/yellow bundles) are positioned above and to the left—requiring careful vertical clearance. A fish is visible to the left of the net, swimming horizontally; it poses a dynamic obstacle that should be avoided by maintaining lateral distance. The robot appears to be approaching from a slight distance and angle; to safely reach the net, it should first adjust laterally to the right to avoid both the fish and the left-side cables, then proceed forward while maintaining safe altitude under the overhead cables. The net’s illumination helps with visual tracking during approach.",
      "action": [
        "Turn right slightly",
        "Move forward slowly while maintaining depth below overhead cables",
        "Adjust leftward gently if needed to center on net entrance",
        "Continue forward toward the illuminated yellow net structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0015_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green ring-shaped coral on the right while avoiding the overhanging rock formation above.",
    "output": {
      "reasoning": "The glowing green ring-shaped coral is clearly visible on the right side of the image, partially obscured by a large overhanging rock formation above it. To approach it safely, I must avoid collision with the overhead rock. The coral appears at mid-depth and slightly forward-right from the current viewpoint. A safe path would involve: (1) moving slightly right to align toward the coral while staying below the overhang; (2) advancing forward cautiously, maintaining clearance under the rock; (3) fine-tuning position to center on the coral. Since the rock overhang is directly above the coral’s location, I should not ascend—instead, stay at or slightly below current depth and move laterally first.",
      "action": [
        "Turn right slightly",
        "Go straight slowly while maintaining current depth",
        "Adjust rightward gently to center on the green ring coral"
      ]
    },
    "images": [
      "./data/images/0250.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure while maintaining a safe distance from the suspended cable above it.",
    "output": {
      "reasoning": "The image shows a large green cylindrical structure lying horizontally on the seabed, with a suspended cable running diagonally above it (from upper left to lower right). The robot appears to be approaching from the left side, slightly below the structure. To approach safely while avoiding the cable: (1) First, align the robot parallel to the cylinder’s long axis to maintain predictable clearance; (2) Since the cable is above and slightly ahead, we should move forward cautiously while keeping depth stable—avoid ascending toward the cable; (3) The current view suggests the robot is already at a safe vertical distance below the cable, so the priority is lateral alignment and controlled forward motion. We’ll proceed straight while monitoring relative position to the cable and structure.",
      "action": [
        "Turn slightly right to align parallel with the green cylinder",
        "Go straight slowly while maintaining current depth",
        "Monitor cable position and adjust depth downward if needed"
      ]
    },
    "images": [
      "./data/images/0277.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column while avoiding the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, illuminated by light rays from above and positioned slightly to the right of center. Scattered debris (small rocks and fragments) lies on the seabed, especially in the foreground and left side. To approach the column safely, I should first move forward while staying centered to avoid the left-side debris, then make a slight right correction as I get closer to align with the column’s base. The path appears relatively clear directly ahead toward the column, but minor lateral adjustments are needed to steer around nearby debris.",
      "action": [
        "Go straight",
        "Slightly turn right",
        "Go straight toward the column base"
      ]
    },
    "images": [
      "./data/images/temp_307_0043_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater canyon, keeping centered between the two rocky walls while maintaining a safe distance from the coral-covered surfaces.",
    "output": {
      "reasoning": "The robot is currently positioned in a narrow underwater canyon with rocky walls on both left and right sides. The coral-covered surfaces are visible along the walls, especially on the lower portions and near the edges—these must be avoided to prevent damage. The robot’s current heading is aligned with the canyon’s central axis, as indicated by the light rays converging ahead and the symmetric wall geometry. To maintain centerline navigation while avoiding coral, the robot should proceed forward with minor lateral corrections: slight left adjustments if drifting right toward the right wall (which appears closer and more textured with coral), and vice versa. Given the current view, the right wall is slightly nearer and more heavily covered in coral; thus, a small leftward correction is prudent before continuing straight. Subsequent steps will involve monitoring wall proximity and adjusting incrementally.",
      "action": [
        "Slightly turn left",
        "Go straight",
        "Monitor left and right distances; adjust minimally to stay centered",
        "Continue forward while maintaining safe clearance from coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0047_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green car positioned between the two large stone columns and inspect its front bumper for damage.",
    "output": {
      "reasoning": "The green car is clearly visible in the center of the image, positioned between two large stone columns. The robot appears to be at a moderate distance behind and slightly above the car, with a clear forward path toward it. To approach the car and inspect its front bumper, I need to: (1) move straight forward toward the car, (2) possibly adjust laterally to center on the car if needed, and (3) get close enough to inspect the front bumper—likely requiring a slight downward or forward-tilt maneuver once near. Given the open space and lack of immediate obstacles, a direct forward approach is safe and efficient.",
      "action": [
        "Go straight toward the green car",
        "Adjust slightly left or right to center on the car if needed",
        "Move closer until the front bumper is in clear view",
        "Hover and inspect the front bumper for damage"
      ]
    },
    "images": [
      "./data/images/0146.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical perforated pipe structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The vertical perforated pipe structure is directly in front of the robot and dominates the central field of view, indicating it is relatively close. The lighting highlights its surface texture and perforations, suggesting good visibility for inspection. To inspect the surface thoroughly, the robot should first approach closer to ensure high-resolution imaging, then perform a slow circumferential movement (e.g., orbit or rotate around it) to cover all sides. Since no immediate obstacles block the direct path, the initial action is to move forward toward the base of the pipe. After reaching an optimal inspection distance (~1–2 m), a controlled right turn (or left, depending on desired inspection direction) can begin the orbital sweep. Given the instruction emphasizes 'approach' first, we prioritize forward motion before rotation.",
      "action": [
        "Go straight toward the pipe structure",
        "Slow down as you near the base",
        "Begin a gentle right turn to orbit the pipe"
      ]
    },
    "images": [
      "./data/images/temp_307_0087_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged turquoise car and inspect the blue conical object mounted on its roof.",
    "output": {
      "reasoning": "The turquoise car is clearly visible in the center of the image, resting on the seabed. A blue conical object (likely a sensor or beacon) is mounted on its roof, emitting bubbles upward. The robot appears to be positioned slightly in front and to the left of the car, with clear visibility and no immediate obstacles blocking the path. To approach and inspect the blue conical object, the robot should first move forward toward the car, then adjust laterally (slightly right) to align directly beneath or beside the roof-mounted object for close inspection. Given the car’s orientation (front facing camera), approaching from the front-left to front-center is safe and optimal.",
      "action": [
        "Go straight toward the turquoise car",
        "Slightly turn right to align with the car's centerline",
        "Move forward slowly to get closer to the car's roof",
        "Hover and inspect the blue conical object on the roof"
      ]
    },
    "images": [
      "./data/images/0104.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central concrete pillar structure and inspect its surface for marine growth or damage, focusing on the circular openings at mid-height.",
    "output": {
      "reasoning": "The image shows a central concrete pillar structure with visible circular openings at mid-height, partially covered in marine growth. The robot is currently positioned at a moderate distance, slightly to the left and front of the pillar. To approach and inspect the circular openings, I need to: (1) move forward toward the pillar while maintaining alignment with its center; (2) adjust laterally if needed to center the target openings in the field of view; (3) close in to a suitable inspection distance (~1–2 m) without colliding; (4) possibly rotate slightly to ensure full visibility of all openings around the circumference. No immediate obstacles block the path, but the seabed is uneven—so maintain stable altitude. Given the current perspective, the most direct path is to go straight ahead first, then fine-tune position.",
      "action": [
        "Go straight toward the central concrete pillar",
        "Adjust left/right slightly to center the pillar in view",
        "Continue forward until ~1.5 meters from the pillar",
        "Hover and slowly rotate clockwise to inspect all circular openings at mid-height"
      ]
    },
    "images": [
      "./data/images/0105.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall textured pillar on the right while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The image shows an underwater scene with a prominent tall textured pillar on the right side of the frame. Surrounding it are irregular rock formations—some to the left, some in front, and one large rounded rock near the bottom right. The robot appears to be positioned slightly left of center, facing forward. To approach the pillar safely, I must avoid colliding with nearby rocks: the large foreground rock on the bottom right, the mid-ground rocks near the pillar’s base, and the left-side rock wall. A safe path would involve first moving slightly right (but not too close to the bottom-right rock), then advancing straight toward the pillar while keeping lateral clearance. Given the pillar is clearly visible and relatively distant, a two-step plan suffices: (1) gently reorient rightward to align with the pillar’s axis while staying clear of obstacles, (2) proceed forward at a steady pace, monitoring proximity to surrounding rocks.",
      "action": [
        "Turn slightly right",
        "Go straight toward the tall textured pillar"
      ]
    },
    "images": [
      "./data/images/temp_307_0026_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater instrument and inspect its top sensor module while avoiding the nearby rocky outcrop on the right.",
    "output": {
      "reasoning": "The red cylindrical instrument is the primary target, positioned slightly left of center and angled upward. Its top sensor module (white cap with black components) is visible at the upper end. A rocky outcrop occupies the right side of the frame, posing a collision risk if the robot moves directly rightward or drifts. To safely approach: (1) First, move forward cautiously while maintaining left bias to avoid the right-side rocks; (2) Then, adjust heading slightly right to align with the instrument’s axis without encroaching on the rock; (3) Finally, position directly above or slightly in front of the top sensor module for inspection—ensuring sufficient clearance from both the rock and the instrument’s body. The current orientation shows the instrument tilted, so a slight upward pitch may also help stabilize viewing angle during final approach.",
      "action": [
        "Go straight while keeping left of center to avoid the right rocky outcrop",
        "Gently turn right to align with the red instrument's longitudinal axis",
        "Move forward slowly to approach the top sensor module",
        "Hover steadily above the top sensor module for inspection"
      ]
    },
    "images": [
      "./data/images/0254.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated spherical coral in the center while avoiding the large rock formations on the left and right.",
    "output": {
      "reasoning": "The illuminated spherical coral is clearly visible in the center of the image, directly under a light source (likely the robot's own lamp or an external beacon), making it the primary target. Large rock formations flank both the left and right sides of the scene, forming a narrow corridor toward the coral. To avoid collision, the robot must maintain a central trajectory. The coral appears at moderate distance ahead, so initial forward motion is safe if centered. No immediate obstacles block the direct path, but precise lateral positioning is critical. Therefore, the plan is: (1) slightly adjust to center alignment if needed, (2) proceed straight while monitoring proximity to left/right rocks, (3) decelerate as approaching the coral to ensure safe proximity.",
      "action": [
        "Center yourself between the left and right rock formations",
        "Go straight toward the illuminated spherical coral",
        "Slow down as you near the coral"
      ]
    },
    "images": [
      "./data/images/0184.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted green station wagon and inspect its front grille for attached marine growth or debris.",
    "output": {
      "reasoning": "The rusted green station wagon is clearly visible in the center-left of the image, oriented diagonally with its front grille facing toward the lower-right. The robot appears to be positioned slightly ahead and to the right of the vehicle (based on perspective). To approach the front grille for inspection, the robot should first move forward to close distance, then adjust leftward to align directly with the grille. Since the grille is the target inspection point, final positioning should be close (~1–2 m) and head-on. No immediate obstacles block the path; the large circular structure above and the engine-like object to the right are at safe distances. Lighting from above illuminates the scene adequately.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight slowly",
        "Stop in front of the front grille"
      ]
    },
    "images": [
      "./data/images/0261.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow branching coral on the left while maintaining a safe distance from the school of fish on the right.",
    "output": {
      "reasoning": "The image shows the robot (submarine) currently oriented horizontally, with a prominent yellow branching coral on the left side of the frame and a school of fish on the right. The coral is relatively close and in clear view, while the fish are mid-to-far right, indicating potential collision risk if moving directly rightward or drifting. To approach the coral safely: (1) First, ensure lateral clearance from the fish—since they’re on the right, a slight leftward adjustment is safe and aligns with the target direction. (2) Then proceed forward toward the coral, maintaining a buffer distance to avoid disturbing it or risking entanglement. (3) Given the coral’s position slightly ahead and left, a gentle left turn followed by straight advance is optimal. No sharp turns needed—the coral is already in field of view.",
      "action": [
        "Turn slightly left",
        "Go straight toward the yellow branching coral",
        "Maintain steady distance from the school of fish on the right"
      ]
    },
    "images": [
      "./data/images/0177.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green and purple striped cylindrical object on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large green and purple striped cylindrical object lies on the seabed in the lower-left to center portion of the image. The robot is currently positioned above and slightly to the right of the object, with clear visibility and no immediate obstacles blocking the path. To approach and inspect its surface, I need to: (1) descend toward the seabed to reach the object’s depth; (2) move forward (toward the object) while maintaining a safe distance initially; (3) circle or reposition laterally to enable close-up visual inspection of its surface for damage or marine growth (e.g., algae, barnacles). Given the object’s orientation (horizontal, lying on side), a slight leftward movement may help align for optimal viewing angle before closing in.",
      "action": [
        "Descend slowly toward the seabed",
        "Move forward toward the green and purple striped cylinder",
        "Shift left slightly to align with the cylinder’s midsection",
        "Approach closer for detailed surface inspection",
        "Pan camera gently around the cylinder to inspect full circumference"
      ]
    },
    "images": [
      "./data/images/temp_307_0027_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ribbed cylindrical object lying on the seabed and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The large green ribbed cylindrical object is clearly visible on the seabed, positioned slightly to the right and ahead of the robot’s current viewpoint. Sunlight rays indicate the robot is relatively shallow and oriented roughly toward the object. To approach it for inspection, the robot should first move forward while maintaining alignment with the object. As it gets closer, a slight rightward correction may be needed to center the object in the field of view. Once within ~1–2 meters, it should slow and stabilize to enable detailed surface inspection (e.g., for organisms or damage). No immediate obstacles block the direct path, though surrounding coral structures require cautious navigation to avoid contact.",
      "action": [
        "Go straight",
        "Slightly turn right",
        "Go straight slowly",
        "Hover and stabilize near the object"
      ]
    },
    "images": [
      "./data/images/0137.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-lit submersible vehicle while avoiding the foreground pipeline and yellow coral formations.",
    "output": {
      "reasoning": "The large green-lit submersible is visible in the mid-to-upper right of the image, illuminated by a strong green beam pointing downward. A prominent pipeline runs diagonally across the foreground from lower-left to mid-right, and yellow coral formations are clustered on the right side near the bottom. To approach the submersible safely: (1) First, move left to clear the pipeline’s immediate path; (2) Then proceed forward while staying slightly left of center to avoid both the pipeline (now on the right) and the yellow coral (on the far right); (3) Finally, adjust rightward gently as the submersible comes closer to align with it, ensuring clearance from coral. The submersible appears stationary and elevated, so upward pitch may be needed later, but initial steps focus on lateral avoidance and forward progress.",
      "action": [
        "Turn left",
        "Go straight",
        "Turn slightly right",
        "Go straight toward the green-lit submersible"
      ]
    },
    "images": [
      "./data/images/0279.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and directly ahead in the current frame. The robot is positioned at a moderate distance, with clear water and no immediate obstacles between it and the pillar. To inspect the surface for marine growth or structural damage, the robot must first approach closely—ideally within 1–2 meters—and then perform a slow circumferential scan to observe all sides. Given the current orientation (facing the pillar head-on), the optimal first step is to move straight forward toward the base of the pillar. Once near, a controlled orbit (e.g., slight right turn while maintaining distance) will allow full visual inspection.",
      "action": [
        "Go straight toward the central stone pillar",
        "Slow down as you near the base (within 1.5 meters)",
        "Orbit clockwise around the pillar at constant distance",
        "Inspect surface details during orbit, focusing on discoloration, barnacles, cracks, or erosion"
      ]
    },
    "images": [
      "./data/images/0235.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange buoy with the green handle and inspect its base for attached marine growth.",
    "output": {
      "reasoning": "The orange buoy with a green handle is clearly visible in the foreground, centered and slightly to the right of the robot’s current position. The base of the buoy is partially submerged and rests on the textured green seabed. To inspect its base for marine growth, I need to: (1) move forward toward the buoy while maintaining alignment, (2) get close enough to resolve fine details at the waterline/base interface, and (3) possibly adjust vertically or laterally to center the base in view. Given the proximity (~1–2 m), a short forward motion followed by a slight descent and fine positioning should suffice. No obstacles block the direct path.",
      "action": [
        "Go straight toward the orange buoy",
        "Descend slightly to align camera with buoy base",
        "Hover and stabilize near the buoy base for inspection"
      ]
    },
    "images": [
      "./data/images/0113.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue cylindrical pipe lying on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large blue cylindrical pipe is clearly visible lying horizontally on the seabed, occupying the central-left portion of the image. The robot appears to be positioned slightly above and behind it (given the perspective and light rays from surface). To approach and inspect its surface, I need to: (1) move forward toward the pipe while maintaining a safe distance; (2) align laterally to bring the pipe into optimal inspection range; (3) possibly circle or translate alongside it for full surface coverage. Since the pipe is long and oriented left-to-right in the frame, approaching from the current angle likely requires moving forward and slightly left to reach its midsection first. No immediate obstacles block the path, but caution is needed near the seabed structure on the left. Inspection will require close proximity (~0.5–1 m) and stable positioning.",
      "action": [
        "Go straight slowly",
        "Move slightly left to align with the pipe center",
        "Approach closer until ~1 meter from pipe surface",
        "Translate right along the pipe while maintaining distance",
        "Pause and inspect surface for anomalies"
      ]
    },
    "images": [
      "./data/images/0140.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle directly ahead while maintaining a safe distance from the surrounding rocky seabed.",
    "output": {
      "reasoning": "The submarine-like vehicle is centered in the image, directly ahead of the robot’s current position. The seabed is rocky on both left and right sides, with visible boulders close to the vehicle’s path. To approach safely, the robot must maintain a central trajectory to avoid lateral obstacles. Given the clear forward visibility and the vehicle’s stable orientation, a controlled forward movement with minor course corrections (if needed) is appropriate. No immediate turns are required since the target is straight ahead and unobstructed in the mid-field. The safe distance can be preserved by advancing at moderate speed while monitoring proximity to seabed features.",
      "action": [
        "Go straight",
        "Maintain central heading",
        "Proceed cautiously toward the submarine"
      ]
    },
    "images": [
      "./data/images/0158.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red underwater vehicle and inspect its front-mounted sensor array.",
    "output": {
      "reasoning": "The red underwater vehicle is directly in front of the robot, positioned slightly to the left of center in the image, and appears at a moderate distance (~2–3 meters). Its front-mounted sensor array (a circular white housing with a central lens) is clearly visible. To inspect it closely, I need to approach straight ahead while maintaining alignment with the vehicle’s longitudinal axis. No immediate obstacles block the path; the seabed is clear ahead, and coral is to the right. A gentle forward motion will bring the robot within inspection range (~0.5–1 m), after which a slight pitch-up or hover may be needed for optimal sensor view—but the instruction only requires approaching and inspecting, so initial approach is priority.",
      "action": [
        "Go straight toward the red vehicle",
        "Slow down as you near the vehicle",
        "Hover facing the front-mounted sensor array"
      ]
    },
    "images": [
      "./data/images/0094.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular coral structure in the foreground while avoiding the vertical column on the left.",
    "output": {
      "reasoning": "The image shows a large circular coral structure in the foreground (right-lower quadrant), and a prominent vertical column on the left side of the frame. To approach the coral while avoiding the column, the robot should first move rightward to clear the column’s proximity, then proceed forward toward the coral. Since the coral is already relatively close and centered in the lower-right, a slight right turn followed by straight movement should safely align the robot with the target without colliding with the left column.",
      "action": [
        "Turn right slightly",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0126.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red-and-blue cylindrical underwater structure and inspect its midsection for attached sensors or connectors.",
    "output": {
      "reasoning": "The red-and-blue cylindrical structure is clearly visible in the center of the image, upright and anchored among coral formations. The robot appears to be positioned slightly to the left and at a moderate distance (~2–3 m) from the structure, with a clear forward path. To inspect the midsection (approximately the blue band region), the robot must first close the distance while maintaining stable alignment. Given the structure’s vertical orientation and the presence of coral on both sides, a straight approach is feasible but requires slight rightward correction to center on the cylinder. Once near (~1 m), a slow hover or slight upward/downward adjustment may be needed to align the camera with the midsection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the red-and-blue cylinder",
        "Slightly turn right to center on the cylinder",
        "Continue forward until ~1 meter away",
        "Hover and adjust altitude to align camera with the blue midsection",
        "Inspect the midsection for sensors or connectors"
      ]
    },
    "images": [
      "./data/images/0182.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue pickup truck and inspect its front right wheel for entanglement or damage.",
    "output": {
      "reasoning": "The blue pickup truck is visible in the center-right of the image, resting on the seabed between two large columns. Its front right wheel is partially visible but not yet close enough for detailed inspection. To inspect the front right wheel, I need to approach the vehicle from a position that allows clear access—ideally from the front-right quadrant. Given the current perspective (slightly elevated and left-of-center), the optimal path is: (1) move forward toward the truck while slightly adjusting right to align with the front-right side; (2) once near, fine-tune position to bring the front right wheel into close view; (3) stabilize for inspection. No immediate obstacles block this path, though care must be taken around the column on the left and seabed debris.",
      "action": [
        "Move forward and slightly right toward the blue pickup truck",
        "Approach the front-right side of the truck",
        "Position closely in front of the front right wheel",
        "Stabilize and inspect the front right wheel for entanglement or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0053_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green ridged coral in the foreground while maintaining a safe distance from the yellow sponge on the left and the underwater structure in the background.",
    "output": {
      "reasoning": "The green ridged coral is in the foreground, centered-right in the image. The yellow sponge is on the left side, close to the robot’s current position. An underwater structure (likely a platform or observatory) is visible in the background, slightly left of center. To approach the green coral safely: (1) avoid drifting left toward the yellow sponge; (2) maintain forward progress without getting too close to the background structure; (3) steer slightly right to align with the coral while keeping lateral clearance from the sponge. Given the coral is already relatively close and well-lit, a gentle rightward correction followed by straight advance should suffice.",
      "action": [
        "Turn slightly right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/0163.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange cylindrical pipe lying on the seabed and inspect its open end for blockages or damage.",
    "output": {
      "reasoning": "The large orange cylindrical pipe is clearly visible on the seabed, lying horizontally with its open end facing roughly toward the camera's current position. The robot appears to be at a moderate distance (~2–3 m) and slightly elevated above the seabed. To inspect the open end for blockages or damage, the robot must: (1) descend gently to seabed level to avoid parallax and ensure stable inspection, (2) move forward straight toward the pipe’s open end, (3) stop at close range (~0.5 m) to allow high-resolution visual inspection, and (4) optionally adjust yaw slightly to center the open end in the field of view if needed. No immediate obstacles are present between the robot and the pipe’s open end.",
      "action": [
        "Descend to seabed level",
        "Go straight toward the orange pipe's open end",
        "Stop approximately 0.5 meters from the open end",
        "Adjust yaw slightly to center the pipe's opening in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0010_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green mesh pipeline while maintaining a safe distance from the red-and-black flexible hose on the right.",
    "output": {
      "reasoning": "The image shows an underwater scene with a green mesh pipeline running diagonally from bottom-left toward upper-right, and a red-and-black flexible hose on the right side, curving upward. The robot (submersible) is currently positioned near the center, slightly above the pipeline, with bubbles indicating forward motion. To follow the green mesh pipeline while avoiding the red-and-black hose on the right, the robot must: (1) align itself parallel to the pipeline’s direction; (2) maintain lateral offset to the left of the pipeline (since the hose is on the right, staying left ensures safe distance); (3) proceed forward steadily, adjusting heading as the pipeline curves gently. The pipeline appears to extend forward and slightly upward; thus, a slight rightward heading correction may be needed initially to stay aligned, but overall the trajectory should track the pipeline’s left edge.",
      "action": [
        "Turn slightly right to align with the green mesh pipeline",
        "Go straight while keeping the green mesh pipeline on the left side",
        "Maintain distance from the red-and-black hose on the right",
        "Adjust heading gently right if pipeline curves right"
      ]
    },
    "images": [
      "./data/images/temp_307_0076_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central black pipe while avoiding the surrounding green anemones and maintaining clearance from the right-hand ribbed structure.",
    "output": {
      "reasoning": "The image shows a central black pipe extending vertically upward, flanked by bright green anemones on the left and bottom, and a ribbed structure on the right. The robot is positioned below and slightly in front of the pipe. To approach the central pipe safely: (1) First, maintain distance from the right-hand ribbed structure—so avoid moving right; (2) Green anemones are clustered to the left and near the base—so steer slightly right of center but not too close to the ribbed structure; (3) The pipe is directly ahead and upward, so proceed forward while gently correcting left-to-center to stay aligned with the pipe’s axis without encroaching on anemones. Given the perspective, a slight leftward bias initially may be needed to clear the left-side anemones, then centering as we ascend.",
      "action": [
        "Gently turn left to clear left-side green anemones",
        "Move forward toward the central black pipe",
        "Slightly adjust right to maintain clearance from the right-hand ribbed structure",
        "Continue straight while keeping the pipe centered in view"
      ]
    },
    "images": [
      "./data/images/0280.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted pickup truck on the seabed and inspect its front grille for marine growth or damage.",
    "output": {
      "reasoning": "The rusted pickup truck is clearly visible on the seabed, positioned in the lower-left to center of the frame. Its front grille faces roughly toward the camera but slightly angled to the right. To inspect the front grille closely, I need to approach it directly while maintaining a stable, forward-facing orientation. The large cylindrical object above is suspended and not obstructing the direct path to the truck’s front. There are no immediate obstacles between the robot’s current position (assumed near camera viewpoint) and the truck’s front grille. Therefore, the plan is: (1) move forward toward the truck, (2) adjust slight rightward heading if needed to center on the grille, (3) stop at close inspection distance (~1–2 m) and stabilize for visual assessment.",
      "action": [
        "Go straight toward the rusted pickup truck",
        "Slightly turn right to align with the front grille",
        "Stop at ~1.5 meters from the grille and hold position"
      ]
    },
    "images": [
      "./data/images/0290.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the stone tower structure from the front while maintaining a safe distance from the red fish swimming to its right.",
    "output": {
      "reasoning": "The stone tower is centered in the image, directly ahead of the robot's current position. A red fish is visible to the right of the tower (from the robot’s perspective), swimming near its base. To approach the tower from the front while avoiding the red fish, the robot should first maintain a central heading toward the tower, then slightly adjust leftward if needed to ensure safe lateral clearance from the fish. Since the fish is to the right, a small left bias during forward motion will increase safety margin without deviating from the frontal approach. The robot should proceed slowly and monitor relative positions.",
      "action": [
        "Go straight toward the stone tower",
        "Slightly turn left to maintain distance from the red fish on the right",
        "Continue straight at a steady pace, keeping the tower centered in view"
      ]
    },
    "images": [
      "./data/images/0196.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large rusted pipe structure on the right while avoiding the rocky outcrop in the foreground.",
    "output": {
      "reasoning": "The image shows a large rusted pipe structure on the right side of the frame, partially illuminated by light rays from above. In the foreground, there is a prominent rocky outcrop occupying the lower-left to center area, which must be avoided. The robot appears to be positioned slightly left of center, facing forward. To approach the pipe structure while avoiding the rocks, the robot should first move rightward to clear the rocky outcrop, then proceed forward toward the pipe. A slight right turn followed by straight motion aligns with the spatial layout: the pipe is to the right and somewhat ahead, and the rocks are directly in front-left. No immediate obstacles block the rightward path.",
      "action": [
        "Turn right slightly",
        "Go straight",
        "Continue straight toward the pipe structure"
      ]
    },
    "images": [
      "./data/images/0228.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of yellow-tipped coral formations on the left while maintaining a safe distance from the large dark overhang above.",
    "output": {
      "reasoning": "The image shows a vibrant underwater scene with a cluster of yellow-tipped coral formations on the left side of the frame. Above them, there is a large dark overhang (likely rock or reef structure) casting shadows and occupying the upper portion of the image. To approach the coral safely, I must: (1) orient toward the left where the target corals are located, (2) maintain horizontal distance from the overhang—i.e., avoid moving upward or too close vertically beneath it—and (3) proceed gradually to ensure safe clearance. The lighting rays suggest the overhang is overhead but not directly above the corals; the corals lie in the mid-left foreground. Therefore, the safest path is to first turn left to align with the coral cluster, then advance straight while keeping depth constant (to stay clear of the overhang), and optionally make minor right corrections if drifting too close to the overhang.",
      "action": [
        "Turn left",
        "Go straight slowly",
        "Maintain current depth",
        "Slight right adjustment if nearing overhang"
      ]
    },
    "images": [
      "./data/images/0298.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical vehicle and inspect its front hatch and sensor array for damage or debris.",
    "output": {
      "reasoning": "The submerged cylindrical vehicle is visible in the center-right of the image, lying on the seabed at a slight angle. Its front hatch (rectangular, dark panel with grid-like texture) and sensor array (circular ports and protruding sensors near the nose) are facing approximately toward the camera’s current position but slightly to the left. The robot appears to be positioned somewhat behind and to the left of the vehicle’s midsection, based on perspective and lighting. To inspect the front hatch and sensor array, the robot must first approach the front end—requiring forward movement while adjusting heading to align with the vehicle’s longitudinal axis. Given the vehicle’s orientation (nose pointing ~30° left of camera’s forward direction), the robot should first turn slightly left to face the front, then advance steadily while maintaining alignment. Once within ~2 meters, it can perform a slow close-in inspection pass (e.g., slight upward/downward or lateral micro-adjustments) to visually scan for damage or debris on the hatch and sensors. No immediate obstacles block the path, though seabed rocks are present to the right—so maintain central clearance.",
      "action": [
        "Turn left 15 degrees",
        "Go straight 3 meters",
        "Turn left 5 degrees",
        "Go straight 2 meters",
        "Hover and slowly ascend 0.5 meters",
        "Drift forward 0.8 meters while keeping front hatch centered in view",
        "Perform slow 10-degree yaw right to scan sensor array"
      ]
    },
    "images": [
      "./data/images/0157.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted orange submersible module and inspect its green-rimmed circular opening for signs of damage or obstruction.",
    "output": {
      "reasoning": "The rusted orange submersible module is directly in front of the robot, centered in the frame, with its green-rimmed circular opening clearly visible and facing the camera. The distance appears moderate—close enough to approach without large maneuvers, but not yet at inspection range. To inspect the opening for damage or obstruction, I need to move forward carefully to get a closer view while maintaining stable orientation. No obstacles block the direct path. Therefore, the first step is to advance straight toward the module until near the opening; then a slight adjustment may be needed to center the opening in the field of view for detailed inspection.",
      "action": [
        "Go straight toward the submersible module",
        "Slow down as you near the green-rimmed opening",
        "Position yourself directly in front of the opening for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0061_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red-and-white cylindrical device suspended by a cable and inspect its lower section near the coral formations.",
    "output": {
      "reasoning": "The red-and-white cylindrical device is suspended centrally in the image, hanging from a cable extending upward toward the surface. Coral formations are visible in the foreground and lower-left, indicating the target inspection zone is near the bottom of the device, close to the corals. The robot appears to be positioned slightly below and to the left of the device (based on perspective and lighting). To approach the lower section safely without disturbing the coral or entangling the cable, I should: (1) move forward slightly to close distance while maintaining clearance from corals; (2) adjust rightward to align with the device’s vertical axis; (3) descend gently to reach the lower portion near the coral base; (4) stabilize and orient for inspection. Avoid upward motion to prevent cable contact.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Descend slowly",
        "Hold position near lower section of device"
      ]
    },
    "images": [
      "./data/images/0245.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted submarine hull on the right and inspect the circular hatch for signs of damage or marine growth.",
    "output": {
      "reasoning": "The image shows a rusted submarine hull on the right side of the frame, with a visible circular hatch near its conning tower. The robot is currently positioned slightly left and forward of the hull, with a large pipe structure in the mid-left foreground. To approach the hull and inspect the circular hatch, the robot must first move rightward to align with the hull, then advance straight toward the hatch while maintaining a safe distance to avoid collision with the pipe or hull protrusions. Once near, it should position itself directly in front of the hatch for clear visual inspection.",
      "action": [
        "Turn right",
        "Go straight slowly",
        "Move closer to the circular hatch",
        "Hover in front of the hatch for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0038_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central green-striped cylindrical structure while avoiding the two large textured coral formations on either side.",
    "output": {
      "reasoning": "The image shows a central green-striped cylindrical structure directly ahead, flanked symmetrically by two large textured coral formations (left and right). The robot appears to be positioned on a flat seabed path leading toward the structure. To approach the target while avoiding the corals, the robot should maintain a straight trajectory along the central corridor—no lateral deviation is needed since the path is already clear and centered. The structure is relatively close and well-lit by sun rays, indicating good visibility. Thus, the safest and most direct action is to proceed straight forward.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/0190.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green sedan and inspect its front bumper for damage or marine growth.",
    "output": {
      "reasoning": "The green sedan is directly in front of the robot, positioned at a moderate distance (~3–4 meters), with its front bumper clearly visible and facing the camera. The vehicle appears stationary on the seabed, partially covered in marine growth (e.g., algae or barnacles), especially on the hood and bumper. No immediate obstacles block the direct path. To inspect the front bumper closely, I need to: (1) advance straight toward the car to reduce distance, (2) fine-tune position to center the bumper in view, and (3) hold steady for visual inspection. Given the lighting and clarity, a slow approach is safest to avoid disturbing sediment.",
      "action": [
        "Go straight toward the green sedan",
        "Slow down as you near the front bumper",
        "Position yourself 1 meter in front of the bumper, centered",
        "Hold position and inspect the front bumper for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/0171.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue underwater vehicle hovering near the rocky reef and inspect its orientation relative to the sunlit water column.",
    "output": {
      "reasoning": "The blue underwater vehicle is visible in the center-left of the image, hovering near a rocky reef. Sunlight penetrates from above, forming a distinct sunlit water column on the right side of the frame. The vehicle is angled slightly upward and toward the light, with its nose pointing roughly toward the upper-right (sunlit zone). To inspect its orientation relative to the sunlit column, I need to: (1) move closer to get a clear view without disturbing it, (2) position myself laterally to observe both the vehicle and the light column in the same field of view, and (3) stabilize for observation. Given current distance (~2–3 m), a straight approach followed by a slight rightward shift will align the vehicle and light column for comparative orientation assessment.",
      "action": [
        "Go straight toward the blue underwater vehicle",
        "Slow down as you near the vehicle",
        "Shift slightly right to align the vehicle and sunlit water column in view",
        "Hold position and observe orientation"
      ]
    },
    "images": [
      "./data/images/0169.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the right while maintaining a safe distance from the seabed and avoiding the scattered spherical corals in front.",
    "output": {
      "reasoning": "The robot is currently facing forward, with scattered spherical corals directly in front (near the seabed) and a large textured coral formation visible on the right side of the frame. To approach the large coral on the right while avoiding the spherical corals ahead and maintaining safe altitude above the seabed, I must: (1) first steer right to align toward the target coral, (2) simultaneously ascend slightly to increase clearance from the seabed and spherical corals, and (3) then proceed forward cautiously once clear of obstacles. The initial turn right should be moderate to avoid overshooting; after repositioning, a straight advance with continued altitude monitoring will bring the robot closer to the target formation.",
      "action": [
        "Turn right gently",
        "Ascend slightly",
        "Go straight slowly while monitoring distance to spherical corals and seabed"
      ]
    },
    "images": [
      "./data/images/temp_307_0067_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater structure with the glowing blue ring and inspect its central opening.",
    "output": {
      "reasoning": "The green cylindrical underwater structure with a glowing blue ring is directly in front of the robot, centered in the field of view. Its central opening is visible and appears accessible. To inspect it closely, the robot should first move forward to reduce distance, then fine-tune positioning to center the opening in the camera frame for detailed inspection. No immediate obstacles block the direct path. The current orientation is already aligned with the target, so no turning is needed before advancing.",
      "action": [
        "Go straight",
        "Go straight",
        "Hover and center the central opening in view"
      ]
    },
    "images": [
      "./data/images/0244.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large perforated brown sponge in the foreground while avoiding the rocky outcrop to the right.",
    "output": {
      "reasoning": "The large perforated brown sponge is clearly visible in the foreground, centered-left in the image. A rocky outcrop occupies the right side of the frame, partially shadowed and extending vertically — this must be avoided. The robot appears to be positioned slightly behind and above the sponge, with open water/sandy bottom between it and the target. To approach safely: (1) First, steer left to clear the right-side rocky outcrop; (2) Then proceed straight toward the sponge, maintaining a safe lateral offset from the rock; (3) Finally, close in gently on the sponge without veering right into the obstacle. No immediate upward/downward motion is needed—the sponge lies on the seabed at similar depth.",
      "action": [
        "Turn left slightly",
        "Go straight toward the sponge",
        "Approach the sponge carefully"
      ]
    },
    "images": [
      "./data/images/temp_307_0041_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the left fluted column and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows two fluted columns: one prominently on the left (closer, larger in frame) and another on the right (farther, partially visible). The user instruction specifies approaching the *left* fluted column. From the robot’s current perspective, the left column is already in front-left view, but not yet close enough for surface inspection. To approach it safely and align for inspection, the robot should first move forward slightly while maintaining orientation toward the left column, then possibly adjust laterally if needed. Given the clear path and no immediate obstacles in front-left, a direct forward motion toward the left column is appropriate, followed by a slight leftward correction to center on it for inspection.",
      "action": [
        "Move forward",
        "Slightly turn left",
        "Approach the left fluted column closely",
        "Inspect its surface for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/0125.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater robot and inspect its front sensor array and blue illuminated indicator light.",
    "output": {
      "reasoning": "The green cylindrical underwater robot is clearly visible in the center-right of the image, angled slightly upward and tilted toward the viewer. Its front sensor array (a rectangular panel with a circular orange element and surrounding markings) and blue illuminated indicator light (a bright blue circular LED on a black band) are both visible from the current perspective. To properly inspect these features, the robot must approach closer while maintaining a stable frontal view—ideally aligning directly in front of the target at a short distance (~1–2 m) to ensure high-resolution observation without occlusion. The seabed terrain appears uneven but navigable; no immediate obstacles block a direct approach path. Therefore, the plan is: (1) move forward steadily toward the robot’s front face, (2) fine-tune lateral position to center the sensor array in the field of view, (3) stop at optimal inspection distance, and (4) hold position for visual inspection.",
      "action": [
        "Go straight toward the green cylindrical robot",
        "Slightly adjust left to center the front sensor array in view",
        "Continue advancing slowly until ~1 meter from the robot",
        "Hold position and stabilize orientation for inspection"
      ]
    },
    "images": [
      "./data/images/0242.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral formation in the center while avoiding the tall green seaweed on the right.",
    "output": {
      "reasoning": "The large green ridged coral formation is centrally located in the image, slightly to the left of center and ahead. Tall green seaweed is visible on the right side of the frame, extending vertically and occupying the right foreground/midground. To approach the coral while avoiding the seaweed, the robot should first steer slightly left to create clearance from the seaweed, then proceed straight toward the central coral formation. No immediate obstacles block the path forward once positioned left of center.",
      "action": [
        "Turn left slightly",
        "Go straight toward the central green ridged coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0001_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-lit underwater vehicle near the coiled pipes and inspect its connection to the pipeline.",
    "output": {
      "reasoning": "From the image, a yellow-lit underwater vehicle is visible in the upper-right quadrant, positioned near coiled pipes that run diagonally across the scene. The vehicle appears to be connected to a pipeline extending from the left side. To approach it safely and inspect the connection, I need to: (1) navigate toward the vehicle while avoiding the coiled pipes on the left and right; (2) align with the vehicle’s position—likely requiring a slight right turn since it's to the right of center; (3) move forward carefully to get within inspection range without disturbing sediment or obstructing view; (4) stabilize and orient for close visual inspection of the connection point between vehicle and pipeline. The lighting source above may cause glare, so maintaining optimal distance and angle is important.",
      "action": [
        "Turn right slightly",
        "Go straight toward the yellow-lit vehicle",
        "Slow down as you near the vehicle",
        "Position yourself parallel to the vehicle-pipeline connection",
        "Inspect the connection point visually"
      ]
    },
    "images": [
      "./data/images/0297.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater vehicle positioned vertically in the center, maintaining a safe distance while aligning with its front-facing light source.",
    "output": {
      "reasoning": "The green cylindrical underwater vehicle is clearly visible in the center of the image, oriented vertically. Its front-facing light source (a bright yellow circular light) is illuminated and serves as the alignment target. The robot appears to be positioned slightly below and in front of the vehicle, with clear open water ahead—no immediate obstacles block the path. To approach while maintaining a safe distance and aligning with the front light, the robot should: (1) move forward cautiously to reduce range without getting too close; (2) adjust horizontally (if needed) to center the light in the field of view; (3) fine-tune pitch/depth to match the light’s vertical position. Since the light is already centered in the image and the vehicle is directly ahead, minimal lateral correction is needed—primarily forward motion with slight depth stabilization.",
      "action": [
        "Go straight slowly",
        "Maintain depth to keep the front light centered vertically",
        "Slightly adjust left/right if the light drifts off-center"
      ]
    },
    "images": [
      "./data/images/temp_307_0018_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater vehicle lying on the seabed near the coral formations and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The cylindrical underwater vehicle is lying horizontally on the seabed, partially surrounded by coral formations. It is positioned in the mid-to-lower center of the image, with clear visibility from the current robot perspective. To approach and inspect its surface, I need to: (1) move forward toward the vehicle while maintaining safe clearance from nearby corals; (2) position the robot parallel to the vehicle’s longitudinal axis for optimal inspection; (3) perform a slow lateral pass along its side to observe surface condition (damage/marine growth). The upper vehicle appears to be hovering or suspended—likely not the target—so focus remains on the seabed-lying cylinder. No immediate obstacles block a direct forward path, but careful navigation near corals is required.",
      "action": [
        "Go straight slowly toward the cylindrical vehicle on the seabed",
        "Maintain slight left offset to avoid coral on the right",
        "Stop when ~1 meter away from the vehicle’s midsection",
        "Turn right slightly to align parallel with the vehicle’s long axis",
        "Move laterally left along the vehicle’s side at constant distance for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0012_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated coral formation directly ahead while maintaining a safe distance from the surrounding textured coral structures.",
    "output": {
      "reasoning": "The image shows an illuminated coral formation directly ahead, centered in the field of view and highlighted by a downward beam of light (likely from the robot’s own light source). Surrounding it are textured, darker coral structures on both left and right sides, which must be avoided to maintain safety. The robot appears to be positioned at a moderate distance, with clear forward visibility. To approach the target while avoiding obstacles, the plan is: (1) proceed straight cautiously toward the illuminated coral; (2) monitor lateral proximity to textured corals and make minor course corrections if needed; (3) stop at a safe distance before contact. Since the target is directly ahead and no immediate obstruction blocks the path, the initial action is to go straight while maintaining orientation.",
      "action": [
        "Go straight",
        "Maintain heading toward illuminated coral",
        "Adjust slightly left or right only if nearing textured corals"
      ]
    },
    "images": [
      "./data/images/0144.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large corroded pipeline and inspect the green biofouling patches along its midsection.",
    "output": {
      "reasoning": "The image shows a large, horizontally oriented corroded pipeline extending from left to right, with visible green biofouling patches along its midsection (center-left portion of the pipe). The robot appears to be positioned slightly above and to the right of the pipeline, at a moderate distance. To approach and inspect the green biofouling patches on the midsection, the robot should first move forward toward the pipeline while maintaining alignment with its axis, then adjust laterally (leftward) to center on the target patches, and finally close in for detailed inspection without colliding. The seabed is relatively clear, so no major obstacles impede direct approach.",
      "action": [
        "Go straight toward the pipeline",
        "Move left to align with the green biofouling patches on the midsection",
        "Slowly advance to inspect the green patches closely"
      ]
    },
    "images": [
      "./data/images/0200.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue underwater vehicle and inspect its tether connection near the seabed.",
    "output": {
      "reasoning": "The blue underwater vehicle is visible in the center of the image, upright on the seabed with its tether trailing downward and slightly to the right. The robot appears to be positioned at a moderate distance (~2–3 m) directly in front of it. To inspect the tether connection near the seabed, I need to: (1) move forward to reduce distance safely, (2) adjust laterally (right) to align with the tether’s base, and (3) descend slightly to get a close-up view of the connection point near the seabed. Obstacles (rocks, coral) are present on both sides but leave a clear path forward and slight rightward maneuver.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Descend slowly",
        "Approach the tether connection point near the seabed"
      ]
    },
    "images": [
      "./data/images/0101.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow and black patterned pipe on the right while maintaining a safe distance from the blue-and-yellow autonomous underwater vehicle on the left.",
    "output": {
      "reasoning": "The image shows the robot's current view: a yellow-and-black patterned pipe is on the right side of the frame, extending forward; a blue-and-yellow autonomous underwater vehicle (AUV) is on the left, parallel to the pipe and slightly ahead. To approach the pipe while avoiding the AUV, the robot should first steer gently right to align toward the pipe, then proceed forward while maintaining lateral separation from the AUV. Since the AUV is on the left and the pipe is on the right, a slight rightward heading adjustment followed by straight motion will safely close distance to the pipe without encroaching on the AUV’s space. No immediate obstacle blocks the path, and the seabed provides visual reference for stable navigation.",
      "action": [
        "Turn slightly right",
        "Go straight while keeping the blue-and-yellow AUV visible on the left",
        "Continue approaching the yellow-and-black pipe"
      ]
    },
    "images": [
      "./data/images/temp_307_0068_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible vehicle lying on the seabed and inspect its front hatch and side panels for damage or marine growth.",
    "output": {
      "reasoning": "The green cylindrical submersible is lying on the seabed, angled slightly with its front hatch (with visible portholes and a rectangular panel) facing toward the upper-right of the image. The robot appears to be positioned at a moderate distance (~2–3 m) and slightly to the left-front of the vehicle. To inspect the front hatch and side panels, I need to: (1) approach steadily while maintaining a safe distance to avoid disturbing sediment; (2) align with the front of the vehicle to get a clear view of the hatch; (3) then move laterally along the side to inspect side panels. Given the current perspective, the first step is to go straight toward the vehicle’s front section, then adjust rightward to center on the hatch.",
      "action": [
        "Go straight toward the green submersible",
        "Turn slightly right to align with the front hatch",
        "Move forward slowly to inspect the front hatch",
        "Drift right along the side to inspect side panels"
      ]
    },
    "images": [
      "./data/images/temp_307_0051_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange and black netted basket on the seabed and inspect its contents.",
    "output": {
      "reasoning": "The orange and black netted basket is clearly visible on the seabed in the foreground, slightly to the right of center. Sunlight from above illuminates it, confirming its position and accessibility. The robot appears to be positioned a short distance away and slightly above it. To approach and inspect its contents, the robot should first move forward (downward/forward relative to current orientation) to get closer, then possibly adjust laterally to center on the basket opening, and finally stabilize for inspection. Given the basket lies flat on the seabed and its opening faces upward, a direct forward descent with minor right correction will align the robot for optimal viewing.",
      "action": [
        "Go straight forward",
        "Slightly turn right",
        "Descend gently toward the basket",
        "Hover and inspect the contents"
      ]
    },
    "images": [
      "./data/images/temp_307_0024_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green columnar structure and inspect its base for attached marine growth or debris.",
    "output": {
      "reasoning": "The image shows a prominent central glowing green columnar structure, illuminated and standing vertically. The robot appears to be positioned slightly to the left and at a moderate distance from the structure, with its base visible and surrounded by net-like debris and some marine growth (e.g., mesh entanglements and possible biofouling near the bottom). To inspect the base closely, the robot must first approach the structure directly while maintaining stable orientation. Given the relatively open space around the column, a straight-forward approach is feasible. Once near the base (within ~1–2 m), a slight downward tilt or hover will allow visual inspection of attached growth/debris. No immediate obstacles block the direct path to the base.",
      "action": [
        "Go straight toward the central green column",
        "Slow down as you near the base of the column",
        "Hover and tilt camera downward to inspect the base for marine growth or debris"
      ]
    },
    "images": [
      "./data/images/temp_307_0084_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical object on the seabed while avoiding the large porous rock to its right.",
    "output": {
      "reasoning": "The green cylindrical object lies on the seabed in the lower-left portion of the image. A large porous rock is positioned to its right (from the robot's perspective), occupying the mid-right area. To approach the green object while avoiding the rock, the robot should first move forward cautiously toward the green cylinder, then slightly adjust leftward to maintain clearance from the rock. Since the green object is already relatively close and in clear view, a direct but slightly left-biased approach is safe and efficient.",
      "action": [
        "Go straight",
        "Slightly turn left",
        "Continue approaching the green cylindrical object"
      ]
    },
    "images": [
      "./data/images/0233.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green truck and inspect its rear cargo area for attached objects or damage.",
    "output": {
      "reasoning": "The green truck is visible in the mid-left portion of the image, partially obscured by a stone column on the right. It is oriented with its rear facing roughly toward the camera, slightly angled to the left. To inspect the rear cargo area, I need to approach from the front or side and then maneuver behind it. Given the column is to the right of the truck, approaching from the left side avoids obstruction. The seabed is relatively clear, allowing forward movement. Step 1: Move forward and slightly left to align with the truck’s left side. Step 2: Continue forward to reach a position parallel to the truck’s rear. Step 3: Turn right to face the rear cargo area directly. Step 4: Move forward slowly to get close enough for detailed inspection.",
      "action": [
        "Move forward and slightly left",
        "Continue forward to align with the truck's rear",
        "Turn right to face the rear cargo area",
        "Move forward slowly for close inspection"
      ]
    },
    "images": [
      "./data/images/0226.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large dark rock on the seafloor while maintaining a safe distance from the floating green AUV above.",
    "output": {
      "reasoning": "The image shows a large dark rock on the seafloor to the right of the current robot position (viewed from the robot's perspective). A green AUV is floating above, slightly left and ahead. To approach the rock safely while avoiding the AUV: (1) First, move forward cautiously to close distance to the rock without ascending toward the AUV’s depth; (2) Then, gently turn right to align with the rock’s position; (3) Finally, continue forward at a low altitude to approach the rock while staying well below the AUV’s vertical position (the AUV is near surface, robot is near seafloor—so horizontal maneuvering suffices for safety). The orange object in foreground appears to be part of the robot or a nearby marker; it does not obstruct the path.",
      "action": [
        "Go straight slowly",
        "Turn right slightly",
        "Go straight toward the large dark rock"
      ]
    },
    "images": [
      "./data/images/0253.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured rock on the left while avoiding the overhanging pipe above.",
    "output": {
      "reasoning": "The image shows a large textured rock on the left side of the frame, partially illuminated by sun rays. An overhanging pipe (or cylindrical structure) spans horizontally across the upper portion of the scene, directly above the rock and the robot’s likely path. To approach the rock safely, the robot must avoid ascending too high (to prevent collision with the pipe) and instead maintain a lower trajectory while moving leftward toward the rock. The rock is relatively close and visible, so a controlled lateral movement followed by forward advance should suffice. Since the pipe is overhead, staying near the seabed (where smaller rocks and sediment are visible) provides clearance.",
      "action": [
        "Turn left slightly",
        "Descend gently to stay below the pipe",
        "Move forward toward the large textured rock on the left",
        "Stop when near the rock surface"
      ]
    },
    "images": [
      "./data/images/0165.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its carved surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and positioned directly in front of the robot’s current viewpoint. It stands upright on the seabed, with its carved surface facing the camera. To inspect the surface for inscriptions or damage, the robot must first close the distance to ensure high-resolution visual capture. Given the clear path ahead and no immediate obstacles, the optimal sequence is: (1) move forward toward the pillar until within close inspection range (~1–2 meters), (2) optionally adjust orientation slightly to align the camera perpendicular to the carved face for optimal lighting and detail, and (3) perform a slow circumferential scan or hover to examine all visible sides if needed. Since the instruction specifies 'approach' and 'inspect its carved surface', the initial priority is forward motion to reach proximity.",
      "action": [
        "Go straight toward the central stone pillar",
        "Slow down and stop at ~1.5 meters from the pillar",
        "Adjust heading to face the carved surface directly",
        "Hover and perform a slow vertical scan of the pillar's front face"
      ]
    },
    "images": [
      "./data/images/temp_307_0019_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The submerged stone column is clearly visible in the center of the image, standing vertically on the seabed. Sunlight rays illuminate it from above, enhancing visibility. The robot appears to be positioned at a moderate distance, slightly in front and possibly to the left of the column (based on shadow orientation and perspective). To approach and inspect its surface, the robot should first move forward toward the base of the column, then circle or reposition laterally to examine different facets—especially areas where marine growth (e.g., algae, barnacles) or cracks might be present. Given the clear water and lack of immediate obstacles, a direct approach followed by a close circumferential inspection is optimal.",
      "action": [
        "Go straight toward the stone column",
        "Slow down and stop ~1 meter from the base",
        "Circle clockwise around the column at close range",
        "Pause briefly at each quadrant to inspect surface details",
        "Tilt camera upward to examine upper sections and cap"
      ]
    },
    "images": [
      "./data/images/0145.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large ridged green object on the seabed while maintaining a safe distance from the coral structure to its right.",
    "output": {
      "reasoning": "The image shows a large ridged green object (likely the target) lying horizontally on the seabed, oriented roughly left-to-right. To its right is a dense coral structure with fan-shaped corals—this must be avoided per the instruction. The robot appears to be positioned slightly in front and to the left of the green object, with clear space on its left side and ahead. To approach safely while avoiding the coral on the right, the optimal path is to move forward while gently steering left to maintain clearance from the coral. No immediate obstacle blocks direct forward-left motion. A single coordinated action—'Go straight while slightly turning left'—suffices to begin safe approach; fine adjustments can follow in subsequent steps if needed.",
      "action": [
        "Go straight while slightly turning left"
      ]
    },
    "images": [
      "./data/images/temp_307_0032_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine along its starboard side while maintaining a safe distance from the algae-covered seabed and the hull.",
    "output": {
      "reasoning": "The submarine is oriented diagonally in the frame, with its stern (propeller and rudder) visible on the left and bow extending toward the upper right. The starboard side (right side when facing forward) is the side currently visible — it has yellow markings and is adjacent to the algae-covered seabed on the left of the image (which is port side relative to the sub). To approach along the starboard side while avoiding the algae-covered seabed (on the port side of the sub), I must position myself to the right of the submarine’s longitudinal axis. From the current viewpoint, the robot appears slightly ahead and to the port side of the sub (since the sub’s starboard side is facing us, and we see its full starboard hull). To get into a safe parallel trajectory along the starboard side: (1) First, move backward slightly to increase clearance from the hull and reposition; (2) Then turn right (clockwise) to align parallel to the sub’s starboard flank; (3) Finally, advance slowly while maintaining lateral distance to avoid both the hull and the algae bed (which lies on the opposite side, so staying on the starboard side inherently avoids it, but we must not drift left).",
      "action": [
        "Move backward",
        "Turn right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0034_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spiral-shaped coral formation in the center-left while avoiding the shadowed overhang above.",
    "output": {
      "reasoning": "The large spiral-shaped coral formation is clearly visible in the center-left of the image, illuminated by sun rays. Above it, there is a dark, shadowed overhang extending from the upper right toward the center, which must be avoided. To approach the coral safely, I should first move forward slightly to get closer while staying below the overhang’s underside. Then, gently steer left to align with the coral’s position, ensuring the robot remains in the well-lit, open water column and avoids entering the shadowed region directly above the target. The seabed appears relatively flat and obstacle-free in the path toward the coral.",
      "action": [
        "Go straight slowly",
        "Turn left slightly",
        "Go straight toward the spiral coral",
        "Maintain depth to stay below the overhang"
      ]
    },
    "images": [
      "./data/images/0296.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the textured vertical pipe structure while avoiding the surrounding coral formations and sediment on the seabed.",
    "output": {
      "reasoning": "The textured vertical pipe is the central target, positioned slightly right of center in the image. Coral formations are visible on the left and near the base of the pipe, and sediment covers the seabed. To approach safely: (1) Start by moving forward cautiously to close distance while staying centered to avoid left-side coral; (2) Slightly adjust rightward as approaching the mid-section to clear the left coral mass; (3) Maintain altitude to avoid disturbing seabed sediment; (4) Final approach straight toward the pipe’s mid-to-upper section, where it’s most visible and unobstructed. The pipe’s strong vertical alignment and light refraction help guide orientation.",
      "action": [
        "Go straight slowly",
        "Slight right turn to center on pipe",
        "Maintain steady altitude",
        "Approach pipe vertically"
      ]
    },
    "images": [
      "./data/images/0166.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater device with glowing red lights and inspect its front face for markings or damage.",
    "output": {
      "reasoning": "The cylindrical underwater device with glowing red lights is clearly visible in the center-left of the image, resting on the seabed. Its front face (the circular end with four red lights and central ports) is oriented toward the camera, indicating the robot is already roughly aligned with the target. However, the current distance appears moderate (~2–3 meters), and to inspect markings or damage, the robot must move closer while maintaining frontal alignment. No immediate obstacles block the direct path. Therefore, the plan is: (1) advance straight toward the device to reduce distance, (2) fine-tune position to center the front face in view, (3) halt at close inspection range (~0.5–1 m) for detailed observation.",
      "action": [
        "Go straight toward the cylindrical device",
        "Adjust position slightly to center the front face in view",
        "Stop at close range for inspection"
      ]
    },
    "images": [
      "./data/images/0211.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue segmented pipe on the right and inspect its connection point with the wall.",
    "output": {
      "reasoning": "The blue segmented pipe is visible on the right side of the image, extending diagonally downward and connecting to a vertical wall structure. The robot is currently facing forward with a black pipe in the center-left foreground; the blue pipe lies to its right and slightly ahead. To approach the connection point, the robot must first reorient toward the blue pipe (turn right), then move forward while maintaining alignment with the pipe’s axis, and finally stop near the wall interface for inspection. The lighting from above suggests good visibility along this path, and no immediate obstacles block the route.",
      "action": [
        "Turn right",
        "Go straight toward the blue segmented pipe",
        "Align with the pipe axis",
        "Approach the wall connection point",
        "Stop and inspect the connection"
      ]
    },
    "images": [
      "./data/images/0099.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-banded pipe structure while avoiding entangled yellow ropes and netting on the seafloor.",
    "output": {
      "reasoning": "The large green-banded pipe structure is directly ahead and slightly elevated in the image, serving as the primary target. Entangled yellow ropes and netting are densely spread across the seafloor in the foreground and left side, posing entanglement risks. To safely approach, I must first ascend slightly to clear the immediate netting, then navigate rightward to avoid the dense rope cluster on the left, and finally proceed forward toward the pipe while maintaining clearance from debris below. The robot’s current orientation faces the pipe, so minor lateral correction is needed before advancing.",
      "action": [
        "Ascend slightly",
        "Turn right 15 degrees",
        "Go straight slowly",
        "Maintain altitude to avoid seafloor debris"
      ]
    },
    "images": [
      "./data/images/temp_307_0016_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its base for signs of structural damage or marine growth.",
    "output": {
      "reasoning": "The central stone column is clearly visible and positioned slightly to the right of the robot’s current forward view. Sunlight beams from the upper left suggest the robot is approaching from a diagonal front-left angle. To inspect the base, the robot must first close the distance while maintaining a stable approach trajectory—avoiding the chain-linked structure on the left and debris on the right. The optimal path is to move forward while slightly adjusting rightward to center the column in the field of view, then continue straight until near the base (~1–2 m), where a final slight downward tilt or hover may be needed for inspection (though camera pitch is not an explicit action here; we assume visual inspection occurs upon proximity). No immediate obstacles block the direct path to the column’s base.",
      "action": [
        "Move forward",
        "Slightly turn right",
        "Move forward",
        "Move forward"
      ]
    },
    "images": [
      "./data/images/0209.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange cylindrical object on the seabed while avoiding the surrounding rocks and maintaining a safe distance from the nearby fish.",
    "output": {
      "reasoning": "The orange cylindrical object is clearly visible on the seabed in the foreground, slightly to the right of center. Surrounding it are several rocks: one large dark rock to the right, a textured rounded rock to the left, and smaller rocks scattered nearby. A fish is swimming in the upper-left midwater region, away from the direct path but requiring caution. To approach safely: (1) Start by moving forward slowly toward the orange object while keeping it centered in view; (2) Slightly adjust leftward to avoid the large rock on the right and maintain clearance from the left rock; (3) Monitor the fish’s position—since it’s not directly in the path and appears distant, no evasive maneuver is needed yet, but maintain awareness. The optimal path is a gentle forward-left arc toward the object, staying ~0.5–1 m away from nearest rocks.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Continue forward while keeping orange object centered",
        "Maintain distance from left and right rocks",
        "Approach orange cylindrical object steadily"
      ]
    },
    "images": [
      "./data/images/temp_307_0059_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green pipeline along the seabed while maintaining a safe distance from the coral formations on the left.",
    "output": {
      "reasoning": "The image shows a green pipeline extending diagonally from the upper left toward the lower right, lying along the seabed. Coral formations are clearly visible on the left side of the frame, close to the pipeline. To follow the pipeline while maintaining a safe distance from the corals, the robot should position itself slightly to the right of the pipeline’s centerline. The pipeline curves gently to the right in the distance, so the robot must adjust its heading gradually to stay aligned. Initial action: move forward while slightly veering right to establish a safe lateral offset from the left-side corals. Subsequent actions will involve minor course corrections to track the pipeline’s curvature.",
      "action": [
        "Go straight while slightly turning right",
        "Maintain alignment with the green pipeline",
        "Adjust rightward as the pipeline curves right"
      ]
    },
    "images": [
      "./data/images/0160.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red bucket suspended by the net and inspect its contents.",
    "output": {
      "reasoning": "The red bucket is suspended centrally in the image, hanging from a net attached to a vertical structure above. It is clearly visible and positioned slightly ahead and to the left of the robot’s current viewpoint (based on perspective and lighting). To approach it, the robot should first move forward toward the bucket while maintaining alignment. As it gets closer, a slight leftward adjustment may be needed to center the bucket in view. Once near, it should stop and orient itself for inspection—likely by positioning directly in front and possibly tilting the camera downward to peer into the bucket. No immediate obstacles block the direct path, though the blue object on the seabed and the right-side cage should be avoided by staying centered.",
      "action": [
        "Go straight toward the red bucket",
        "Slightly turn left to align with the bucket",
        "Go straight until close to the bucket",
        "Stop and position directly in front of the bucket",
        "Tilt camera downward to inspect contents"
      ]
    },
    "images": [
      "./data/images/0173.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green vehicle and inspect its front-right side for structural damage or marine growth.",
    "output": {
      "reasoning": "The submerged green vehicle is visible in the center-right of the image, lying on the seabed at a slight angle. Its front-right side (from the robot’s perspective) is partially exposed and accessible, with visible marine growth and possible structural damage (e.g., dents, missing panels). The robot appears to be positioned slightly behind and to the left of the vehicle. To approach and inspect the front-right side, the robot should: (1) move forward toward the vehicle while maintaining clearance from nearby coral (left) and sediment; (2) adjust rightward to align with the front-right quadrant; (3) close in for detailed inspection without disturbing sediment or contacting obstacles.",
      "action": [
        "Go straight toward the green vehicle",
        "Turn slightly right to align with the front-right side",
        "Move forward slowly to inspect the front-right side closely"
      ]
    },
    "images": [
      "./data/images/0223.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red spherical object with circular indentations on the right while avoiding the nearby coral formations.",
    "output": {
      "reasoning": "The red spherical object with circular indentations is visible on the right side of the image, partially surrounded by coral formations (dark, textured structures with ridges and openings). The robot is currently positioned left-center, facing forward. To approach the target while avoiding corals: first, the robot must steer right to align toward the red sphere; second, it should maintain a safe lateral distance from the nearby coral on the right (visible just left of the sphere); third, proceed forward cautiously as the sphere is relatively close but obstructed slightly by foreground coral. No immediate backward or sharp turns are needed—gradual right turn followed by straight advance suffices.",
      "action": [
        "Turn right slightly",
        "Go straight slowly",
        "Maintain slight rightward offset to avoid coral"
      ]
    },
    "images": [
      "./data/images/0141.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submarine visible through the cave opening while avoiding the coral formations on the seabed.",
    "output": {
      "reasoning": "The submarine is visible through the cave opening ahead and slightly above the current viewpoint. The seabed in the foreground contains prominent coral formations (especially a large dark one at bottom center and others to the left), which must be avoided. To approach the submarine safely, the robot should first ascend slightly to clear the near-field corals, then proceed forward toward the cave opening while maintaining a central path to avoid lateral corals. Given the lighting and perspective, the submarine lies directly ahead beyond the cave threshold, so a straight-forward trajectory with minor upward adjustment is optimal.",
      "action": [
        "Ascend slightly",
        "Go straight toward the cave opening",
        "Maintain central path to avoid left and right corals"
      ]
    },
    "images": [
      "./data/images/0107.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue submarine resting on the rocky seabed and inspect its rear thruster area for damage or debris.",
    "output": {
      "reasoning": "The blue submarine is visible resting on the rocky seabed, oriented diagonally with its rear (thruster end) pointing toward the lower-left of the image. Thrusters are located at the tail end, identifiable by the cylindrical housings and exhaust plume trailing upward from the rear. To inspect the rear thruster area, I must first approach the submarine from a safe angle—ideally from the side or slightly behind—to avoid disturbing sediment or colliding with rocks. Given current perspective, the robot is likely ahead and slightly above the sub. The optimal path is: (1) move forward cautiously to close distance while maintaining clearance from rocks; (2) reposition to the submarine’s starboard (right) side to get lateral access; (3) maneuver aft (toward the tail) along the side to align with the rear thruster zone; (4) stabilize and orient camera toward the thrusters for inspection. No immediate obstacles block this path, but careful navigation near the rocky base is required.",
      "action": [
        "Go straight slowly to approach the submarine",
        "Turn right to position alongside the submarine's starboard side",
        "Move aft (backward relative to submarine orientation) to reach the rear thruster area",
        "Hover and adjust orientation to face the rear thrusters for inspection"
      ]
    },
    "images": [
      "./data/images/0240.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central corroded column and inspect the circular openings along its shaft for signs of marine growth or structural damage.",
    "output": {
      "reasoning": "The central corroded column is clearly visible in the image, positioned directly ahead of the robot's current viewpoint. It features multiple circular openings along its shaft, some of which appear partially obscured by marine growth (e.g., the cluster of greenish-yellow organisms near the top-right opening). To inspect these openings for marine growth or structural damage, the robot must first approach the column to reduce distance and improve visual resolution. Given the column’s central placement and the relatively clear path forward (no immediate obstacles in front), the optimal first step is to move straight toward it. Once closer, fine positioning (e.g., slight lateral adjustments or hovering) would be needed for detailed inspection—but per the instruction, the immediate macro-step is *approach*. The image shows no red obstacles or hazards requiring avoidance in the direct path.",
      "action": [
        "Go straight toward the central corroded column"
      ]
    },
    "images": [
      "./data/images/0255.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral structure in the center while maintaining a safe distance from the surrounding smaller corals and sponges.",
    "output": {
      "reasoning": "The large green ridged coral structure is clearly centered in the image and dominates the scene. Smaller corals and sponges are scattered around its base and periphery—specifically, a yellow spiky sponge in the lower right, a purple-spotted sponge nearby, and several smaller green corals to the left and front. To approach safely, I must avoid lateral drift toward these obstacles. The robot appears to be positioned slightly in front and below the main structure, with open water above and ahead. A controlled forward advance with slight upward pitch will bring the robot closer while preserving clearance from the surrounding benthic elements. No immediate turn is needed since the target is directly ahead and centered.",
      "action": [
        "Go straight slowly",
        "Maintain slight upward pitch to stay above seabed obstacles",
        "Monitor left and right for proximity to smaller corals and sponges"
      ]
    },
    "images": [
      "./data/images/temp_307_0039_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car and inspect its open driver-side door for potential artifacts or structural damage.",
    "output": {
      "reasoning": "The green submerged car is clearly visible in the center-left of the image, resting on the seabed. Its driver-side door is open and facing toward the camera's current position, suggesting it is accessible from the front-left approach angle. To inspect the open driver-side door for artifacts or damage, I need to: (1) move forward toward the car while maintaining a safe distance to avoid disturbing sediment; (2) adjust laterally to align with the open door (which appears on the left side of the vehicle from our perspective); (3) close in for a detailed visual inspection. The seabed is relatively clear, and no immediate obstacles block the path. Sunlight from above provides good illumination, aiding visibility.",
      "action": [
        "Go straight toward the green car",
        "Move slightly left to align with the open driver-side door",
        "Approach closer to inspect the interior and door frame for artifacts or damage"
      ]
    },
    "images": [
      "./data/images/0186.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green giant clam on the seabed while maintaining a safe distance to avoid disturbing it.",
    "output": {
      "reasoning": "The image shows a large green giant clam centered on the seabed, with its mantle fully extended. The robot appears to be positioned slightly above and in front of it (based on perspective and lighting). To approach while maintaining a safe distance, we should avoid sudden movements or descending too close—instead, proceed slowly and horizontally toward it from the current position. Since the clam is directly ahead and occupies most of the frame, a gentle forward motion (without descending) will bring the robot closer while keeping clearance. No immediate obstacles are present in the direct path, but small sea urchins are nearby; thus, maintain slight lateral clearance if needed. The safest initial step is to move straight forward at a moderate pace, monitoring proximity.",
      "action": [
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/0133.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue cylindrical structure with yellow bands and inspect its midsection for attached sensors or biofouling.",
    "output": {
      "reasoning": "The blue cylindrical structure with yellow bands is directly in front of the robot and dominates the central field of view. It appears vertically oriented and stationary, resting on the seabed. The midsection (roughly middle third of the cylinder) is visible but not yet close enough for detailed inspection of sensors or biofouling. To inspect it properly, the robot must first approach closer while maintaining a stable frontal orientation. No immediate obstacles block the direct path forward. Therefore, the plan is: (1) move straight toward the structure to reduce distance, (2) position at optimal inspection range (~1–2 m), (3) stabilize and possibly adjust yaw/pitch slightly to center the midsection in frame for visual inspection.",
      "action": [
        "Go straight toward the blue cylindrical structure",
        "Slow down as you near the structure",
        "Position at ~1.5 meters distance facing the midsection",
        "Stabilize and adjust heading to center the midsection in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0049_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate upward along the central light beam while avoiding the algae-covered rock walls on both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage with algae-covered rock walls on left and right. A strong vertical light beam illuminates the center, suggesting an upward path toward the surface or open water. The robot is positioned near the bottom of this shaft, with bubbles rising along the central axis—indicating upward motion is feasible and aligned with the light. To avoid collisions, the robot must stay centered within the light beam, maintaining equal clearance from both algae-covered walls. Since the passage appears relatively straight vertically, a sequence of controlled upward movements with minor lateral corrections (if needed) will suffice. No immediate turns are required; the priority is steady ascent while monitoring proximity to walls.",
      "action": [
        "Go straight upward along the central light beam",
        "Maintain center alignment to avoid left and right algae-covered walls",
        "Continue ascending steadily"
      ]
    },
    "images": [
      "./data/images/temp_307_0013_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the stone pillar structure and inspect its base for signs of marine growth or structural damage.",
    "output": {
      "reasoning": "The stone pillar structure is clearly visible in the center-right of the image, standing vertically on the seabed. Sunlight rays illuminate it from above and left, indicating good visibility. The base of the pillar is partially visible but not yet close enough for detailed inspection of marine growth or damage. To inspect the base, I need to approach it directly while maintaining a stable orientation. Given its central position and clear path (no immediate obstacles in front), the optimal first step is to move straight toward the pillar. Once closer, I can circle or adjust position to examine the base thoroughly—but the immediate atomic action is forward motion toward the target.",
      "action": [
        "Go straight toward the stone pillar"
      ]
    },
    "images": [
      "./data/images/temp_307_0081_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of green mushroom-shaped corals in the center foreground while avoiding the large blue-striped shell on the left.",
    "output": {
      "reasoning": "The image shows a cluster of green mushroom-shaped corals in the center foreground, which is the target. To the left, there is a large blue-striped shell (likely a giant clam) that must be avoided. The robot is currently positioned such that the corals are directly ahead and slightly to the right of center, while the shell occupies the left midground. To safely approach the corals, the robot should first steer slightly right to clear the shell’s position, then proceed straight toward the coral cluster. No immediate obstacles lie directly in the central path once the left-side shell is bypassed.",
      "action": [
        "Turn slightly right",
        "Go straight toward the green mushroom-shaped corals"
      ]
    },
    "images": [
      "./data/images/temp_307_0079_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the weathered vertical column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The image shows a weathered vertical column centered in the frame, illuminated by sunlight from above. The robot appears to be positioned at a moderate distance (~2–3 m) directly in front of the column, with clear visibility of its surface. To inspect for marine growth or structural damage, the robot must first approach closer (within ~0.5–1 m) to resolve fine details, then possibly circle or tilt to examine different facets. Given the column is straight ahead and no immediate obstacles block the path, the optimal initial action is to move forward steadily while maintaining alignment with the column’s axis.",
      "action": [
        "Go straight toward the column",
        "Slow down as you near the column (within 1 meter)",
        "Hover and rotate slowly clockwise to inspect the surface"
      ]
    },
    "images": [
      "./data/images/0155.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the coiled yellow rope on the seafloor while avoiding the large submerged pipe and netting overhead.",
    "output": {
      "reasoning": "The coiled yellow rope is visible on the seafloor in the lower-left foreground. A large submerged pipe runs diagonally across the midground, and netting hangs overhead (especially above and behind the pipe). To approach the rope safely: (1) The robot must stay low to avoid the overhead netting; (2) It should navigate *around* the pipe—since the rope lies to the left of the pipe’s near end, approaching from the left side avoids collision; (3) The current view suggests the robot is slightly right and rearward of the rope, so initial movement should be leftward and slightly forward, then fine-tune toward the rope while maintaining clearance from the pipe’s left edge. No immediate obstacles block the direct left-forward path to the rope.",
      "action": [
        "Turn left",
        "Go straight slowly",
        "Move forward toward the coiled yellow rope",
        "Maintain altitude low to avoid overhead netting",
        "Adjust left to pass clear of the pipe’s left edge"
      ]
    },
    "images": [
      "./data/images/temp_307_0089_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green sponge-like structure in the center while avoiding the coral formations on the right and left.",
    "output": {
      "reasoning": "The large green sponge-like structure is centrally located and appears to be the main target. Coral formations are visible on both left (tall, dark, textured) and right (branching, yellowish) sides, forming a narrow corridor. The robot is currently positioned slightly below and in front of the sponge, with clear forward path but requiring lateral correction to stay centered and avoid corals. To safely approach: first, adjust heading slightly right to center relative to the sponge (since left coral is closer and more prominent), then proceed straight toward the sponge while maintaining central alignment. No immediate sharp turns needed—small corrections suffice.",
      "action": [
        "Turn slightly right",
        "Go straight toward the green sponge-like structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0002_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front right side for damage or marine growth.",
    "output": {
      "reasoning": "The submerged car is visible in the lower-right portion of the image, oriented diagonally with its front facing roughly toward the upper-left. The front right side (from the car's perspective) corresponds to the side closest to the camera and slightly to the right — i.e., the visible right fender and headlight area in the image. To inspect it closely, I need to: (1) move forward toward the car while maintaining a safe distance initially; (2) adjust position to align with the front right quadrant; (3) close in for detailed visual inspection. There are no immediate obstacles blocking the path, though stone structures are nearby—so lateral movement must be cautious. Given the current view, the most direct approach is to advance straight, then slightly right to position for inspection.",
      "action": [
        "Go straight toward the submerged car",
        "Slightly turn right to align with the car's front right side",
        "Move closer to inspect the front right fender and headlight area"
      ]
    },
    "images": [
      "./data/images/temp_307_0074_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured cylindrical structure on the right while avoiding the foreground sea anemones and the central vertical pipe.",
    "output": {
      "reasoning": "The image shows a large textured cylindrical structure on the right side of the frame, which is the target. In the foreground (bottom-left and center), there are sea anemones with distinct ridged textures that must be avoided. A central vertical pipe runs upward near the center, also to be avoided. The robot appears to be positioned slightly left and front of the target cylinder. To approach safely: first, move rightward to clear the foreground anemones and align laterally with the cylinder; then, proceed forward while maintaining distance from the central pipe; finally, close in on the cylinder’s side, ensuring no collision with nearby obstacles.",
      "action": [
        "Turn right slightly",
        "Move forward cautiously",
        "Adjust left to avoid central pipe",
        "Continue forward toward the cylindrical structure",
        "Approach the side of the large textured cylinder"
      ]
    },
    "images": [
      "./data/images/0256.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large teal cylindrical structure with visible portholes and inspect its base where it connects to the seabed pipeline.",
    "output": {
      "reasoning": "The image shows a large teal cylindrical structure with visible portholes, standing vertically and connected at its base to a horizontal pipeline lying on the seabed. The robot appears to be positioned slightly in front and below the structure, with the base and pipeline clearly visible in the lower part of the frame. To inspect the base connection, the robot must move closer while maintaining a stable orientation to avoid disturbing sediment or losing visual clarity. Given the current perspective, the optimal path is to advance straight toward the base, then possibly adjust laterally for a better inspection angle if needed. Since the instruction specifies approaching and inspecting the base connection, the first step is to go straight toward the structure’s lower section.",
      "action": [
        "Go straight toward the base of the teal cylindrical structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0037_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central cylindrical structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a central cylindrical structure (likely a pilings or column) surrounded by other debris and smaller structures on the seabed. Sunlight penetrates from above, illuminating the scene. The robot appears to be positioned slightly left and front of the target structure, at moderate distance (~3–5 m). To approach and inspect its surface, the robot should first move forward toward the structure while maintaining alignment with its center. As it nears (~1–2 m), it should adjust laterally (slightly right) to position itself parallel to the cylinder for optimal visual inspection. Finally, it should circle or hold position to scan the surface for marine growth or damage.",
      "action": [
        "Go straight toward the central cylindrical structure",
        "Adjust slightly right to align parallel to the cylinder",
        "Move closer until ~1 meter away",
        "Hold position and slowly rotate clockwise to inspect the surface"
      ]
    },
    "images": [
      "./data/images/0286.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged pipe structure and inspect the flexible blue hose connected to its side for signs of damage or detachment.",
    "output": {
      "reasoning": "The image shows a large submerged pipe structure lying horizontally on the seabed, with a flexible blue hose connected to its side (visible on the right end of the pipe). The robot is currently positioned slightly above and to the left of the pipe, with clear visibility of the hose connection point. To inspect the hose for damage or detachment, the robot must first approach the pipe structure closely, then maneuver to position itself parallel and near the hose connection—ideally at a slight angle to allow visual inspection of the joint and hose surface. Given the current view, the optimal path is: (1) move forward toward the pipe to reduce distance, (2) adjust rightward to align with the hose connection zone, (3) fine-tune position to stabilize for close inspection.",
      "action": [
        "Go straight toward the pipe structure",
        "Turn right slightly to align with the blue hose connection",
        "Move forward slowly to get closer to the hose",
        "Hold position for visual inspection of the hose and its attachment point"
      ]
    },
    "images": [
      "./data/images/temp_307_0075_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the bright light source at the center of the circular tunnel while maintaining a safe distance from the textured inner walls.",
    "output": {
      "reasoning": "The image shows a circular tunnel with a bright light source at the center, indicating the forward direction of travel. The inner walls are highly textured and patterned, suggesting potential hazards or turbulence if too close; therefore, maintaining central alignment is critical for safety. The robot appears to be positioned near the tunnel axis, facing the light. To navigate safely: (1) Keep heading directly toward the light (i.e., straight ahead), as it marks the tunnel’s centerline; (2) Slightly adjust left/right as needed to avoid drifting toward the textured walls—however, since no immediate wall proximity is observed in the current frame, initial action is to proceed straight while monitoring lateral deviation. No turns are required yet; sustained forward motion with minor course correction suffices.",
      "action": [
        "Go straight",
        "Maintain center alignment toward the light",
        "Monitor distance to walls and correct laterally if drifting"
      ]
    },
    "images": [
      "./data/images/0110.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green and orange buoy structure and inspect the circular opening near its midsection.",
    "output": {
      "reasoning": "The image shows a green-and-orange buoy structure directly in front of the robot, with a clearly visible circular opening near its midsection (green section). The robot is already aligned with the buoy and at moderate distance. To inspect the circular opening closely, the next steps should be: (1) move forward to reduce distance while maintaining alignment, (2) fine-tune position to center the opening in view, and (3) stabilize for inspection. No obstacles block the direct path.",
      "action": [
        "Go straight toward the buoy",
        "Adjust position to center the circular opening in view",
        "Hold position for inspection"
      ]
    },
    "images": [
      "./data/images/0213.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large brain coral and inspect its central opening for signs of marine life or debris.",
    "output": {
      "reasoning": "The large brain coral is directly in front of the robot, centered in the image, with its central opening clearly visible and facing the camera. The robot is at a moderate distance—close enough to identify features but not yet in inspection range. To inspect the central opening for marine life or debris, the robot should first move forward steadily to reduce distance without disturbing sediment. Once near, a slight upward or lateral adjustment may be needed to align the camera with the opening’s interior, but the initial priority is forward approach. No obstacles block the direct path; a small fish is to the left but poses no interference.",
      "action": [
        "Go straight toward the brain coral",
        "Slow down as you near the central opening",
        "Position camera to center on the opening for close inspection"
      ]
    },
    "images": [
      "./data/images/0197.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green brain coral in the center while avoiding the surrounding smaller corals and sponges.",
    "output": {
      "reasoning": "The large green brain coral is clearly centered in the image and serves as the primary target. Surrounding it are smaller corals (yellow-brown brain-type on the left) and sponges (e.g., green ring-shaped near bottom right). To approach safely, I must navigate through a relatively clear path—observing that the space directly in front of the large coral (slightly lower-center) appears less cluttered. The robot should first align toward the center, then advance cautiously while monitoring proximity to nearby obstacles. Since the large coral dominates the central field, a straight-forward approach with minor lateral correction is optimal. No immediate collision risk is visible in the direct forward path, but small protrusions near the base suggest slow, controlled movement is needed.",
      "action": [
        "Turn slightly left to center the large green brain coral in the field of view",
        "Move forward slowly",
        "Maintain slight rightward offset to avoid the yellow brain corals on the left",
        "Continue forward until within 1 meter of the large green brain coral"
      ]
    },
    "images": [
      "./data/images/0246.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green ancient tower while avoiding the large rock archway on the left.",
    "output": {
      "reasoning": "The glowing green ancient tower is the target, positioned centrally-right in the scene. A large rock archway dominates the left side, forming a potential collision hazard if the robot proceeds straight or drifts left. The open seafloor path to the right of the archway provides a safe corridor toward the tower. To avoid the archway while approaching the tower, the robot should first steer slightly right to clear the archway’s outer edge, then proceed straight toward the tower, maintaining a safe lateral offset from the left obstacle.",
      "action": [
        "Turn right slightly",
        "Go straight toward the glowing green tower",
        "Maintain slight rightward offset to avoid the rock archway"
      ]
    },
    "images": [
      "./data/images/temp_307_0025_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small blue cylindrical object resting on the sandy seabed near the coral cluster.",
    "output": {
      "reasoning": "From the image, a small blue cylindrical object is visible on the sandy seabed, positioned slightly left of center and in the mid-ground, near a coral cluster to its right. The robot appears to be at a higher elevation and farther back (given perspective and light rays). To approach it safely: first, descend gently toward the seabed to reduce altitude; then move forward while maintaining orientation toward the object; finally, fine-tune lateral position to align directly with the cylinder, avoiding nearby coral.",
      "action": [
        "Descend slowly",
        "Move forward toward the blue cylindrical object",
        "Adjust left/right position to center on the object",
        "Approach closely without disturbing sediment"
      ]
    },
    "images": [
      "./data/images/temp_307_0069_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central blue dome-shaped coral while maintaining a safe distance from the surrounding yellow spiky corals.",
    "output": {
      "reasoning": "The central blue dome-shaped coral is clearly visible in the image, positioned slightly ahead and centered in the field of view. Surrounding it are several yellow spiky corals—especially prominent in the lower-left and left mid-ground—that pose collision risk if approached directly. To maintain a safe distance, I should first align the robot to face the blue coral directly (ensuring no immediate obstruction), then advance cautiously while monitoring lateral proximity to yellow corals. Given the layout, a slight rightward offset may help avoid the densest yellow cluster on the left. The path appears relatively clear forward from current position, so a controlled forward motion with minor heading correction is feasible.",
      "action": [
        "Turn slightly right to center the blue dome-shaped coral in the view",
        "Move forward slowly while keeping the blue coral centered",
        "Maintain at least one robot-width distance from yellow spiky corals on the left"
      ]
    },
    "images": [
      "./data/images/0217.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large central coral formation with bulbous, textured polyps and inspect its surface for signs of marine life or anomalies.",
    "output": {
      "reasoning": "The image shows a large central coral formation with distinctive bulbous, textured polyps, clearly the target. It is positioned directly ahead and slightly elevated relative to the robot’s current viewpoint. The surrounding seabed is relatively clear, with smaller coral patches on either side but no immediate obstacles blocking a direct approach. To inspect its surface closely for marine life or anomalies, I need to: (1) move forward steadily toward the base of the formation, (2) adjust altitude/position to bring the camera closer to the polyp surfaces without disturbing sediment, and (3) perform a slow circumferential scan to examine multiple facets. Given the lighting from above, approaching from below-front will maximize visibility of texture details.",
      "action": [
        "Go straight toward the central coral formation",
        "Slowly ascend while maintaining forward motion to reach mid-height of the coral",
        "Hover gently 0.5 meters in front of the lower bulbous section",
        "Perform a slow clockwise rotation to inspect the surface"
      ]
    },
    "images": [
      "./data/images/0294.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical green pipe structure while avoiding the surrounding coral formations and the yellow ring-shaped object on the seabed.",
    "output": {
      "reasoning": "The vertical green pipe structure is directly ahead and centrally located in the image, illuminated by overhead light rays. Coral formations are visible on the left and right sides near the seabed, and a yellow ring-shaped object lies on the seabed to the right of the pipe (near the lower-right quadrant). To approach the pipe safely, the robot must maintain a central trajectory, avoiding lateral deviations toward the corals or the yellow ring. Since the pipe is already aligned with the robot’s current heading, the immediate action is to advance straight while monitoring proximity to obstacles. No turning is needed initially; slight course corrections may be required later, but the first step is forward motion along the clear central path.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0017_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged military vehicle behind the central stone pillar and inspect its front right wheel assembly.",
    "output": {
      "reasoning": "The image shows a submerged military vehicle located behind a central stone pillar. The vehicle is partially visible to the right of the pillar from the current viewpoint. To approach the vehicle, I must first navigate around the pillar—likely to its right side, since the vehicle is positioned on that side and appears accessible without backtracking. Once past the pillar, I need to move closer to the vehicle’s front right wheel assembly, which is visible in the image (front-right corner of the vehicle, near the seabed). The path appears clear of major obstacles; minor seabed rocks are present but navigable at low altitude. Therefore, the plan is: (1) turn right to begin circumventing the pillar, (2) proceed forward while maintaining clearance from the pillar, (3) align with the vehicle’s front right side, and (4) move in for close inspection of the wheel assembly.",
      "action": [
        "Turn right",
        "Go straight to pass the right side of the stone pillar",
        "Approach the military vehicle from its front-right side",
        "Move closer to inspect the front right wheel assembly"
      ]
    },
    "images": [
      "./data/images/0106.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Inspect the large submerged pipeline for visible damage or leaks, focusing on the segmented joints and surface corrosion.",
    "output": {
      "reasoning": "The image shows a large submerged pipeline lying horizontally on the seabed, with clearly visible segmented joints (reinforced with yellow bands) and surface texture suggesting possible corrosion. The robot is positioned at one end, looking along the pipeline’s length. To inspect for damage or leaks—especially at joints and corroded surfaces—the robot must move parallel to the pipeline, maintaining close proximity for high-resolution visual inspection. Given the pipeline extends into the distance, a systematic forward motion with slight lateral adjustments will allow full coverage of joints and surface anomalies. No immediate obstacles block the path, so the primary action is to advance steadily while keeping the pipeline centered in view.",
      "action": [
        "Go straight along the pipeline",
        "Maintain close distance to pipeline surface",
        "Pause briefly at each segmented joint for detailed inspection",
        "Scan for discoloration, pitting, or bubbles indicating leaks"
      ]
    },
    "images": [
      "./data/images/0260.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green column and inspect its surface for biological growth or structural damage.",
    "output": {
      "reasoning": "The glowing green column is the central target, positioned slightly left of center in the image and illuminated from above. It stands vertically on the seabed, with visible texture suggesting possible biological growth or damage. The robot is currently at a moderate distance, facing roughly toward the column but not directly aligned. To approach and inspect its surface, I need to: (1) move forward to reduce distance, (2) adjust heading slightly right to center the column in the field of view (since it appears left-of-center), (3) continue approaching until close enough for detailed inspection (within ~1–2 m), and (4) possibly circle or tilt to examine surface features. Given the monocular view and no immediate obstacles, a direct approach with minor lateral correction is safe and efficient.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight",
        "Go straight",
        "Hover and inspect surface"
      ]
    },
    "images": [
      "./data/images/0265.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged shell-like structure on the seabed while avoiding the smaller pink coral to its left.",
    "output": {
      "reasoning": "The image shows a large green ridged shell-like structure on the seabed, positioned centrally-right in the frame. To its left (from the robot's perspective), there is a smaller pink coral with a distinct brain-like texture. The robot is currently at some distance and slightly elevated, facing the scene. To approach the green structure while avoiding the pink coral, the robot should first steer right to clear the coral’s position, then proceed straight toward the green structure. Since the coral is directly to the left of the target, a slight rightward correction before advancing ensures safe clearance.",
      "action": [
        "Turn right slightly",
        "Go straight toward the green ridged shell"
      ]
    },
    "images": [
      "./data/images/0214.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green submarine and inspect its front hull for damage or marine growth.",
    "output": {
      "reasoning": "The green submarine is clearly visible in the center-left of the image, resting on the seabed at a moderate distance. Its front hull (nose section with portholes and grilles) faces roughly toward the camera, slightly angled to the right. To inspect the front hull closely, I need to: (1) approach straight ahead while maintaining alignment with the front; (2) possibly adjust laterally if needed to center the hull in view; (3) stop at a close but safe inspection distance (~1–2 m) to allow visual assessment of damage or marine growth. No immediate obstacles block the direct path. Given the current perspective, a slight rightward correction may help align with the centerline of the front hull before closing in.",
      "action": [
        "Go straight toward the green submarine",
        "Slightly turn right to align with the front hull centerline",
        "Continue forward slowly until ~1.5 meters from the front hull",
        "Hover and stabilize for visual inspection"
      ]
    },
    "images": [
      "./data/images/0304.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central ornate column and inspect its lower perforated section for marine growth or damage.",
    "output": {
      "reasoning": "The image shows an underwater scene with multiple ornate columns; the central column is clearly visible and features a lower perforated section (with circular holes) that is the inspection target. The robot appears to be positioned slightly to the left and at a moderate distance from the central column. To approach it, the robot should first move forward toward the column while maintaining alignment with its center. Once near, it should position itself close to the lower perforated section—ideally at eye level with that region—for detailed visual inspection of marine growth or damage. No immediate obstacles block the direct path, though fish are present but non-obstructive.",
      "action": [
        "Move forward toward the central ornate column",
        "Adjust heading slightly right to center on the column",
        "Continue forward until within 1 meter of the column",
        "Descend gently to align camera with the lower perforated section",
        "Hover and inspect the perforated area for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/0236.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green submarine wreckage from the front, maintaining a safe distance to inspect its open hatch and surrounding debris field.",
    "output": {
      "reasoning": "The green submarine wreckage is directly in front of the robot, oriented horizontally with its open hatch facing forward. The robot appears to be at a moderate distance (~3–4 meters) and slightly below the hull. To approach from the front while maintaining a safe distance (e.g., ~1.5–2 m), I should first align directly with the centerline of the wreck’s bow. The debris field lies on the seabed in front and to the left; I must avoid drifting downward or leftward into it. A slow forward advance with minor pitch-up adjustment will keep the robot at neutral buoyancy and centered on the hatch. No immediate obstacles block the frontal approach path.",
      "action": [
        "Align heading directly toward the submarine's bow (center of open hatch)",
        "Gently ascend 0.3 meters to match mid-hull height",
        "Advance slowly forward for 2 meters",
        "Pause and stabilize at ~1.8 meters from the hatch"
      ]
    },
    "images": [
      "./data/images/0305.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The image shows a tall, submerged stone pillar centered in the frame, with visible marine growth (greenish algae/biofouling) and possible erosion or damage on its surface. The robot is currently at a moderate distance, facing the pillar directly. To inspect the surface closely, the robot must first approach it while maintaining a stable heading, then circle or hover near the pillar to examine different sections (front, sides, top). Given the clear visibility and lack of immediate obstacles, the safest initial action is to move straight toward the base of the pillar, then adjust position for detailed inspection.",
      "action": [
        "Go straight toward the stone pillar",
        "Slow down as you near the base",
        "Circle clockwise around the pillar at close range",
        "Pause briefly at each quadrant to inspect surface details",
        "Tilt camera upward to examine upper sections and cap"
      ]
    },
    "images": [
      "./data/images/0195.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and green dome-shaped structure in the foreground while avoiding the overhanging moss-covered ledge above.",
    "output": {
      "reasoning": "The red and green dome-shaped structure is clearly in the foreground, occupying the lower-left to center of the frame. An overhanging moss-covered ledge looms above it (upper-right), posing a collision risk if the robot ascends or moves upward carelessly. To safely approach the dome, the robot should first maintain a low altitude to stay clear of the ledge, then move forward toward the dome’s base. Since the dome is slightly left-of-center and close, a slight rightward correction may help align with its center while avoiding the ledge’s shadowed underside. No immediate obstacles block the direct forward path at current depth.",
      "action": [
        "Go straight slowly",
        "Slightly turn right",
        "Maintain low altitude to avoid overhead ledge",
        "Continue approaching the red-green dome"
      ]
    },
    "images": [
      "./data/images/0114.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large vertical pillar covered in marine growth and inspect its surface for attached organisms or structural features.",
    "output": {
      "reasoning": "The image shows a large vertical pillar centrally located, covered in marine growth (e.g., barnacles, algae, possibly sponges), with visible structural irregularities and attached organisms. The robot appears to be at a moderate distance, slightly below and in front of the pillar. To inspect its surface closely, the robot must first approach it directly while maintaining a safe distance to avoid collision. Given the pillar’s vertical orientation and the presence of nearby coral structures on the left and right, a straight-forward approach is safest. Once near, a slow lateral movement (e.g., slight left or right) may be needed for full surface coverage—but the initial macro-instruction only requires *approaching* and *inspecting*, so the first step is to close the distance. Since the pillar dominates the center of the frame, no major reorientation is needed—just forward motion toward it.",
      "action": [
        "Go straight toward the central pillar",
        "Slow down as you near the pillar (within ~1–2 m)",
        "Maintain stable depth and position to begin visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0029_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red buoy on the seabed and inspect its top valve for signs of leakage or damage.",
    "output": {
      "reasoning": "The red buoy is clearly visible on the seabed in the foreground, slightly to the right of center. It is upright and stable, with its top valve (a cylindrical red protrusion) facing upward and accessible. The robot appears to be positioned a short distance away (~1–2 m), with clear water and minimal obstacles between it and the buoy. To inspect the top valve, the robot must first approach directly toward the buoy, then position itself vertically above or slightly in front of the valve for close visual inspection. No red obstacles are present in the immediate path—only rocks to the left and right, which can be avoided by maintaining a central trajectory.",
      "action": [
        "Go straight toward the red buoy",
        "Slow down as you near the buoy",
        "Hover 0.5 meters in front of the buoy's top valve",
        "Tilt camera downward slightly to focus on the valve",
        "Inspect the valve for cracks, corrosion, or fluid traces"
      ]
    },
    "images": [
      "./data/images/0203.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green giant clam in the center while maintaining a safe distance to avoid disturbing it, and observe its mantle for signs of activity.",
    "output": {
      "reasoning": "The large green giant clam is clearly centered in the image, resting on the seabed with its mantle partially exposed. It occupies the mid-to-near field, and there are no immediate obstacles between the robot and the clam. To approach safely without disturbing it, I should first move forward slowly toward it while maintaining a lateral offset (e.g., slightly to the left or right) to avoid direct frontal approach, which may trigger a defensive closure. Given its size and position, a gentle forward motion followed by a slight lateral adjustment will allow observation of the mantle from a safe distance (~1–1.5 m). The fish nearby suggest the environment is active but not threatening; thus, slow, steady movement is appropriate. No turning is needed initially since the clam is directly ahead.",
      "action": [
        "Go straight slowly",
        "Slightly adjust left to maintain safe lateral distance",
        "Pause and observe the mantle for movement"
      ]
    },
    "images": [
      "./data/images/0198.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Inspect the green segmented pipeline for structural integrity along its visible length.",
    "output": {
      "reasoning": "The green segmented pipeline is prominently visible, extending diagonally from the lower-left toward the upper-right of the frame. It lies on a flat seabed surface, with clear visibility along its length under directional lighting. To inspect its structural integrity along the visible length, the robot must approach parallel to the pipeline and maintain a consistent distance for high-resolution visual assessment. Given its orientation, the optimal path is to move forward while slightly adjusting left or right to stay aligned with the pipeline’s axis. Since the pipeline appears straight and unobstructed in the visible segment, a continuous forward motion with minor lateral correction will suffice for full inspection of the visible portion.",
      "action": [
        "Move forward slowly while keeping the green pipeline centered in the field of view",
        "Slightly adjust left to maintain parallel alignment with the pipeline",
        "Continue forward until the end of the visible pipeline segment is reached"
      ]
    },
    "images": [
      "./data/images/0259.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and green segmented pipe lying on the seabed, focusing on the glowing yellow end cap for inspection.",
    "output": {
      "reasoning": "The image shows a segmented pipe on the seabed, colored red and green, with a glowing yellow end cap on the left side (near the robot's current position). The robot is positioned slightly above and to the right of the pipe, looking down its length. The glowing yellow end cap is clearly visible and is the inspection target per the instruction. To approach it safely and precisely, the robot should first move forward toward the pipe while maintaining orientation, then adjust laterally if needed to center on the yellow cap. Since the cap is already in view and relatively close, the primary action is to advance straight toward it, possibly with a slight leftward correction to align directly.",
      "action": [
        "Go straight toward the pipe",
        "Slightly turn left to center on the glowing yellow end cap",
        "Approach closely for inspection"
      ]
    },
    "images": [
      "./data/images/0212.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submerged vehicle and inspect its right-side windows for damage or marine growth.",
    "output": {
      "reasoning": "The green submerged vehicle is clearly visible in the center-left of the image, oriented diagonally with its front pointing toward the lower-right. Its right side (from the vehicle’s perspective) faces the camera’s current position, but the robot appears to be slightly ahead and to the left of the vehicle’s midsection. To inspect the right-side windows, the robot must first approach closer along a path that maintains visibility of that side—ideally moving parallel to the vehicle’s longitudinal axis from front to rear or vice versa. Given the current angle, the optimal first step is to move forward slightly to align better with the mid-section, then shift right to bring the right-side windows into clear view without occlusion. The orange vertical structures above suggest mooring or reference points; they should be avoided but are not immediate obstacles. No visible damage or marine growth can be assessed yet—close inspection requires proximity (~1–2 m).",
      "action": [
        "Go straight slowly for 2 meters",
        "Drift right 1 meter while maintaining depth",
        "Hover and stabilize facing the right-side windows",
        "Perform slow forward scan along the right-side window row"
      ]
    },
    "images": [
      "./data/images/temp_307_0009_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow and blue submersible vehicle hovering near the orange sea fans and inspect its underside for debris or damage.",
    "output": {
      "reasoning": "The yellow and blue submersible is visible in the mid-left of the image, hovering above the seabed near prominent orange sea fans. It is oriented diagonally, with its underside partially visible but not fully accessible from the current viewpoint. To inspect its underside for debris or damage, I need to: (1) approach it carefully to avoid disturbing sediment or colliding with sea fans; (2) position the robot directly beneath or slightly below and behind the vehicle to get a clear view of the underside; (3) maintain stable hover for inspection. Given its location relative to the sea fans (to their left and slightly forward), the safest approach path is from the front-right quadrant—moving straight toward it while slightly adjusting left to clear the nearest sea fan cluster, then descending gently once close.",
      "action": [
        "Go straight toward the yellow and blue submersible",
        "Slightly turn left to avoid the nearest orange sea fan cluster",
        "Continue approaching until ~1 meter away",
        "Gently descend to position below the submersible",
        "Hover and inspect the underside for debris or damage"
      ]
    },
    "images": [
      "./data/images/0229.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular coral structure in the center-left and inspect its top surface for signs of marine growth or damage.",
    "output": {
      "reasoning": "The image shows a large circular coral structure located in the center-left of the scene, elevated above the seabed and clearly visible. It has a distinct flat top surface with radial patterns, ideal for inspection. The robot is currently positioned at a moderate distance and slightly below the structure (as light rays come from above and the structure looms overhead). To approach and inspect the top surface, the robot should: (1) move forward toward the structure while maintaining a safe vertical clearance; (2) ascend slightly to align with the top plane; (3) position directly above or just in front of the top surface for optimal visual inspection; (4) hover/stabilize to allow detailed observation of marine growth or damage. No immediate obstacles block the path, though smaller corals are nearby—so lateral precision is needed.",
      "action": [
        "Move forward toward the large circular coral in the center-left",
        "Ascend slightly to align with the top surface height",
        "Position directly above the coral’s top surface",
        "Hover and stabilize for visual inspection"
      ]
    },
    "images": [
      "./data/images/0131.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted orange cylindrical object in the foreground and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The rusted orange cylindrical object is clearly visible in the foreground on the right side of the image. It is upright and positioned closer to the camera than the larger greenish structure behind it. To approach and inspect its surface, the robot should first move forward toward it while maintaining a safe distance, then circle or reposition to get a close-up view of its surface (especially around the holes and bands) for signs of marine growth (e.g., barnacles, algae) or corrosion/damage. Given its proximity and orientation, a direct forward movement followed by a slight leftward arc will allow optimal inspection without collision.",
      "action": [
        "Go straight toward the orange cylindrical object",
        "Move slightly left to align with its midsection",
        "Hover closely and rotate slowly to inspect surface details"
      ]
    },
    "images": [
      "./data/images/0220.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine and inspect its hull near the visible chain and rope attachments.",
    "output": {
      "reasoning": "The submarine is positioned horizontally in the mid-to-upper portion of the image, slightly angled with its bow pointing left. A prominent chain and rope assembly is visible near the submarine’s midsection, hanging vertically from above and draping toward the foreground. The robot appears to be at a distance and slightly below/forward of the submarine. To inspect the hull near the chain and rope attachments, I need to: (1) move forward toward the submarine while maintaining depth to avoid collision; (2) adjust lateral position to align with the chain/rope area (likely near the center of the hull); (3) close in carefully for detailed inspection without disturbing the ropes or sediment. Given the lighting from above, approaching from below may improve visibility of the hull surface. The foreground ropes are not obstructive but should be avoided during approach.",
      "action": [
        "Go straight slowly toward the submarine",
        "Slightly ascend to match the submarine's depth",
        "Drift right to align with the chain and rope attachment point",
        "Move forward gently to inspect the hull near the chains"
      ]
    },
    "images": [
      "./data/images/temp_307_0035_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater vehicle positioned near the coral formation and inspect its side-mounted sensor array.",
    "output": {
      "reasoning": "The cylindrical underwater vehicle is visible to the right of the coral formation, oriented horizontally with its side-mounted sensor array facing roughly toward the camera. The robot appears to be positioned slightly left and forward of the vehicle, with clear water between them. To approach and inspect the side-mounted sensor array, the robot should first move laterally right to align with the vehicle’s midsection, then advance straight while maintaining a safe distance to avoid disturbing the coral or the vehicle. A slight downward adjustment may be needed to match the vehicle’s depth, as it appears slightly below the current viewpoint. Finally, a minor yaw adjustment (e.g., slight left turn) will orient the camera directly toward the side sensor array for inspection.",
      "action": [
        "Move right",
        "Go straight",
        "Descend slightly",
        "Turn left slightly"
      ]
    },
    "images": [
      "./data/images/temp_307_0057_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical pipe on the left while avoiding the rising bubbles and staying centered in the tunnel.",
    "output": {
      "reasoning": "The large green cylindrical pipe is clearly visible on the left side of the tunnel. The robot is currently centered in the tunnel, with rising bubbles concentrated near the top-left and above the pipe—likely from equipment or vents near the pipe. To approach the pipe while avoiding bubbles and staying centered, I should: (1) maintain centerline alignment initially to avoid drifting into bubble zones; (2) gradually steer left *after* passing the densest bubble cluster (which appears just ahead and above the pipe); (3) keep forward motion steady to minimize turbulence-induced drift. The pipe’s position suggests a gentle left turn is needed once clear of bubbles, but not too early to avoid collision or entering bubble-rich water.",
      "action": [
        "Go straight while maintaining centerline",
        "Slightly turn left after passing the main bubble cluster",
        "Continue forward toward the green pipe"
      ]
    },
    "images": [
      "./data/images/0119.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue car and inspect its front grille and headlights for damage or marine growth.",
    "output": {
      "reasoning": "The blue car is visible in the center-right of the image, partially buried in sediment, with its front grille and headlights facing the camera. The robot appears to be positioned slightly left and forward of the car, at a moderate distance (~2–3 m). To inspect the front grille and headlights closely, the robot must first approach straight toward the car’s front, then possibly adjust laterally to center on the grille area. No immediate obstacles block the direct path, though marine growth (e.g., fan-like organisms) is present nearby—caution needed to avoid contact. Since the goal is inspection, final positioning should allow clear, stable view of the front fascia (grille + headlights), ideally within 1 m.",
      "action": [
        "Go straight toward the blue car",
        "Slow down as you near the front bumper",
        "Adjust slightly right to center on the grille",
        "Hover steadily 0.8 meters in front of the grille for inspection"
      ]
    },
    "images": [
      "./data/images/0103.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central red-and-teal subsea tooling assembly and inspect the orange square housing for visible damage or debris.",
    "output": {
      "reasoning": "The image shows a clear frontal view of the central red-and-teal subsea tooling assembly, with the orange square housing prominently visible at the lower front. The robot is already aligned with the assembly and appears to be at moderate distance—close enough to identify major features but not yet at inspection range. To inspect the orange square housing for damage or debris, the robot must approach closer while maintaining stable orientation. No immediate obstacles block the direct path forward. Therefore, the next logical step is to advance straight toward the assembly until the orange housing fills more of the frame (e.g., ~1–2 m away), enabling high-resolution visual inspection.",
      "action": [
        "Go straight toward the central assembly"
      ]
    },
    "images": [
      "./data/images/0208.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the purple sponge-like structure on the seabed while avoiding the coral formation to its right.",
    "output": {
      "reasoning": "The purple sponge-like structure is visible on the seabed in the lower-left portion of the image. A coral formation is present to its right (image right), which must be avoided. The robot is currently positioned above and slightly behind the sponge, facing roughly toward it. To approach safely while avoiding the coral, the robot should first move forward slightly to align with the sponge’s left side, then gently steer left to maintain clearance from the coral, and finally descend or move closer to reach the sponge without contact.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight toward the purple sponge"
      ]
    },
    "images": [
      "./data/images/0093.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged concrete pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The image shows a tall, submerged concrete pillar centered in the frame, illuminated from above with visible marine growth and surface irregularities. The robot appears to be positioned at a moderate distance directly in front of the pillar, with clear visibility and no immediate obstacles. To inspect the surface thoroughly, the robot should first close the distance to get within optimal inspection range (e.g., 1–2 meters), then perform a slow circumferential movement around the pillar to examine all sides, while maintaining stable altitude and orientation for high-resolution imaging.",
      "action": [
        "Go straight toward the pillar",
        "Slow down and stop at ~1.5 m distance",
        "Circle the pillar clockwise at constant depth",
        "Maintain steady forward motion while tilting camera slightly downward to scan surface details",
        "Pause briefly at each quadrant to capture high-res imagery"
      ]
    },
    "images": [
      "./data/images/0239.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged pipeline and inspect its surface for corrosion or damage along the visible section.",
    "output": {
      "reasoning": "The image shows a large submerged pipeline extending diagonally from the lower-left toward the upper-right, with visible structural supports and surface texture. The robot appears to be positioned slightly in front and to the left of the pipeline’s near end (marked by a blue-and-yellow cap). To approach and inspect the visible section, the robot should first move forward to get closer to the pipeline, then align parallel to it (likely by turning right), and finally proceed alongside it while maintaining a consistent standoff distance for visual inspection. Given the pipeline’s orientation and the robot’s current position, the initial action is to advance straight toward the pipeline’s near end, then adjust heading to run parallel along its length.",
      "action": [
        "Go straight toward the pipeline",
        "Turn right to align parallel with the pipeline",
        "Move forward slowly alongside the pipeline while inspecting its surface"
      ]
    },
    "images": [
      "./data/images/0139.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged turquoise car and inspect its front bumper for damage or marine growth.",
    "output": {
      "reasoning": "The turquoise car is visible in the center-right of the image, resting on the seabed among coral and rocks. Its front bumper faces roughly toward the camera but is slightly angled to the left. The robot appears to be positioned behind and slightly above the car (based on perspective and lighting). To approach the front bumper for inspection, the robot should first move forward toward the car while maintaining a safe distance from surrounding coral. As it gets closer, it should adjust laterally (slightly right) to align directly with the front bumper, then descend gently to get a close-up view. No immediate obstacles block the direct path, though care must be taken near the coral on the left.",
      "action": [
        "Go straight toward the turquoise car",
        "Slightly turn right to align with the front bumper",
        "Descend gently to get closer to the front bumper",
        "Hover and inspect the front bumper for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/0224.png"
    ]
  }
]