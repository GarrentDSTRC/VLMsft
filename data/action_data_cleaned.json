[
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方缓慢前进，避开正前方的大型珊瑚礁，注意观察近处绿色珊瑚的动态变化。",
    "output": {
      "reasoning": "图像正前方占据视野的是一个大型深色珊瑚礁（障碍物），直接前进会发生碰撞。左前方区域光线较好且路径相对开阔。近处（左下及底部）有明显的绿色波纹状珊瑚。为了执行指令，机器人需要向左侧转向以绕过中心障碍物，并保持低速以确保对近处绿色珊瑚的细致观察。",
      "action": [
        "Move forward slowly",
        "Turn left to bypass the central large reef",
        "Maintain low speed and observe nearby green coral",
        "Continue towards the front-left open area",
        "Keep a safe distance from the reef"
      ]
    },
    "images": [
      "data/images/0001.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左绕过正前方的大型柱状结构，保持匀速，注意观察左侧珊瑚群的缝隙。",
    "output": {
      "reasoning": "图像正前方占据视野中心的是一个巨大的柱状生物结构（疑似大型海绵或珊瑚），构成了直接障碍。左侧区域有紫色和绿色的珊瑚群，且存在明显的缝隙。为了执行指令，机器人需要先向左转向以避开中心柱状体，随后在保持匀速前进的过程中，将视觉焦点偏向左侧珊瑚群进行观察。",
      "action": [
        "Turn left",
        "Go straight",
        "Maintain constant speed",
        "Observe gaps in the left coral reef",
        "Bypass the central pillar structure",
        "Stop"
      ]
    },
    "images": [
      "data/images/0002.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，绕过正前方的绿色基座并向右侧的黑色柱状区域探索，注意避开左侧的珊瑚丛。",
    "output": {
      "reasoning": "图像中心是一个大型绿色机械基座，左侧分布着密集的尖刺状珊瑚丛，右侧背景可见黑色的柱状分支结构。根据指令，机器人需要先匀速前进，通过向右侧偏转来同时避开左侧珊瑚并绕过正前方的绿色基座障碍，最后对准右侧目标区域进行探索。",
      "action": [
        "Go straight at constant speed",
        "Turn right to avoid left coral bushes",
        "Bypass the green base from the right side",
        "Approach the black columnar area on the right",
        "Explore the target area",
        "Stop"
      ]
    },
    "images": [
      "data/images/0003.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过面前这组柱状物，注意避开左侧的绿色结构并留意上方光影的变化。",
    "output": {
      "reasoning": "图像左侧近处有一个明显的绿色复杂结构，正前方分布着若干深色柱状物。为了安全穿过柱状物并避开左侧结构，机器人需要向右前方微调航向，进入柱状物之间的缝隙，同时在移动过程中保持恒定速度并监测上方光影的亮度变化。",
      "action": [
        "Maintain constant speed",
        "Slightly turn right to avoid the green structure",
        "Go straight towards the gap between columns",
        "Monitor light and shadow changes from above",
        "Pass through the columnar objects",
        "Keep moving forward"
      ]
    },
    "images": [
      "data/images/0004.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，抵近观察正前方的石柱，注意避开底部的碎石堆。",
    "output": {
      "reasoning": "图像显示目标石柱位于正前方，其底部周围散落有明显的碎石堆。为了执行指令，机器人需要保持直线匀速前进，在接近过程中通过视觉反馈微调路径以避开底部的障碍物，最后在石柱前的安全距离停下进行观察。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Navigate to avoid bottom rubble",
        "Approach stone pillar",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0005.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过左侧的沉车并抵近观察前方的石柱，注意避开底部的散落障碍物。",
    "output": {
      "reasoning": "图像显示左侧有一辆绿色的沉没汽车，前方中景处有一根石柱（目标），底部散落着多块方形石块（障碍物）。任务要求绕过左侧车辆并避开底部障碍物抵近石柱。规划路径为：保持匀速前进，向右侧微调以避开沉车，同时通过高度或路径调整避开底部的散落物，最终靠近石柱。",
      "action": [
        "Move forward at constant speed",
        "Steer right to bypass the car on the left",
        "Navigate to avoid scattered obstacles on the seabed",
        "Approach the stone pillar",
        "Stop and observe target"
      ]
    },
    "images": [
      "data/images/0006.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过左侧大型珊瑚与中心石柱之间的开阔水域，保持匀速并注意观察底部礁石的分布。",
    "output": {
      "reasoning": "图像左侧有一个明显的大型扁平珊瑚，中心位置有一个垂直的石柱状珊瑚结构，两者之间存在一段清晰的开阔水域。底部散布着零散的小型礁石。机器人需要对准该间隙，保持稳定速度穿过，同时向下监测底部地形以防碰撞。",
      "action": [
        "Align with the gap between the left coral and the center pillar",
        "Move forward at constant speed",
        "Monitor bottom reef distribution",
        "Pass through the open water",
        "Maintain steady course"
      ]
    },
    "images": [
      "data/images/0007.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，穿过前方的岩石拱门向深处探索，注意避开底部的礁石并观察光影下的环境变化。",
    "output": {
      "reasoning": "图像显示前方有一个由岩石构成的天然拱门，阳光从左上方斜射入水下，形成明显的光影对比。路径底部散布着不规则的礁石。为了安全穿过拱门并向深处探索，机器人需要首先调整高度以避开底部的礁石障碍，随后保持稳定速度直线穿过拱门开口，并持续监测光影变化下的环境细节。",
      "action": [
        "Maintain constant speed",
        "Ascend slightly to clear bottom reefs",
        "Go straight toward the archway",
        "Pass through the rock arch",
        "Move forward into the deep area",
        "Observe environmental changes",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0008.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近绿色舱体，注意避开左侧的橙色管道，保持匀速并重点观察舱窗内部情况。",
    "output": {
      "reasoning": "图像显示绿色舱体位于前方，其左侧有两根显著的橙色竖直管道。为了安全靠近并观察，机器人需要先微调航向向右以确保与左侧管道保持安全距离，随后以低速匀速向前推进，最终悬停在舱窗附近进行细节观察。",
      "action": [
        "Adjust heading slightly right",
        "Move forward slowly",
        "Maintain constant speed",
        "Keep distance from left orange pipes",
        "Approach capsule windows",
        "Hover and observe interior",
        "Stop"
      ]
    },
    "images": [
      "data/images/0009.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "",
    "output": {
      "reasoning": "图像中心偏左位置有一个显著的红色圆柱形障碍物。为了执行‘避开并向右移动’的指令，机器人需要先向右转动以调整航向，避开障碍物的正面，然后直线行驶进入右侧的开阔水域，同时保持与障碍物的安全距离。",
      "action": [
        "Turn right",
        "Go straight",
        "Maintain distance from red obstacle",
        "Stop"
      ]
    },
    "images": [
      "data/images/0010.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "",
    "output": {
      "reasoning": "视觉感知显示，图像右侧中景处存在一个巨大的、具有褶皱纹理的深色岩石结构。当前视野中心被一个大型潜航器占据，目标岩石位于其右后方。为了安全接近，机器人需要先向右侧偏转航向以避开中心物体并对准岩石，随后向前推进，在接近目标时减速并停止。",
      "action": [
        "Turn right",
        "Go straight",
        "Approach the rock",
        "Slow down",
        "Stop"
      ]
    },
    "images": [
      "data/images/0011.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右前方绕过大型圆柱体目标，注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "图像显示前方有一个大型圆柱体目标（疑似潜航器或管道），其占据了视野中心并向右延伸。底部和左前方分布着密集的珊瑚礁。根据指令，需要向右前方绕行，这意味着机器人需要先向右转向以避开圆柱体的主体，同时在移动过程中保持高度，防止碰撞底部的珊瑚礁，并维持稳定的推进速度。",
      "action": [
        "Maintain constant speed",
        "Turn right",
        "Go straight to bypass the cylindrical target",
        "Maintain depth to avoid bottom coral reefs",
        "Adjust heading to front-right",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0012.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，穿过两侧的珊瑚礁缝隙向正前方的光源处前进，注意避开中间的垂直线缆并时刻关注近处的环境变化。",
    "output": {
      "reasoning": "图像显示机器人正处于由两侧珊瑚礁构成的狭窄通道入口，正前方远处有明亮光源。路径正中央存在一条明显的垂直线缆。为了安全穿过，机器人需要保持稳定速度，在进入缝隙后向左或向右微调航向以避开中心线缆，同时持续监测与两侧岩壁的距离，最终抵达光源处。",
      "action": [
        "Maintain constant speed",
        "Go straight into the gap",
        "Slightly steer to bypass the central cable",
        "Continue straight toward the light source",
        "Monitor nearby environment for changes",
        "Reach target and stop"
      ]
    },
    "images": [
      "data/images/0013.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前方光亮处的狭窄通道推进，注意避开右侧的线缆结构并时刻观察两侧岩石的距离。",
    "output": {
      "reasoning": "图像显示前方中心区域有明显的光束投射，形成一个通往深处的狭窄通道。右侧可见一串垂直分布的线缆或浮标状结构，左侧为陡峭的岩石壁。任务要求在保持匀速的同时，精准穿越该通道并避开右侧障碍物。因此，规划逻辑为：首先对准光亮中心，设定恒定推进速度，在前进过程中向左微调以远离线缆，并持续监测两侧环境特征点的距离。",
      "action": [
        "Align with the bright passage ahead",
        "Set constant forward speed",
        "Go straight towards the light",
        "Slightly steer left to maintain distance from the right cable",
        "Monitor clearance of rocks on both sides",
        "Maintain steady propulsion through the narrow gap"
      ]
    },
    "images": [
      "data/images/0014.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近中心的发光装置，注意避开右侧的黄色网状障碍物。",
    "output": {
      "reasoning": "图像中心可见一个明亮的发光装置（目标），右侧和下方存在大面积的黄色网状结构（障碍物）。路径规划需要机器人在保持匀速前进的同时，稍微向左偏移以确保与右侧障碍物保持安全距离，最终精准靠近中心目标。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Slightly steer left to avoid the yellow mesh on the right",
        "Approach the central glowing device",
        "Slow down",
        "Stop"
      ]
    },
    "images": [
      "data/images/0015.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿蓝色条纹管道向上方水面缓慢抵近，注意避开左侧的潜水员和周围的网状缆绳。",
    "output": {
      "reasoning": "图像显示蓝色条纹管道位于视野中心并向上延伸至水面。左侧中景有一名潜水员，左侧及下方近景分布有密集的网状缆绳。为了执行指令，机器人需要向右侧微调以确保与左侧潜水员和缆绳保持安全距离，随后锁定管道为参照物，匀速缓慢上升，直至接近水面。",
      "action": [
        "Turn slightly right to clear the diver and cables",
        "Align with the blue-striped pipe",
        "Ascend slowly at a constant speed",
        "Maintain safe distance from the mesh cables",
        "Approach the water surface",
        "Stop"
      ]
    },
    "images": [
      "data/images/0016.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢移动，靠近那个黄色圆圈标记的底座目标，注意避开右侧的珊瑚礁，保持匀速前进。",
    "output": {
      "reasoning": "视觉分析显示，黄色圆圈标记的目标底座位于当前视角的右前方。右侧有一块巨大的多孔状岩石/珊瑚礁，构成了路径上的主要障碍。为了执行指令，机器人需要先向右微调航向，随后以恒定低速向前推进，在移动过程中需实时监测右侧距离以避开珊瑚礁，最终精准靠近目标底座。",
      "action": [
        "Turn right slightly",
        "Move forward slowly at constant speed",
        "Keep distance from the right coral reef",
        "Approach the yellow circle target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0017.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过两侧的珊瑚礁群，注意观察上方光影区域的动态变化。",
    "output": {
      "reasoning": "图像显示机器人当前位于两侧珊瑚礁形成的自然通道中心，前方路径开阔无障碍。上方有强烈的阳光射入，形成明显的光影区域。根据指令，机器人需要维持当前航向匀速前进，同时将视觉感知重点分配给上方区域以监测光影动态。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Monitor upper light and shadow area",
        "Pass through the coral reef corridor",
        "Keep observing dynamic changes"
      ]
    },
    "images": [
      "data/images/0018.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右侧绕过正前方的雕花石柱，保持匀速向深处的遗迹群探索，注意观察周围障碍物的分布。",
    "output": {
      "reasoning": "图像正前方有一根巨大的雕花石柱作为主要障碍物，遮挡了通往深处遗迹的路径。背景中可见散落在沙地上的小型金字塔状和圆顶状遗迹构件。根据指令，机器人需要先向右转向以避开石柱，随后切回向前的航向深入探索，同时需持续扫描周围散布的小型障碍物以确保安全。",
      "action": [
        "Turn right",
        "Go straight",
        "Turn left slightly to realign",
        "Go straight at constant speed",
        "Scan surroundings for obstacles",
        "Approach the distant ruins"
      ]
    },
    "images": [
      "data/images/0019.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向前靠近那辆生锈的汽车进行观察，保持匀速并注意避开地面的圆形障碍物。",
    "output": {
      "reasoning": "目标生锈汽车位于机器人前方偏右位置。地面散布着多个深色圆形障碍物，直接直线行驶可能会发生碰撞。需要采取小幅度转向策略绕过障碍物，并按照指令要求以恒定低速靠近汽车侧面进行观察。",
      "action": [
        "Move forward slowly",
        "Steer slightly right to avoid circular obstacles",
        "Maintain constant speed",
        "Approach the side of the rusty car",
        "Stop and hover for observation"
      ]
    },
    "images": [
      "data/images/0020.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过左侧珊瑚礁与右侧岩壁之间的通道，注意观察近处阴影中的障碍物。",
    "output": {
      "reasoning": "图像显示左侧为隆起的珊瑚礁群，右侧为巨大的深色岩壁，两者之间形成了一个明显的航行通道。上方光线充足，但通道底部和岩壁根部存在深色阴影区，可能隐藏礁石。机器人需要对准通道中心，保持匀速前进，并实时监测阴影区域的深度变化以避障。",
      "action": [
        "Go straight at constant speed",
        "Align with the center of the channel",
        "Monitor shadows for potential obstacles",
        "Maintain steady forward movement",
        "Pass through the gap between coral and rock",
        "Stop"
      ]
    },
    "images": [
      "data/images/0021.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前穿过面前的岩石通道，向远处明亮的出口探索，注意保持匀速并留意两侧的障碍物。",
    "output": {
      "reasoning": "图像显示机器人正处于一个由深色岩石构成的水下狭窄通道入口，正前方远处有一个明亮的青蓝色出口。路径两侧是高耸且不规则的岩壁，地面分布有零星的小型岩石。为了安全穿过，机器人需要对准出口中心，保持匀速直线行驶，并实时监测与两侧岩壁的距离以防碰撞。",
      "action": [
        "Align with the center of the passage",
        "Go straight at constant speed",
        "Monitor obstacles on both sides",
        "Move towards the bright exit",
        "Maintain steady pace",
        "Reach the open area"
      ]
    },
    "images": [
      "data/images/0022.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向右前方移动以避开正前方的红色浮标，保持匀速并注意观察右侧远处的障碍物位置。",
    "output": {
      "reasoning": "图像显示正前方有一个巨大的红色条纹浮标，直接阻挡了直线路径。右前方区域相对开阔，但在右侧背景中可以观察到另一个较小的条纹障碍物。为了避开近处目标并遵循指令，机器人需要先向右转向，随后以缓慢且匀速的状态向右前方斜向推进，同时视觉传感器需持续锁定并监测右侧远端障碍物的相对位置，确保航道安全。",
      "action": [
        "Turn right",
        "Move forward slowly",
        "Maintain constant speed",
        "Keep right-front heading",
        "Monitor distant obstacle on the right",
        "Continue moving forward"
      ]
    },
    "images": [
      "data/images/0023.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方红黑相间的吊篮靠近进行近距离观察，注意保持匀速并留意上方线缆的张力变化。",
    "output": {
      "reasoning": "目标红黑相间的吊篮位于视野正前方，路径相对开阔。根据指令，需要以匀速靠近目标，同时视觉系统需重点监测吊篮上方的线缆连接处，确保在接近过程中不触碰线缆并观察其张力状态。最后在适宜观察的近距离处平稳停止。",
      "action": [
        "Align with the red and black basket",
        "Move forward at a steady speed",
        "Monitor the tension of the cables above",
        "Slow down as approaching the target",
        "Stop at a safe distance for close observation"
      ]
    },
    "images": [
      "data/images/0024.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方发光的绿色石柱靠近，注意观察左侧岩石结构的间隙。",
    "output": {
      "reasoning": "图像中心偏右位置有一个明显的发光绿色石柱，这是主要目标。左侧是一个巨大的岩石结构，其间隙需要被监测。路径前方视线良好，无直接障碍物。机器人需要设定恒定推进速度，在向石柱移动的过程中，将视觉传感器或注意力分配给左侧的岩石缝隙。",
      "action": [
        "Set constant speed",
        "Adjust heading towards the green pillar",
        "Go straight",
        "Monitor gaps in the left rock structure",
        "Approach the green pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0025.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢航行，靠近中间那根带波纹的石柱进行抵近观察，注意避开左侧的大型珊瑚礁并保持航向稳定。",
    "output": {
      "reasoning": "图像中心有一根明显的带波纹长方体石柱（目标），左侧前景分布着大型珊瑚礁（障碍物）。为了安全抵达目标，机器人需要缓慢起航，在前进过程中保持航向稳定并略微向右偏移以确保与左侧珊瑚礁保持安全距离，最后减速靠近石柱进行观察。",
      "action": [
        "Move forward slowly",
        "Adjust heading slightly to the right to avoid the left coral reef",
        "Maintain stable heading towards the central pillar",
        "Approach the corrugated stone pillar",
        "Decelerate for close-up observation",
        "Stop and hover"
      ]
    },
    "images": [
      "data/images/0026.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方的黄色潜水器方向缓慢推进，注意避开右侧的礁石，保持平稳航行。",
    "output": {
      "reasoning": "图像显示目标黄色潜水器位于中景的右前方，而右侧区域分布有黑色的礁石障碍物。为了安全执行指令，机器人需要缓慢启动并向前推进，同时微调航向指向潜水器，并始终保持在右侧礁石的左方安全距离外，以确保航行平稳且避开碰撞。",
      "action": [
        "Start slow forward movement",
        "Adjust heading towards the yellow submersible",
        "Keep left to avoid the rocks on the right",
        "Maintain steady speed and stability",
        "Approach the yellow submersible",
        "Stop"
      ]
    },
    "images": [
      "data/images/0027.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从绿色浮标和右侧巨大珊瑚礁之间穿过，向远处的深色结构体缓慢靠近，注意观察近处海床的小型障碍物。",
    "output": {
      "reasoning": "图像分析显示，近处海床分布有数个小型黑色岩石障碍物，中景左侧为一个绿色浮标状物体，右侧为巨大的孔洞状珊瑚礁，两者之间存在明显的通道。远景处可见深色的结构体阴影。规划路径需首先微调方向避开近处的小型障碍物，随后保持匀速穿过浮标与珊瑚礁之间的间隙，最后向深色目标点推进。",
      "action": [
        "Maintain constant speed",
        "Steer to avoid small obstacles on the seabed",
        "Navigate through the gap between the green buoy and the right coral reef",
        "Go straight towards the distant dark structure",
        "Slowly approach the target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0028.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右侧绕过正前方的巨大立柱，前往远处那个阴影中的柱子，注意保持匀速并观察周围环境。",
    "output": {
      "reasoning": "图像中心有一根巨大的立柱阻挡了直接通往目标的路径，目标（另一根柱子）位于右侧远处的阴影中。根据指令，机器人需要先向右转向以避开正前方的障碍物，随后在绕行过程中保持匀速并进行环境扫描，最后对准远处的阴影柱子并接近。",
      "action": [
        "Turn right",
        "Go straight at constant speed",
        "Scan surroundings",
        "Turn left slightly to align with the distant pillar",
        "Go straight towards the target pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0029.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持当前航向，匀速向正前方的高大立柱靠近，注意观察底部珊瑚的分布情况。",
    "output": {
      "reasoning": "视觉分析显示，正前方有一个高大的深色立柱结构，路径上无明显障碍物。底部散布着若干黄色珊瑚礁。根据指令，机器人需要维持当前航向直接前进，并保持匀速以确保视觉传感器能清晰记录底部珊瑚的分布细节。最后在接近立柱基座时减速停止。",
      "action": [
        "Maintain current heading",
        "Go straight at constant speed",
        "Scan seabed for coral distribution",
        "Approach the pillar base",
        "Stop"
      ]
    },
    "images": [
      "data/images/0030.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方航行，绕过正前方巨大的球形珊瑚，同时保持匀速并注意观察左侧珊瑚礁的动态。",
    "output": {
      "reasoning": "图像正前方是一个巨大的球形脑珊瑚，占据了主要航道，构成障碍物。左前方有较开阔的沙地路径，且左侧分布有层叠状的珊瑚礁。根据指令，机器人需要向左微调航向以避开中心球体，同时在前进过程中保持恒定动力输出，并持续监测左侧珊瑚礁区域的视觉变化。",
      "action": [
        "Turn slightly left",
        "Maintain constant speed",
        "Go straight towards the front-left gap",
        "Monitor the left coral reef",
        "Bypass the spherical coral",
        "Stop"
      ]
    },
    "images": [
      "data/images/0031.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方绕过那个巨大的脑纹状珊瑚结构，保持匀速前进，同时注意观察后方大型珊瑚礁的间隙。",
    "output": {
      "reasoning": "图像中心是一个巨大的脑纹状珊瑚结构，其右侧和后方分布着更大型的暗色珊瑚礁。为了执行指令，机器人需要首先向右转向以避开正前方的脑纹珊瑚，随后切入右前方航道并保持匀速推进。在移动过程中，视觉系统需重点扫描右侧及后方大型珊瑚礁之间的缝隙，以确保航行安全并完成观察任务。",
      "action": [
        "Turn right",
        "Move forward at constant speed",
        "Bypass the brain-patterned coral structure",
        "Scan gaps in the large coral reef on the right",
        "Maintain steady heading and speed"
      ]
    },
    "images": [
      "data/images/0032.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，对准前方的网格状目标进行抵近观察，注意避开左侧的绿色结构并留意环境光线变化。",
    "output": {
      "reasoning": "图像左侧近处有一个巨大的绿色机械结构（障碍物），目标网格状物体位于前方中心偏右位置。上方有强烈的环境光束射入，可能导致局部过曝。规划路径需要先向右微调以避开左侧结构，然后保持匀速直线对准网格目标抵近，同时需实时监测光影变化对视觉定位的影响。",
      "action": [
        "Move forward at constant speed",
        "Slightly turn right to clear the green structure",
        "Align with the grid target",
        "Maintain steady approach",
        "Monitor light intensity changes",
        "Stop at observation distance"
      ]
    },
    "images": [
      "data/images/0033.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着主管道继续向前推进，保持匀速，注意避开左侧的珊瑚礁脊。",
    "output": {
      "reasoning": "图像显示主管道位于视野中心并向深处延伸，左侧紧邻一条明显的绿色珊瑚礁脊。为了安全执行指令，机器人需要首先校准航向与管道平行，并向右侧保持一定的安全偏移量以避开左侧的礁脊，随后维持匀速向前推进。",
      "action": [
        "Align with the main pipe",
        "Shift slightly to the right to maintain distance from the coral reef",
        "Move forward at a constant speed",
        "Keep tracking the pipe direction",
        "Monitor left-side clearance"
      ]
    },
    "images": [
      "data/images/0034.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，注意避开左侧缠绕的黄色缆绳，向深处继续探索。",
    "output": {
      "reasoning": "图像显示左侧近景处有大量缠绕的黄色缆绳，构成潜在的挂蹭风险。前方航道相对开阔，上方有网状结构，光线由上方射入。为执行指令，机器人需要向右侧微调以确保与左侧缆绳保持安全距离，随后维持恒定动力输出，向前方深色未知区域持续推进。",
      "action": [
        "Turn slightly right",
        "Maintain constant speed",
        "Go straight",
        "Keep distance from yellow cables on the left",
        "Explore deeper into the dark area"
      ]
    },
    "images": [
      "data/images/0035.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过前方的网状结构，注意避开右侧的大型潜航器主体并留意上方缆绳的动态。",
    "output": {
      "reasoning": "图像显示右侧紧邻大型潜航器主体，左侧及下方分布有密集的网状结构。为了安全穿过，机器人需要保持在网状结构与潜航器之间的空隙行进。上方有多条缆绳，需维持当前深度以防挂蹭。操作重点在于微调航向向左偏移以避开右侧障碍，并保持恒定推力通过。",
      "action": [
        "Maintain constant speed",
        "Slightly steer left to avoid the submersible on the right",
        "Monitor overhead cables for clearance",
        "Align with the gap in the mesh structure",
        "Go straight through the mesh",
        "Maintain stable depth and heading"
      ]
    },
    "images": [
      "data/images/0036.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着海底线缆匀速向前，抵近正前方的青色传感器立柱，同时注意观察左侧潜水员的动向。",
    "output": {
      "reasoning": "视觉分析显示，海底有一条明显的线缆引导至正前方的青色传感器立柱，目标路径清晰。左侧中景处有一名潜水员，属于动态目标，需在移动过程中保持监测。规划逻辑为：以线缆为基准线匀速前进，实时关注左侧潜水员位置，最终在立柱前安全距离处停止。",
      "action": [
        "Align with the seabed cable",
        "Move forward at a constant speed",
        "Monitor the diver's movement on the left",
        "Approach the cyan sensor pillar",
        "Slow down and stop"
      ]
    },
    "images": [
      "data/images/0037.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从左侧石柱与蓝色管道之间的通道穿过去，注意观察近处管道的连接处。",
    "output": {
      "reasoning": "图像左侧为纹理复杂的石柱，中间偏左是一根带有加固环的蓝色水平管道。两者之间存在一个可通行的间隙。机器人当前位于右侧，需要先向左微调以对准该通道。在匀速通过时，需将视觉焦点对准蓝色管道表面的连接部位进行检查。",
      "action": [
        "Turn left slightly to align with the gap",
        "Maintain constant speed forward",
        "Approach the passage between the stone pillar and the blue pipe",
        "Focus camera on the connection joints of the blue pipe",
        "Pass through the gap while observing the pipe",
        "Keep moving forward"
      ]
    },
    "images": [
      "data/images/0038.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前方巨大的绿色环状珊瑚靠近，注意避开底部的小型障碍物。",
    "output": {
      "reasoning": "图像中心是一个巨大的绿色环状珊瑚（目标），底部散布着黄色、紫色和圆顶状的小型珊瑚（障碍物）。为了安全靠近，机器人需要保持恒定速度，并稍微提升高度以确保越过底部的障碍物，随后直线向目标推进。",
      "action": [
        "Maintain constant speed",
        "Slightly ascend to clear bottom obstacles",
        "Go straight towards the green ring coral",
        "Slow down",
        "Stop near the target"
      ]
    },
    "images": [
      "data/images/0039.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右绕过正前方的大型珊瑚礁，注意保持匀速并观察后方的开阔水域。",
    "output": {
      "reasoning": "图像正前方是一个占据主要视野的大型绿色珊瑚礁。为了执行‘向右绕过’的指令，机器人需要先向右修正航向以避开障碍物，随后保持匀速直线行驶。在完全越过珊瑚礁后，需要向左转向或调整视角，以观察并确认珊瑚礁后方的开阔水域情况。",
      "action": [
        "Turn right",
        "Go straight at constant speed",
        "Bypass the large coral reef",
        "Turn left to observe the open water behind",
        "Maintain constant speed",
        "Monitor the rear open area",
        "Stop"
      ]
    },
    "images": [
      "data/images/0040.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从正前方的圆形珊瑚和右侧石柱之间的缝隙穿过去，注意观察底部环境变化。",
    "output": {
      "reasoning": "图像显示正前方有一个带孔的圆形珊瑚，右侧是一个高大的石柱，两者之间存在明显的通道。为了安全穿过，机器人需要保持匀速并微调航向对准缝隙，同时在移动过程中俯视观察底部沙地的纹理变化。",
      "action": [
        "Maintain constant speed",
        "Move forward",
        "Adjust heading to the gap between coral and pillar",
        "Pass through the gap",
        "Monitor seabed environment",
        "Continue straight"
      ]
    },
    "images": [
      "data/images/0041.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过两侧的岩石区域，靠近前方底部的红色目标物，注意观察周围环境的动态变化。",
    "output": {
      "reasoning": "图像显示前方底部有一个带有白色标识的红色球形目标物，其位于左右两侧大型岩石构成的通道中间。路径清晰，光照充足。机器人需要保持匀速直线前进，穿过岩石区域，同时持续监测水下环境的动态，最终精准靠近底部的红色目标。",
      "action": [
        "Maintain constant speed forward",
        "Navigate through the gap between the left and right rocks",
        "Monitor surrounding environment for dynamic changes",
        "Approach the red target on the seabed",
        "Slow down and stop near the target"
      ]
    },
    "images": [
      "data/images/0042.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方的石柱靠近，保持匀速并注意观察底部散落的遗迹碎片。",
    "output": {
      "reasoning": "图像中心正前方有一根显著的石柱，光线从上方射入，路径清晰无障碍。石柱底部散落着多块遗迹碎片。机器人需要保持稳定速度直线前进，在接近石柱基座时减速，以便对底部的碎片进行详细视觉观察。",
      "action": [
        "Go straight",
        "Maintain steady speed",
        "Approach the stone pillar",
        "Slow down near the base",
        "Observe relic fragments",
        "Stop"
      ]
    },
    "images": [
      "data/images/0043.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方发光的石柱中心缓慢推进，注意避开地面的障碍物并保持航向稳定。",
    "output": {
      "reasoning": "发光的石柱位于视野正前方，是明确的目标。地面左右两侧分布有零散的岩石障碍物。为了安全抵达目标，机器人需要对准石柱中心，以低速前进以保持航行稳定性，并注意微调航向以避开底部的障碍物。",
      "action": [
        "Align with the center of the glowing pillar",
        "Move forward slowly",
        "Adjust heading to avoid ground obstacles",
        "Maintain stable course",
        "Approach the pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0044.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前下方缓慢推进，抵近观察前方的条纹状目标，保持匀速并注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "图像中心前方有一个明显的绿色条纹状大型目标，其位置处于当前视角的下方。底部散布着珊瑚礁，构成了主要的障碍风险。为了安全抵近观察，机器人需要调整姿态向下俯冲，并以缓慢、匀速的状态向前推进，同时必须严格监控与底部珊瑚的垂直距离以防碰撞。",
      "action": [
        "Pitch down",
        "Move forward slowly",
        "Maintain constant speed",
        "Monitor distance to bottom coral reefs",
        "Approach the striped target",
        "Hover and observe",
        "Stop"
      ]
    },
    "images": [
      "data/images/0045.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右绕过正前方的深色结构，注意避开底部的珊瑚礁，向光线透射的深处探索。",
    "output": {
      "reasoning": "图像正前方有一个巨大的深色柱状结构阻挡路径，底部和四周分布着密集的珊瑚礁。光线从右侧和上方透射向深处。为了绕过障碍物并避开底部珊瑚，机器人需要先向右侧转向，同时保持高度以防触碰底部，最后沿着光束指引的方向匀速前进。",
      "action": [
        "Maintain constant speed",
        "Turn right to bypass the dark structure",
        "Adjust altitude to clear bottom coral reefs",
        "Head towards the light rays in the distance",
        "Go straight into the deep water",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0046.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前穿过面前的珊瑚峡谷，注意近处的动态变化并保持稳定航行。",
    "output": {
      "reasoning": "视觉感知显示机器人正处于一个由两侧巨大珊瑚礁构成的狭窄峡谷入口。前方路径中央有光线透入，且可见另一台潜航器正在作业，表明路径可通行。为了安全穿过，机器人需要对准峡谷中央开口，在前进过程中实时监测与两侧不规则珊瑚壁的距离，并根据水流微调以保持航行稳定性。",
      "action": [
        "Align with the center of the coral canyon",
        "Move forward at a steady speed",
        "Monitor distance to left and right coral walls",
        "Maintain stable depth and attitude",
        "Go straight through the passage",
        "Stop after clearing the canyon"
      ]
    },
    "images": [
      "data/images/0047.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过左侧的弧形残骸边缘，向深处的六边形目标靠近，注意避开底部的珊瑚群。",
    "output": {
      "reasoning": "图像左侧可见巨大的弧形金属残骸边缘，正前方深处有一个明显的六边形人造物体（目标）。底部沙地上散布着多组深色的珊瑚礁。为了执行指令，机器人需要保持匀速，沿左侧残骸的内径轨迹航行，同时通过调整垂直高度或水平位移避开底部的珊瑚障碍，最后直线向深处的六边形目标靠拢。",
      "action": [
        "Go straight at constant speed",
        "Navigate along the inner edge of the curved wreckage on the left",
        "Maintain altitude to clear bottom coral clusters",
        "Adjust heading towards the distant hexagonal target",
        "Approach the hexagonal target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0048.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近蓝色立柱基座进行抵近观察，保持匀速，注意避开右侧的珊瑚群。",
    "output": {
      "reasoning": "图像中心是一个蓝色的立柱状结构，目标基座位于正前方。右侧有明显的珊瑚群障碍物。为了执行抵近观察任务并避开障碍，机器人需要保持正向对准立柱基座，在前进过程中稍微偏左以确保与右侧珊瑚保持安全距离，并最终在目标前减速停止。",
      "action": [
        "Align with the base of the blue pillar",
        "Move forward at a constant speed",
        "Keep a safe distance from the coral on the right",
        "Approach the base of the pillar",
        "Slow down and stop for observation"
      ]
    },
    "images": [
      "data/images/0049.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前探索，注意避开右侧巨大的脑纹珊瑚，并留意上方光影区域的动态变化。",
    "output": {
      "reasoning": "图像显示机器人正处于珊瑚礁缝隙中，右侧分布着大面积的脑纹珊瑚，上方有明显的透射光影。为了安全探索，机器人需要保持稳定的推进速度，同时向左微调航向以与右侧珊瑚保持安全距离，并持续监测上方光照区域的亮度变化以应对潜在动态物体。",
      "action": [
        "Maintain constant speed",
        "Steer slightly left to avoid brain coral on the right",
        "Go straight",
        "Monitor upper light and shadow area",
        "Keep exploring"
      ]
    },
    "images": [
      "data/images/0050.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，靠近前方的青色结构体，注意规避左侧高耸的白色珊瑚礁并观察底部地形。",
    "output": {
      "reasoning": "视觉分析显示，目标青色结构体位于前方中心偏右位置，而高耸的白色珊瑚礁位于左侧，构成潜在碰撞风险。底部地形清晰可见，分布有黑色球状珊瑚。规划路径需先向右微调以拉开与左侧珊瑚礁的间距，随后保持匀速直线推进，同时调整视角向下以兼顾地形观察，最后减速靠近目标。",
      "action": [
        "Go straight at constant speed",
        "Steer slightly right to avoid the white coral reef",
        "Maintain forward propulsion towards the cyan structure",
        "Tilt camera down to observe bottom terrain",
        "Slow down and approach target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0051.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前靠近沉没的车辆，注意避开左侧的珊瑚礁并观察近处的鱼群动态。",
    "output": {
      "reasoning": "图像显示沉没的车辆位于前方中心偏右位置，左侧有明显的黄色脑珊瑚和紫色珊瑚礁作为障碍物。为了安全靠近并观察鱼群，机器人需要先向右微调航向以避开左侧珊瑚区域，随后保持匀速直线前进，同时在接近过程中利用视觉传感器锁定并观察周围游动的鱼群。",
      "action": [
        "Turn slightly right",
        "Go straight at constant speed",
        "Keep distance from the left coral reef",
        "Observe fish dynamics",
        "Approach the sunken car",
        "Slow down and hover"
      ]
    },
    "images": [
      "data/images/0052.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速行驶，穿过两根石柱之间的开阔水域，注意观察右侧卡车周边的环境变化。",
    "output": {
      "reasoning": "图像显示前方有两根巨大的石柱，中间形成了一个开阔的通道。蓝色卡车位于右前方。为了执行指令，机器人需要保持直线匀速前进，穿过石柱间的空旷区域，同时在移动过程中将传感器焦点保持在右侧的卡车及其周边环境，以监测任何变化。",
      "action": [
        "Move forward at constant speed",
        "Maintain heading towards the gap between the pillars",
        "Monitor the truck and its surroundings on the right",
        "Pass through the stone pillars",
        "Continue observation while moving forward"
      ]
    },
    "images": [
      "data/images/0053.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速航行，穿过左侧环形立柱与右侧石柱之间的空隙，抵近观察前方的沉车残骸，注意保持安全距离。",
    "output": {
      "reasoning": "图像中左侧为一个带有多个立柱的环形建筑结构，右侧为一个具有复杂纹理的独立石柱，正前方背景处可见一辆沉没的汽车残骸。两柱之间存在明显的航行空间。任务要求穿过该间隙并抵近残骸，需先保持直线航行，通过间隙后调整速度接近目标，最后在安全距离内停止。",
      "action": [
        "Go straight at constant speed",
        "Pass through the gap between the left circular pillar and the right stone pillar",
        "Approach the car wreck",
        "Slow down and maintain safe distance",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0054.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从正前方的珊瑚礁群中间穿过去，注意观察前方悬浮目标的动态变化。",
    "output": {
      "reasoning": "图像显示正前方有一组珊瑚礁构成的天然通道，通道中心位置有一个带有绿色指示灯的悬浮目标。为了执行指令，机器人需要对准通道中心，保持恒定速度前进。在接近并穿过通道的过程中，视觉系统需持续锁定前方悬浮目标，实时监测其位置或状态的动态变化，确保安全通行。",
      "action": [
        "Align with the center of the reef gap",
        "Maintain constant speed",
        "Go straight",
        "Track the floating target",
        "Pass through the reef group",
        "Monitor target dynamics"
      ]
    },
    "images": [
      "data/images/0055.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前跟随前方的目标潜航器，保持匀速并注意避开左侧巨大的珊瑚礁石。",
    "output": {
      "reasoning": "视觉分析显示，目标潜航器位于视野正中心，路径前方基本开阔。左侧前景存在巨大的深色珊瑚礁石，距离较近。为安全跟随，机器人需要锁定前方目标，航向稍微向右偏移以远离左侧礁石，随后保持稳定速度直线推进。",
      "action": [
        "Lock on target submersible",
        "Slightly steer right to avoid left reef",
        "Go straight",
        "Maintain constant speed",
        "Keep following target"
      ]
    },
    "images": [
      "data/images/0056.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方光照充足的开阔区域匀速前进，注意避开右侧的大型珊瑚礁。",
    "output": {
      "reasoning": "视觉分析显示，图像右上方有明显的透射光线，指示了光照充足的开阔区域。右下角存在大型脑纹状珊瑚礁，作为主要障碍物需要避开。机器人当前位于中层水域，需向右上方调整姿态，在确保与右侧珊瑚礁保持安全距离的前提下，向光亮处匀速推进。",
      "action": [
        "Turn slightly right",
        "Pitch up towards the light rays",
        "Maintain steady speed",
        "Keep distance from the coral reef on the right",
        "Go straight to the open area",
        "Monitor surroundings"
      ]
    },
    "images": [
      "data/images/0057.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿黄色标记物中间的通道向前航行，抵近观察前方的卡车残骸，注意避开底部的障碍物。",
    "output": {
      "reasoning": "图像显示前方有一条由两排黄色圆柱形标记物构成的清晰通道，通道尽头是一辆白色卡车残骸。通道底部的中心位置存在齿轮状的结构，需注意避让。规划路径为：对准通道中心，保持匀速前进，略微提升高度或调整路径避开底部障碍物，最后减速抵近卡车残骸进行观察。",
      "action": [
        "Align with the center of the yellow markers",
        "Move forward at constant speed",
        "Adjust altitude to clear bottom obstacles",
        "Continue forward through the channel",
        "Slow down and approach the truck wreckage",
        "Stop and hover for observation"
      ]
    },
    "images": [
      "data/images/0058.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右侧转向以避开正前方的红色障碍物，随后保持匀速向深处的岩石群探索，注意观察近处的地形起伏。",
    "output": {
      "reasoning": "图像正前方有一个明显的红色圆形障碍物，阻挡了直接前进的路径。目标岩石群位于画面的中远景深处。根据指令，机器人需要先向右转向以绕过红色障碍物，随后重新校准航向指向深处的岩石群。在移动过程中，需要保持匀速并注意观察近处起伏不平的海床地形，确保航行安全。",
      "action": [
        "Turn right",
        "Go straight to bypass the red obstacle",
        "Adjust heading towards the distant rock formations",
        "Maintain steady speed",
        "Observe the nearby undulating terrain",
        "Move towards the deep rocks",
        "Stop"
      ]
    },
    "images": [
      "data/images/0059.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左绕过近处的红色浮标，保持匀速向远处发光的柱状目标靠近，注意观察周围的动态变化。",
    "output": {
      "reasoning": "图像前景中心偏左位置有一个红色的圆形浮标（障碍物），远处背景中有一个带有红色发光点的柱状目标。根据指令，机器人需要先向左转向以避开近处的红色浮标，在确保绕过障碍物后，重新调整航向对准远处的发光目标，并以匀速推进，同时在整个过程中保持对周围环境的动态监测。",
      "action": [
        "Turn left",
        "Go straight to bypass the red buoy",
        "Turn right to align with the glowing columnar target",
        "Go straight at steady speed",
        "Monitor surrounding dynamic changes",
        "Approach the target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0060.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近橙色结构体的绿色开口，注意避开左侧的绿色立柱，保持匀速。",
    "output": {
      "reasoning": "视觉分析显示，目标橙色结构体位于正前方，其绿色开口清晰可见。左侧存在一根高大的绿色立柱，属于潜在碰撞风险。为了执行指令，机器人需要先微调航向以确保与左侧立柱保持安全距离，随后以恒定低速向前推进，最终停在开口附近。",
      "action": [
        "Adjust heading slightly to the right to clear the left pillar",
        "Move forward slowly",
        "Maintain constant speed",
        "Approach the green opening",
        "Stop"
      ]
    },
    "images": [
      "data/images/0061.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方绕过红色的浮标结构，向远处的黑色目标靠近，保持匀速并注意上方悬挂设备的距离。",
    "output": {
      "reasoning": "图像中红色浮标结构（带红锥的绿条纹物体）位于前方偏左，黑色目标位于右后方远处。上方悬挂有摄像/传感器设备。规划路径为：先向右侧转向以避开红色浮标，同时注意上方悬挂设备的垂直距离，随后直线靠近远处的黑色目标并保持匀速。",
      "action": [
        "Turn right",
        "Bypass the red buoy structure",
        "Monitor distance to overhead equipment",
        "Go straight towards the distant black target",
        "Maintain constant speed",
        "Approach target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0062.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从右侧绕过正前方的绿色立柱，注意观察底部地形的变化。",
    "output": {
      "reasoning": "图像正前方有一个巨大的绿色圆柱形立柱，其右侧空间较为开阔且有光线透入，适合绕行。底部地形存在沙丘状起伏和阴影变化。根据指令，机器人需要保持恒定速度，向右侧调整航向以避开立柱，同时在移动过程中保持对下方地形的视觉监测。",
      "action": [
        "Maintain constant speed",
        "Turn right slightly",
        "Go straight to bypass the green pillar",
        "Tilt camera down to observe bottom terrain",
        "Monitor terrain changes",
        "Turn left slightly to resume original heading",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0063.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过两根石柱之间的空隙，注意避开底部的障碍物，保持匀速前进。",
    "output": {
      "reasoning": "图像显示前方有两根巨大的覆苔石柱，中间存在明显的通行间隙。但在机器人正前方的海床上有一块长方形的覆苔岩石（障碍物）。为了安全执行指令，机器人需要先调整高度或方向避开底部的障碍物，随后对准两柱之间的中心路径，保持匀速穿过。",
      "action": [
        "Adjust position to bypass the bottom obstacle",
        "Align with the center of the gap between the two pillars",
        "Go straight at a constant speed",
        "Pass through the gap",
        "Stop"
      ]
    },
    "images": [
      "data/images/0064.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向右前方航行，绕过前方的蓝色目标物并注意避开上方垂下的网状缆绳。",
    "output": {
      "reasoning": "图像中心可见一个蓝色的圆柱形目标物，其上方覆盖并垂下了大面积的网状缆绳（疑似渔网或缆绳）。为了实现向右前方航行并绕过目标的指令，机器人需要首先向右侧调整航向以避开正前方的障碍物核心区，同时需特别注意上方垂下的网状结构，保持安全高度或侧向距离以防缠绕，并全程维持稳定的航行速度。",
      "action": [
        "Maintain constant speed",
        "Turn right",
        "Steer to avoid the hanging mesh cable",
        "Bypass the blue target object from the right side",
        "Continue sailing towards the front-right",
        "Keep clear of obstacles"
      ]
    },
    "images": [
      "data/images/0065.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，沿着右侧的缆绳结构向前方亮光处探索，注意观察上方网格结构的动态变化。",
    "output": {
      "reasoning": "图像显示机器人正处于一个由上方巨大网格结构和右下方粗壮缆绳结构构成的水下通道中。前方远处有明显的亮光作为导航目标。为了执行指令，机器人需要锁定前方光源作为航向参考，同时保持与右侧缆绳的相对位置进行循迹，并实时监测上方网格的动态以防环境变化。",
      "action": [
        "Set constant forward speed",
        "Align heading with the light source ahead",
        "Maintain position relative to the cable on the right",
        "Monitor the grid structure above for changes",
        "Continue forward exploration",
        "Keep steady pace"
      ]
    },
    "images": [
      "data/images/0066.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向左前方推进，避开右侧的大型珊瑚礁，同时注意观察底部散落的目标物。",
    "output": {
      "reasoning": "图像右侧有一个巨大的深色珊瑚礁，构成主要障碍。底部沙地上散落着多个带有黄色斑点的球状目标物。为了向左前方推进并避开右侧珊瑚，机器人需要向左进行偏航调整，同时保持稳定的推进动力，并确保传感器/视角覆盖底部的散落目标。",
      "action": [
        "Turn left slightly",
        "Maintain steady forward speed",
        "Keep distance from the coral reef on the right",
        "Scan and monitor bottom scattered objects",
        "Proceed forward-left",
        "Maintain constant velocity"
      ]
    },
    "images": [
      "data/images/0067.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着右侧的黄色管线继续向前巡检，注意避开底部的珊瑚丛，保持匀速航行。",
    "output": {
      "reasoning": "图像右侧有一条明显的黄色管线向远方延伸，底部中心及左侧分布有数个珊瑚丛。为了沿着管线巡检并避开珊瑚，机器人需要向右侧管线靠拢，同时保持安全高度以越过底部的障碍物，随后进入匀速直行状态。",
      "action": [
        "Align with the yellow pipeline",
        "Adjust altitude to clear bottom coral",
        "Go straight along the pipeline",
        "Maintain constant speed"
      ]
    },
    "images": [
      "data/images/0068.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前方光束汇聚的中心区域推进，注意避开左侧突出的珊瑚礁石。",
    "output": {
      "reasoning": "图像显示光束汇聚在前方中心区域，路径左侧有明显的黄色珊瑚礁石突出。为了安全到达目标并遵循指令，机器人需要先向右微调以避开左侧礁石，随后保持稳定速度直线向光影中心推进。",
      "action": [
        "Adjust heading slightly to the right",
        "Maintain constant speed",
        "Go straight towards the light convergence center",
        "Keep clear of the left coral reef",
        "Stop at the target area"
      ]
    },
    "images": [
      "data/images/0069.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向左前方靠近那几座大型管道入口，注意观察底部的岩石分布。",
    "output": {
      "reasoning": "图像显示左前方有明显的多个大型管道入口，底部沙地上散布着黑色岩石。为了执行指令，机器人需要向左微调航向，以稳定的速度向管道方向推进，同时在移动过程中保持视角覆盖底部的岩石分布情况。",
      "action": [
        "Turn slightly left",
        "Maintain constant speed",
        "Go straight towards the pipe inlets",
        "Scan and observe bottom rocks",
        "Approach the target inlets",
        "Stop"
      ]
    },
    "images": [
      "data/images/0070.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近前方的红白条纹浮标进行抵近观察，注意避开底部的珊瑚群。",
    "output": {
      "reasoning": "图像中心显示一个红白条纹的浮标目标，位于机器人正前方。底部散布着多处珊瑚群（黄色和红色）。为了执行指令，机器人需要保持当前航向匀速前进，同时通过调整垂直高度或保持安全间距来避开底部的珊瑚障碍，最后在靠近浮标时减速以进行稳定观察。",
      "action": [
        "Maintain constant speed forward",
        "Adjust altitude to clear bottom corals",
        "Approach the red-and-white striped buoy",
        "Slow down for close observation",
        "Hover and stop"
      ]
    },
    "images": [
      "data/images/0071.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向左前方绕过红色的障碍物，去探索前方光影下的环状结构，注意观察近处的动态变化。",
    "output": {
      "reasoning": "图像右侧近景处有一个大型红色障碍物，目标环状结构位于左前方且处于上方透射的光影汇聚点。根据指令，需要先向左转向以避开红色障碍物，随后保持匀速向光影区域推进，同时需实时监测近处可能存在的水流或生物动态变化。",
      "action": [
        "Maintain constant speed",
        "Turn left to bypass the red obstacle",
        "Go straight towards the light-shadow area",
        "Monitor nearby dynamic changes",
        "Approach the ring structure",
        "Observe and inspect target"
      ]
    },
    "images": [
      "data/images/0072.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过面前的两根石柱，去调查那辆绿色的汽车，注意近处的动态变化。",
    "output": {
      "reasoning": "视觉分析显示，目标绿色汽车位于两根高大的石柱后方，正上方有强光源照射。路径前方开阔，两根石柱构成了天然的‘门廊’。机器人需要保持稳定速度，对准石柱间的空隙穿过，随后接近汽车。考虑到指令中提到的‘动态变化’，在移动过程中需实时监测周围游动的生物或光影波动。",
      "action": [
        "Maintain constant speed forward",
        "Align with the gap between the two stone pillars",
        "Pass through the pillars",
        "Monitor nearby dynamic objects and light changes",
        "Approach the green car",
        "Stop and investigate the vehicle"
      ]
    },
    "images": [
      "data/images/0073.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过左前方的石门，并注意避开右侧的散落石柱。",
    "output": {
      "reasoning": "图像显示左前方有一个巨大的石门，而右侧分布着数根散落的石柱。机器人当前位于石门右侧，需要先向左微调航向以对准石门开口，同时在移动过程中与右侧石柱保持安全距离，最后匀速直行穿过门洞。",
      "action": [
        "Turn left slightly",
        "Align with the stone gate",
        "Maintain constant speed",
        "Go straight",
        "Keep distance from pillars on the right",
        "Pass through the gate",
        "Stop"
      ]
    },
    "images": [
      "data/images/0074.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，沿着管道外壁进行近距离巡检，注意观察接口处的动态变化。",
    "output": {
      "reasoning": "图像显示一条大型管道沿前方延伸，右侧可见蓝色软管接口及推进装置。机器人当前位于管道左下方，路径前方开阔。为执行巡检任务，需调整姿态使其与管道外壁保持平行且近距离，随后匀速向前推进，并重点对蓝色软管连接处及管道分段接口进行视觉监测。",
      "action": [
        "Align with pipe axis",
        "Maintain close distance to pipe wall",
        "Go straight at constant speed",
        "Scan pipe surface",
        "Focus on joint interfaces",
        "Monitor for dynamic changes",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0075.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方修正航向以避开近处的红黑条纹线缆，随后保持匀速抵近前方的绿色网状结构进行巡检。",
    "output": {
      "reasoning": "图像右侧近处有明显的红黑条纹线缆，构成潜在碰撞风险；前方底部为目标绿色网状结构。根据指令，机器人需先向左前方转向以拉开与右侧线缆的安全距离，避开障碍后，再重新对准前方的绿色网状结构，保持匀速抵近以准备巡检。",
      "action": [
        "Turn left-forward",
        "Bypass the red-black cable",
        "Align with the green mesh structure",
        "Go straight at constant speed",
        "Close to the green mesh",
        "Stop and start inspection"
      ]
    },
    "images": [
      "data/images/0076.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过右侧高大珊瑚柱与左侧珊瑚群之间的空隙，注意观察前方光影区域的动态变化。",
    "output": {
      "reasoning": "图像显示机器人正处于水下环境中，右侧有一座高大的珊瑚柱，左侧和下方分布着珊瑚群。目标路径是两者之间的空隙，正前方有明显的海面透射光影区域。机器人需要保持稳定的动力输出，微调航向以精准对准空隙中心，并在行进过程中持续监测前方光影区域是否存在动态障碍物或环境变化。",
      "action": [
        "Maintain constant speed forward",
        "Adjust heading to align with the gap between the right pillar and left cluster",
        "Pass through the gap",
        "Monitor dynamic changes in the light and shadow area ahead",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0077.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着红色浮标柱一直向前探索，注意避开两侧的珊瑚礁，保持匀速前进。",
    "output": {
      "reasoning": "图像中心有一根垂直延伸的红色浮标柱，两侧分布着明显的珊瑚礁（左侧为深色扇形珊瑚，右侧为黄色脑纹珊瑚）。路径中央相对开阔，机器人需要对准红色浮标柱的轴线，在保持与两侧障碍物安全距离的前提下，匀速向前推进。",
      "action": [
        "Align with the red buoy pole",
        "Maintain central position between coral reefs",
        "Move forward at a steady speed",
        "Monitor lateral distance to corals",
        "Continue exploration along the pole"
      ]
    },
    "images": [
      "data/images/0078.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右前方避开左侧的巨大珊瑚礁，向远处透光的开阔水域探索。",
    "output": {
      "reasoning": "图像左侧占据了巨大的珊瑚礁结构，构成主要障碍。右前方视野开阔，且有明显的阳光（耶稣光）射入，符合指令中‘透光的开阔水域’特征。机器人需要向右修正航向以绕开左侧礁石，并保持稳定速度向光亮区域航行。",
      "action": [
        "Maintain constant speed",
        "Turn right",
        "Go straight towards the light rays",
        "Keep distance from the left coral reef",
        "Navigate to open water",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0079.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从左侧的灰色条纹礁石和右侧的黄色碗状珊瑚之间穿过，向远处的光影区域探索，注意观察近处底部的地形变化。",
    "output": {
      "reasoning": "图像显示正前方有一条清晰的通道，左侧是带有灰色条纹的礁石，右侧是显著的黄色碗状珊瑚。远方背景有明亮的光影区域。路径中心无障碍，适合匀速穿行。需要保持航向处于两者中间，并持续监测底部沙地的地形起伏。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Pass between the grey reef and yellow coral",
        "Monitor bottom terrain changes",
        "Move towards the distant light area",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0080.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近正前方的石柱，保持匀速并注意观察其底部的结构细节。",
    "output": {
      "reasoning": "图像显示正前方有一座高大的石柱，路径平坦且无明显障碍物。目标明确位于视野中心，底部有明显的蓝色结构。根据指令，机器人需要直线前进，在接近底部时减速以进行细节观察。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Approach the base of the stone pillar",
        "Slow down for detailed observation",
        "Stop and inspect the bottom structure"
      ]
    },
    "images": [
      "data/images/0081.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢靠近那辆沉没的汽车进行勘测，注意保持与中间石柱的安全距离。",
    "output": {
      "reasoning": "目标沉没汽车位于图像右前方，中间有一根巨大的石柱作为障碍物。为了安全靠近汽车，机器人需要先向右调整航向以绕开石柱，随后保持低速缓慢前行，在确保与石柱留有足够安全间隙的前提下接近汽车，最后悬停进行勘测。",
      "action": [
        "Turn right",
        "Move forward slowly",
        "Keep safe distance from the pillar",
        "Approach the car",
        "Stop and survey"
      ]
    },
    "images": [
      "data/images/0082.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前方的大型结构物缓慢靠近，注意避开左侧的礁石。",
    "output": {
      "reasoning": "图像显示左侧前景有一块巨大的深色礁石，而目标大型结构物位于正前方远处的明亮区域。为了安全避开左侧障碍物并靠近目标，机器人需要先向右侧微调航向以增大安全间距，随后保持稳定的低速直线航行。",
      "action": [
        "Slightly turn right",
        "Maintain constant slow speed",
        "Go straight",
        "Keep distance from the left reef",
        "Approach the large structure ahead",
        "Monitor proximity to target"
      ]
    },
    "images": [
      "data/images/0083.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，缓慢向前推进，绕过底部的线缆障碍，抵近观察正前方那根发光的绿色立柱。",
    "output": {
      "reasoning": "图像正前方有一根显著的发光绿色立柱（目标），其底部及右侧地面散布着复杂的线缆和网状障碍物。为执行指令，机器人需要先以低速启动，向左前方微调路径以绕开底部的线缆密集区，随后直线对准立柱中心抵近，最后在安全距离内停止以进行观察。",
      "action": [
        "Move forward slowly at constant speed",
        "Steer slightly left to bypass bottom cables",
        "Go straight towards the glowing green pillar",
        "Approach the pillar closely",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0084.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过前方的柱状阵列，注意避开左侧和下方的珊瑚礁。",
    "output": {
      "reasoning": "图像显示前方有数根垂直的柱状结构形成阵列，左侧和下方分布着大面积且凹凸不平的珊瑚礁。机器人当前位于珊瑚礁上方，需微调姿态以对准柱状阵列间的空隙，随后保持稳定速度穿行，同时需持续监测与下方及左侧障碍物的距离以确保安全避障。",
      "action": [
        "Adjust heading to align with the gap between pillars",
        "Maintain constant forward speed",
        "Keep safe distance from left and bottom coral reefs",
        "Pass through the pillar array",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0085.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从右侧绿色浮标和左侧珊瑚礁之间穿过去，注意避开近处的红色突出物。",
    "output": {
      "reasoning": "图像显示右侧是一个带有红色突出物的大型绿色浮标，左侧是珊瑚礁。红色突出物位于路径右侧近处，构成直接障碍。为了安全穿过，机器人需要先向左微调以避开红色突出物，随后在保持匀速的状态下，从绿色浮标与左侧珊瑚礁之间的空隙直线穿过。",
      "action": [
        "Adjust heading slightly to the left",
        "Maintain constant speed",
        "Go straight",
        "Keep clear of the red protrusion on the right",
        "Pass through the gap between the green buoy and the coral reef",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0086.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，缓慢靠近正前方的多孔柱状结构进行近距离观察，注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "图像正前方是一个巨大的多孔柱状结构，这是主要目标。底部和周围散布着绿色的珊瑚礁和岩石。为了执行指令，机器人需要保持稳定的深度以避开底部的珊瑚礁，同时以恒定的低速向前推进。在接近目标时，需要精确控制距离以实现近距离观察而不发生碰撞。",
      "action": [
        "Maintain current depth to clear bottom reefs",
        "Move forward slowly at a constant speed",
        "Keep the porous column in the center of the frame",
        "Approach the target structure",
        "Decelerate as distance to the column decreases",
        "Hover for close-up observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0087.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，抵近观察正前方的雕纹石柱，注意避开底部的珊瑚丛。",
    "output": {
      "reasoning": "图像正前方有一根巨大的雕纹石柱，这是主要目标。石柱底部周围分布着明显的绿色珊瑚丛，属于需要避开的障碍物。路径规划应为：保持匀速直线前进，在接近过程中微调高度或角度以确保避开底部的珊瑚，最后在石柱前减速并悬停观察。",
      "action": [
        "Go straight at constant speed",
        "Maintain altitude to clear bottom corals",
        "Approach the carved pillar",
        "Slow down",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0088.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从上方潜航器和右侧线缆束之间的空隙穿过去，注意避开近处的缆绳并留意环境变化。",
    "output": {
      "reasoning": "图像显示左下方有近处缠绕的缆绳（障碍物），左上方为潜航器，右侧为延伸的线缆束。目标空隙位于潜航器与线缆束之间。规划路径为：首先小幅上升并右转以避开近处缆绳，随后对准空隙中心，保持匀速直线穿过，并持续监测环境动态。",
      "action": [
        "Ascend slightly to clear foreground ropes",
        "Turn slightly right to align with the gap",
        "Maintain constant speed",
        "Go straight through the gap",
        "Monitor environment for changes",
        "Stop after clearing the passage"
      ]
    },
    "images": [
      "data/images/0089.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着右侧的黄色管线继续向前推进，靠近前方的垂直支柱，注意保持匀速并观察左侧的结构变化。",
    "output": {
      "reasoning": "视觉分析显示，图像右侧有一条明显的黄色框架式管线延伸向远方，正前方背景处可见一根连接水面的垂直支柱。左侧为深色的弧形壁面结构。路径清晰，无明显障碍物。机器人需要以右侧管线为引导，保持航向并向支柱靠近，同时监测左侧环境。",
      "action": [
        "Align with the yellow pipeline on the right",
        "Move forward at a constant speed",
        "Monitor structural changes on the left",
        "Approach the vertical pillar ahead",
        "Slow down and stop near the pillar"
      ]
    },
    "images": [
      "data/images/0090.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向前靠近中心那个复杂的珊瑚结构进行近距离观测，保持匀速，注意避开底部突出的尖刺。",
    "output": {
      "reasoning": "图像中心是一个复杂的褶皱状珊瑚结构，其下方底座分布有大量带有黄色尖端的圆锥形刺状突起（障碍物）。为安全执行指令，机器人需要先略微提升高度以避开底部的尖刺路径，随后以低速匀速向前推进，直到接近中心目标进行观测。",
      "action": [
        "Ascend slightly to clear bottom spikes",
        "Go straight slowly",
        "Maintain constant velocity",
        "Approach the central coral structure",
        "Stop at observation distance"
      ]
    },
    "images": [
      "data/images/0091.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近中心的发光珊瑚结构进行抵近观察，保持匀速，注意避开底部的褶皱边缘。",
    "output": {
      "reasoning": "图像中心是一个巨大的发光珊瑚结构，顶部具有脑状纹理，中部有明亮的黄色发光点，底部连接着波浪状的褶皱边缘。目标位于正前方，路径清晰。为了执行抵近观察并避开底部边缘，机器人需要保持中心对准，稍微调整高度以越过底部的物理障碍，然后匀速推进并在安全距离处停下。",
      "action": [
        "Align with the center of the glowing coral",
        "Adjust altitude slightly to clear the bottom ruffled edges",
        "Move forward at a constant speed",
        "Slow down as approaching the target",
        "Stop and hover for close observation"
      ]
    },
    "images": [
      "data/images/0092.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，穿过左侧紫色海绵与右侧珊瑚之间的空隙，注意观察前方开阔水域的动态。",
    "output": {
      "reasoning": "视觉分析显示，左侧下方有一块巨大的紫色海绵，右侧为深色珊瑚礁，两者之间存在一个明显的通道。机器人当前视角正对该间隙，路径清晰。根据指令，需要保持匀速直线航行以穿过该区域，并在进入前方光照充足的开阔水域时保持环境监测。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Pass between the purple sponge and the coral",
        "Navigate into open water",
        "Monitor forward surroundings",
        "Continue cruising"
      ]
    },
    "images": [
      "data/images/0093.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前探索，注意避开右侧的大型珊瑚礁，向光照充足的开阔水域行进。",
    "output": {
      "reasoning": "图像显示右侧存在大型深色珊瑚礁障碍物，而光照主要集中在左前方及上方区域。为了执行指令，机器人需要保持前进动力，同时向左侧微调航向以远离右侧礁石，并最终驶向光线充足的开阔水域。",
      "action": [
        "Go straight at constant speed",
        "Turn slightly left to avoid the coral reef on the right",
        "Move towards the bright open water",
        "Maintain forward exploration"
      ]
    },
    "images": [
      "data/images/0094.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方的螺旋状柱子靠近，注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "图像中心有一个明显的绿色螺旋状柱子，这是目标物体。在机器人的下方和左右两侧分布有大面积的黄色珊瑚礁。为了执行指令，机器人需要保持稳定的高度以避开底部的珊瑚礁，并沿直线向正前方的柱子匀速推进。",
      "action": [
        "Maintain constant speed",
        "Keep current altitude to clear bottom coral reefs",
        "Go straight towards the spiral pillar",
        "Monitor distance to the pillar",
        "Close to target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0095.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向正前方的石柱靠近进行抵近观察，注意保持匀速，并避开底部隆起的珊瑚礁。",
    "output": {
      "reasoning": "视觉分析显示，目标石柱位于视野正中心，距离适中。石柱底部及前方分布有黄色和绿色的隆起珊瑚礁，构成了路径障碍。为执行指令，机器人需要以恒定低速直线前进，同时注意保持安全高度以跨越或绕开底部的珊瑚礁，最终在靠近石柱的位置停止以进行详细观察。",
      "action": [
        "Set speed to slow and constant",
        "Go straight towards the central pillar",
        "Maintain altitude to clear bottom coral reefs",
        "Slowly approach the pillar surface",
        "Stop at observation distance"
      ]
    },
    "images": [
      "data/images/0096.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持当前深度匀速向前，从绿色结构体右侧空隙穿过，注意避开两侧的珊瑚礁并留意水流变化。",
    "output": {
      "reasoning": "图像中心是一个巨大的绿色分段结构体，其左右两侧分布着密集的珊瑚礁。根据指令，路径规划应选择绿色结构体与右侧珊瑚礁之间的空隙。机器人需要保持当前深度，微调航向向右以对准间隙，随后匀速穿过，过程中需实时监测与两侧障碍物的距离，并根据水流波动调整动力输出。",
      "action": [
        "Maintain current depth",
        "Move forward at constant speed",
        "Turn slightly right to align with the gap",
        "Navigate through the gap on the right side of the green structure",
        "Keep distance from the coral reefs on both sides",
        "Monitor and compensate for water current changes",
        "Continue straight after passing the structure"
      ]
    },
    "images": [
      "data/images/0097.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方那个发光的绿色目标物靠近，注意观察其表面的纹理变化。",
    "output": {
      "reasoning": "目标发光绿色物体位于视野正中心，路径清晰且无障碍物。将按照指令保持匀速直线前进，并在接近过程中聚焦于其表面复杂的脊状和颗粒状纹理变化。",
      "action": [
        "Go straight at constant speed",
        "Maintain heading towards the glowing target",
        "Slowly approach the object",
        "Focus on surface texture details",
        "Stop at optimal observation distance"
      ]
    },
    "images": [
      "data/images/0098.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从左侧立柱和右侧浮筒之间的空隙穿过，向着上方光源处探索，注意观察近处的动态气泡。",
    "output": {
      "reasoning": "图像显示左侧有一条垂直延伸的深色立柱，右侧是一个呈倾斜姿态的蓝白相间浮筒，两者之间形成了一个清晰的通道。上方区域有明亮的光源和大量上升的气泡。根据指令，机器人需要对准间隙匀速穿过，随后调整姿态向光源处爬升，过程中需实时监测气泡动态以判断水流干扰。",
      "action": [
        "Align with the gap between the left pillar and the right buoy",
        "Maintain constant speed forward",
        "Pass through the gap",
        "Monitor nearby dynamic bubbles",
        "Tilt camera and body upwards towards the light source",
        "Ascend towards the light",
        "Stop and hover for exploration"
      ]
    },
    "images": [
      "data/images/0099.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方中心立柱平稳推进，注意避开左侧的梯状结构，保持匀速。",
    "output": {
      "reasoning": "图像中心可见明显的立柱结构，左侧存在一个明显的黄色梯状障碍物。为了安全推进，机器人需要首先微调航向以确保与左侧梯子保持安全距离，随后保持恒定速度直线向中心立柱靠近。",
      "action": [
        "Adjust heading slightly to the right",
        "Maintain constant speed",
        "Go straight towards the central pillar",
        "Keep distance from the left ladder structure",
        "Approach the pillar",
        "Stop and stabilize"
      ]
    },
    "images": [
      "data/images/0100.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近蓝色圆柱体，注意避开底部的红色珊瑚，保持匀速前进。",
    "output": {
      "reasoning": "目标蓝色圆柱体位于视野正前方，但在其底座前方有一簇明显的红色刺状珊瑚。为了执行指令，机器人需要直线前进，在接近过程中向左微调路径以绕开底部的红色障碍物，同时保持稳定的推进速度，最后停在目标物旁。",
      "action": [
        "Go straight",
        "Steer slightly left to bypass the red coral",
        "Maintain steady speed",
        "Approach the blue cylinder",
        "Stop"
      ]
    },
    "images": [
      "data/images/0101.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "请向右前方航行，绕过红色的浮标并靠近最远处的深色柱子，注意保持匀速并观察周围珊瑚的分布。",
    "output": {
      "reasoning": "图像中心有一个橙红色的浮标状机器人，右侧远景中有一个巨大的深色圆柱形结构（目标柱子），左侧和底部布满了不同种类的珊瑚。根据指令，需要先向右前方移动以绕过红色浮标，随后直线向远处的深色柱子靠近。在整个移动过程中，需要保持稳定的航速并利用传感器/视觉系统记录左侧珊瑚的分布情况。",
      "action": [
        "Turn right-front",
        "Move forward at constant speed",
        "Bypass the red buoy from the right side",
        "Scan and observe coral distribution on the left",
        "Head towards the distant dark pillar",
        "Approach the target pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0102.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢移动，绕过左侧的大型珊瑚，抵近观察那辆沉没的汽车，注意保持匀速并留意周边的光影变化。",
    "output": {
      "reasoning": "图像显示目标沉没汽车位于右前方，左侧近处有大型扇形珊瑚障碍物。路径规划需先向右侧偏移以绕开左侧珊瑚，随后在保持匀速的同时向汽车抵近。环境光影复杂，需在移动过程中持续监测视觉反馈以确保稳定性。",
      "action": [
        "Start slow movement",
        "Turn right slightly",
        "Go straight to bypass left coral",
        "Maintain constant speed",
        "Monitor light and shadow changes",
        "Approach the sunken car",
        "Hover for observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0103.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前靠近那辆青色越野车，注意避开左前方的珊瑚丛，并重点观察车顶气泡的来源。",
    "output": {
      "reasoning": "目标青色越野车位于视野中心，左前方（图像左下角）有明显的珊瑚丛障碍物。为了执行指令，机器人需要先向右微调航向以绕开左侧珊瑚，随后保持匀速直线推进。在接近车辆后，需要调整姿态或视角，将重点对准车顶带有蓝色螺旋结构的喷气口，以观察气泡来源。",
      "action": [
        "Turn slightly right to clear the left coral bushes",
        "Go straight at a constant speed",
        "Approach the teal SUV",
        "Adjust camera pitch upward to focus on the roof",
        "Observe the blue spiral bubble source",
        "Hover and maintain observation"
      ]
    },
    "images": [
      "data/images/0104.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近前方的绿色立柱进行详细巡检，保持匀速并注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "图像显示目标绿色立柱位于机器人正前方，底部沙地上散布着多处珊瑚礁。为了执行巡检任务，机器人需要保持稳定的高度以避开底部的珊瑚礁障碍，并以恒定速度直线靠近立柱。到达近处后，需减速并进行垂直方向的移动，以便对立柱上的孔洞和结构细节进行全面巡检。",
      "action": [
        "Maintain altitude to clear seabed corals",
        "Go straight at constant speed",
        "Slow down when approaching the pillar",
        "Ascend and descend for vertical inspection",
        "Stop and hover for detailed observation"
      ]
    },
    "images": [
      "data/images/0105.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方航行，绕过中间的石柱去探索后方的沉船，注意保持匀速并避开底部的珊瑚礁。",
    "output": {
      "reasoning": "目标沉船（受损车辆）位于画面右后方，正前方有一根高大的石柱作为障碍物。底部散布着多处珊瑚礁。规划路径为：首先向右前方转向以避开石柱，同时保持恒定航速，并注意垂直高度控制以避开底部的珊瑚礁，最后绕过石柱接近并探索沉船。",
      "action": [
        "Maintain constant speed",
        "Turn right-forward",
        "Ascend slightly to clear bottom coral reefs",
        "Go straight to bypass the stone pillar",
        "Approach the sunken vehicle",
        "Slow down and hover for exploration",
        "Stop"
      ]
    },
    "images": [
      "data/images/0106.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前追踪前方的白色潜标，保持匀速并注意避开左侧的珊瑚礁群。",
    "output": {
      "reasoning": "图像中心上方可见白色的潜标（目标），正处于上方投射的光束中。左侧和左下方分布有明显的珊瑚礁群（障碍物）。为执行指令，机器人需要锁定前方目标，在前进过程中保持偏右的航线以避开左侧礁石，并维持稳定的推力输出。",
      "action": [
        "Lock on the white buoy ahead",
        "Adjust heading slightly to the right to avoid left reefs",
        "Move forward at a constant speed",
        "Maintain distance from the left coral formations",
        "Continue tracking the target"
      ]
    },
    "images": [
      "data/images/0107.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前穿过前方的岩石支柱，注意避开底部的珊瑚礁，稳步向深处探索。",
    "output": {
      "reasoning": "图像显示前方右侧有显著的岩石支柱，底部存在起伏的珊瑚礁和沙地。为了安全穿过支柱并向深处探索，机器人需要保持稳定的巡航高度以避开底部障碍物，对准支柱间的开阔水域，并以恒定速度向前推进。",
      "action": [
        "Maintain altitude to clear bottom coral",
        "Align with the gap between rock pillars",
        "Move forward at constant speed",
        "Pass through the pillars",
        "Proceed into the deep exploration zone",
        "Keep monitoring for obstacles"
      ]
    },
    "images": [
      "data/images/0108.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前穿过面前的管道，保持匀速并向中心的光源处探索。",
    "output": {
      "reasoning": "视觉图像显示机器人正处于一个具有同心圆纹理的管道状结构中，正前方中心有一个明亮的光源作为视觉引导点。路径目前清晰，没有明显的物理障碍物。根据指令，机器人需要对准中心光源，以稳定的速度直线前进，并保持在管道的中心轴线上以完成探索任务。",
      "action": [
        "Align orientation with the central light source",
        "Move forward at a constant speed",
        "Maintain trajectory along the center of the pipe",
        "Continue straight towards the light source",
        "Monitor for any changes in the pipe environment"
      ]
    },
    "images": [
      "data/images/0109.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，正对着前方的强光源中心持续向前推进，注意观察两侧管壁的实时状态。",
    "output": {
      "reasoning": "图像显示机器人正处于一个管状结构的中心，正前方有一个明显的强光源。路径清晰，无障碍物。为了执行指令，机器人需要锁定光源中心作为航向参考点，设定恒定推进速度，并在前进过程中实时监测两侧纹理复杂的管壁状态。",
      "action": [
        "Align with the center of the light source",
        "Set constant forward speed",
        "Go straight towards the light",
        "Observe pipe walls on both sides",
        "Maintain steady propulsion"
      ]
    },
    "images": [
      "data/images/0110.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方的发光绿色柱体靠近，注意观察底部的地形起伏。",
    "output": {
      "reasoning": "图像中心显示一个发光的绿色柱状目标，路径前方无明显障碍物，但底部存在复杂的波纹状地形起伏。为了执行指令，机器人需要保持稳定的动力输出以维持匀速，直接向前方目标推进，并同时调整视角或传感器增益以监测底部的地形细节。",
      "action": [
        "Maintain constant speed",
        "Go straight towards the glowing column",
        "Monitor bottom terrain undulations",
        "Approach the base of the column",
        "Stop"
      ]
    },
    "images": [
      "data/images/0111.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方发光的柱状目标抵近，保持匀速，注意避开左侧缠绕的线缆和右侧的网状障碍。",
    "output": {
      "reasoning": "视觉分析显示，正前方有一个发绿光的柱状目标。左侧视野存在大量缠绕的线缆，右侧地面覆盖着黄色网状障碍物。中间区域路径相对清晰。机器人需要保持航向对准发光柱，匀速直线推进，同时在移动过程中微调姿态以远离两侧的障碍物边界。",
      "action": [
        "Align with the glowing column",
        "Go straight at constant speed",
        "Maintain center path between cables and net",
        "Approach target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0112.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方调整航向，绕过红色的浮标并向深色洞穴入口探索，注意保持匀速并观察近处珊瑚礁的起伏。",
    "output": {
      "reasoning": "图像显示目标深色洞穴位于左前方，红色浮标位于机器人当前航向的左侧路径上。近处海床呈现明显的珊瑚礁起伏。规划路径需要先向左前方转向，在保持匀速以观察珊瑚礁的同时，绕过红色浮标，最后对准洞穴入口进行探索。",
      "action": [
        "Maintain steady speed",
        "Observe coral reef undulations",
        "Turn front-left",
        "Bypass the red buoy",
        "Approach the dark cave entrance",
        "Start exploration"
      ]
    },
    "images": [
      "data/images/0113.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从左侧红色浮标和右侧绿色珊瑚礁之间的缝隙穿过去，注意近处的障碍物距离。",
    "output": {
      "reasoning": "图像显示左侧近处有一个红绿相间的浮标状障碍物，右侧是一个巨大的绿色孔洞状珊瑚礁。两者之间存在明显的缝隙。由于红色障碍物距离镜头非常近，需要先进行微小的航向调整以对准缝隙中心，然后保持恒定速度直线穿过，过程中需严格监控两侧的间距。",
      "action": [
        "Adjust heading to align with the center of the gap",
        "Maintain constant speed",
        "Go straight",
        "Monitor distance to the red buoy on the left",
        "Monitor distance to the green coral on the right",
        "Pass through the gap",
        "Stop"
      ]
    },
    "images": [
      "data/images/0114.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过正前方的石柱去探索后方的遗迹区域，注意观察光影下的环境变化。",
    "output": {
      "reasoning": "图像正中央有一根巨大的石柱阻挡了直接向前的路径。为了到达后方的遗迹区域，需要先匀速接近石柱，然后选择从右侧较开阔的区域绕行，绕过障碍物后再修正航向进入后方建筑群，同时在移动过程中保持对光影变化区域的观察。",
      "action": [
        "Go straight at constant speed",
        "Turn right slightly to bypass the central pillar",
        "Go straight past the pillar",
        "Turn left to realign with the ruins area",
        "Go straight into the ruins",
        "Observe environment and light changes",
        "Maintain constant speed"
      ]
    },
    "images": [
      "data/images/0115.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，从两根石柱中间穿过，注意观察前方远处的遗迹结构。",
    "output": {
      "reasoning": "图像显示前方有两根巨大的开槽石柱，形成了一个天然的“门”状结构，远景处可见更深层的遗迹建筑。路径中心区域开阔且无障碍物，光照条件良好。为了执行指令，机器人需要先对准两柱之间的中心线，随后保持稳定速度直行穿过，并持续向前方远处的遗迹结构靠拢以进行观察。",
      "action": [
        "Align with the center of the two pillars",
        "Maintain constant speed",
        "Go straight",
        "Pass through the pillars",
        "Approach the distant ruins",
        "Slow down and observe",
        "Stop"
      ]
    },
    "images": [
      "data/images/0116.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，注意避开下方密集的珊瑚礁，向右前方开阔区域探索。",
    "output": {
      "reasoning": "图像显示下方和左侧分布有密集的珊瑚礁群，直接下降或左转存在碰撞风险。右前方区域光线较亮，且视觉上没有明显的障碍物阻挡，属于开阔水域。为了执行指令，机器人需要保持当前动力输出以维持匀速，同时进行右转机动，并确保高度足以越过下方的珊瑚边缘。",
      "action": [
        "Maintain constant speed forward",
        "Slightly increase altitude to clear bottom reefs",
        "Turn right towards the light-filled open area",
        "Go straight into the open water",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0117.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，注意避开下方两侧的珊瑚礁，密切观察前方水域的能见度变化。",
    "output": {
      "reasoning": "视觉分析显示，机器人当前处于一个珊瑚礁峡谷中，左下方和右下方均有明显的珊瑚礁群。前方路径相对开阔，上方有光束投射。为了安全执行指令，机器人需要维持在当前深度的中央航道，以匀速直线前进，同时实时监测前方水域的光照和浑浊度变化，确保避障安全。",
      "action": [
        "Maintain current depth",
        "Go straight at constant speed",
        "Stay centered between left and right coral reefs",
        "Monitor forward visibility and light intensity",
        "Keep moving forward"
      ]
    },
    "images": [
      "data/images/0118.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿右侧舱壁向前推进，注意观察左侧设备状态并避开上方气泡区域。",
    "output": {
      "reasoning": "视觉分析显示，图像右侧为结构清晰的弧形舱壁，左侧为长条状管路设备，上方伴有大量上升气泡。为执行指令，机器人需要向右微调以靠近舱壁，同时保持当前深度以避开上方气泡造成的视觉干扰和浮力波动，随后以恒定速度向前推进并保持对左侧设备的监测。",
      "action": [
        "Shift right to align with the bulkhead",
        "Maintain current depth to avoid bubbles above",
        "Move forward at constant speed",
        "Monitor equipment on the left",
        "Keep straight along the right wall"
      ]
    },
    "images": [
      "data/images/0119.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着主管道匀速向前推进，密切关注前方喷涌出的烟雾区域并保持安全距离，穿过该处后继续向深水区探索。",
    "output": {
      "reasoning": "图像显示一条巨大的主管道向画面深处延伸，中段位置有明显的白色和蓝色烟雾（或气泡）喷涌。为了执行指令，机器人需要首先对准管道方向匀速前进，在接近烟雾喷发点时降低速度以进行密切监测，并调整姿态保持安全横向距离，待越过该危险区域后，再加速向深水区继续探索。",
      "action": [
        "Align with the main pipe",
        "Go straight at constant speed",
        "Slow down when approaching the smoke plume",
        "Monitor the smoke area and maintain safe distance",
        "Pass the smoke area carefully",
        "Accelerate and go straight",
        "Explore towards deep water"
      ]
    },
    "images": [
      "data/images/0120.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方调整航向，穿过珊瑚群中的缝隙，注意保持匀速并留意近处珊瑚的边缘。",
    "output": {
      "reasoning": "图像显示正前方和左侧珊瑚群较为密集，右前方光照区域存在明显的珊瑚间隙。为了安全穿过，机器人需要先向右微调航向对准缝隙，随后以恒定速度推进，并利用视觉反馈实时监控两侧珊瑚边缘的距离，防止发生擦碰。",
      "action": [
        "Turn right slightly",
        "Align with the gap",
        "Go straight at constant speed",
        "Monitor coral edges",
        "Pass through the gap",
        "Maintain stability"
      ]
    },
    "images": [
      "data/images/0121.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前缓慢推进，近距离观察前方发光的珊瑚群，注意避开近处的结构边缘。",
    "output": {
      "reasoning": "图像显示前方分布着大量发光的漏斗状珊瑚结构，色彩鲜艳且边缘锐利。当前机器人距离前景珊瑚的边缘非常近，前进时需微调高度或俯仰角以确保避开这些结构边缘。任务要求匀速缓慢推进并近距离观察，因此动作序列应包含避障调整、稳速前进及最终的观察悬停。",
      "action": [
        "Move forward slowly at constant speed",
        "Adjust altitude to clear the nearest coral edges",
        "Approach the glowing coral cluster ahead",
        "Maintain steady hover for close-up observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0122.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "匀速向前推进，注意避开右侧的巨大礁石，向前方光影交汇的开阔区域进行探索。",
    "output": {
      "reasoning": "图像显示右侧有一座巨大的黑色礁石（暗色阴影区域），而前方中心至左上方区域有明显的光束透射，形成光影交汇的开阔水域。为了避开右侧障碍物并到达目标区域，机器人需要保持向前的动力，同时航向略微向左偏移以确保与礁石保持安全距离，最后直行进入光照充足的开阔地带。",
      "action": [
        "Move forward at constant speed",
        "Steer slightly left to keep distance from the reef on the right",
        "Go straight towards the light rays ahead",
        "Enter the open water area",
        "Maintain constant speed for exploration"
      ]
    },
    "images": [
      "data/images/0123.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方加速，跟随远处正在移动的潜航器，注意避开底部的珊瑚礁并保持匀速航行。",
    "output": {
      "reasoning": "目标潜航器位于视野左上方远处，正在移动。路径左下方和右下方存在珊瑚礁障碍物。需要先向左前方转向并加速以缩短距离，同时保持高度以避开底部的珊瑚礁，最后调整动力输出进入匀速跟随模式。",
      "action": [
        "Turn left-front",
        "Accelerate",
        "Ascend slightly to avoid bottom reefs",
        "Track the distant submersible",
        "Maintain steady speed",
        "Keep safe distance from obstacles"
      ]
    },
    "images": [
      "data/images/0124.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从两根石柱之间穿过，向前方光影汇聚的深处探索，注意避开中间的隆起岩石。",
    "output": {
      "reasoning": "图像显示左右两侧各有一根高大的石柱，正前方有一块巨大的隆起岩石，上方有明显的光影汇聚。为了执行指令，机器人需要保持匀速，先向左侧或右侧微调以绕过中间的岩石障碍，随后穿过两根石柱形成的‘门户’，最后对准光影汇聚的深处直线航行。",
      "action": [
        "Maintain constant speed forward",
        "Steer slightly left to bypass the central rock",
        "Pass between the two stone pillars",
        "Align with the converging light rays ahead",
        "Go straight into the depths",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0125.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "注意避开右侧的大型条纹珊瑚，保持匀速从左侧柱子旁穿过，向前方光影汇聚的深水区探索。",
    "output": {
      "reasoning": "图像右下角存在一个明显的大型条纹珊瑚（障碍物），左侧有一根高大的纹理柱状物（地标）。为了执行指令，机器人需要向左侧微调以避开右侧珊瑚，随后保持匀速通过左侧柱子，最后对准前方光影汇聚的开阔深水区前进。",
      "action": [
        "Turn slightly left to avoid the striped coral",
        "Maintain constant speed",
        "Pass by the left pillar",
        "Go straight towards the light rays",
        "Explore the deep water area",
        "Stop"
      ]
    },
    "images": [
      "data/images/0126.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过左侧的柱状珊瑚，向中心巨大的杯状珊瑚靠近。",
    "output": {
      "reasoning": "图像左前方有一株显著的柱状珊瑚，中心位置是一个巨大的杯状珊瑚。为了执行指令，机器人需要先保持匀速前进，同时向右侧微调方向以避开左侧的柱状珊瑚障碍，随后重新对准中心目标并持续靠近。",
      "action": [
        "Go straight at constant speed",
        "Turn right slightly to bypass the columnar coral",
        "Align with the central cup coral",
        "Go straight towards target",
        "Close to target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0127.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前缓慢靠近中心发光的绿色核心，注意观察周围复杂褶皱结构的间隙。",
    "output": {
      "reasoning": "图像中心显示一个发光的绿色核心，位于复杂的有机褶皱结构深处。目标位置明确，但周围环境存在大量起伏的脊状障碍物。为了安全靠近，机器人需要保持稳定的动力输出，对准中心开口缓慢推进，同时利用视觉传感器实时监测两侧褶皱的间隙，确保不发生碰撞并观察结构细节。",
      "action": [
        "Align with the central glowing green core",
        "Move forward slowly at a constant speed",
        "Monitor the clearance of the surrounding folded structures",
        "Scan the gaps between the ridges while advancing",
        "Maintain stable depth and orientation",
        "Approach the green core closely",
        "Stop and hover for observation"
      ]
    },
    "images": [
      "data/images/0128.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方绕过蓝色网状立柱，抵近观察后方的横向波纹管道，注意保持匀速并留意底部的障碍物。",
    "output": {
      "reasoning": "图像中蓝色网状立柱位于正前方，横向波纹管道位于其后方。底部绿色区域散布有黑色的小型设备（障碍物）。任务要求向右前方绕过立柱，因此需要先向右调整航向，避开立柱及底部的黑色障碍物，随后转向并接近后方的波纹管道进行抵近观察。",
      "action": [
        "Turn right",
        "Go straight to bypass the blue pillar",
        "Monitor bottom obstacles",
        "Turn left to face the corrugated pipe",
        "Go straight at steady speed",
        "Close to the corrugated pipe",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0129.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢绕过主管道，注意保持与管道壁的安全距离，向光亮区域继续探索。",
    "output": {
      "reasoning": "图像显示主管道占据了左侧及中部视野，右侧存在垂直的支撑结构。目标光亮区域位于右前方远端。为了执行绕行指令，机器人需要先向右偏航以避开主管道的直接路径，随后在保持与左侧管壁安全距离的同时缓慢前行，绕过右侧的垂直支架，最终对准光源方向进行探索。",
      "action": [
        "Turn right slightly",
        "Move forward slowly",
        "Maintain safe distance from the left pipe wall",
        "Bypass the vertical support pipe on the right",
        "Adjust heading towards the bright area",
        "Go straight to explore"
      ]
    },
    "images": [
      "data/images/0130.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过中间的深色珊瑚柱，注意观察右侧岩石区的动态变化。",
    "output": {
      "reasoning": "图像中心位置有一个巨大的深色圆柱状珊瑚结构，构成了前进路径上的主要障碍。右侧分布有岩石群，且有一条小鱼在游动，符合指令中‘观察动态变化’的要求。规划路径为：首先保持匀速前进，随后向右侧微调方向以绕过中心珊瑚柱，同时将视觉重心偏向右侧岩石区进行监测。",
      "action": [
        "Move forward at constant speed",
        "Steer slightly right to bypass the central dark coral pillar",
        "Maintain visual focus on the right rock area",
        "Continue straight once clear of the obstacle",
        "Monitor for dynamic movements in the rock zone"
      ]
    },
    "images": [
      "data/images/0131.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向前靠近中心的大型异形结构进行探测，注意保持匀速，同时观察其内部孔洞的动态变化。",
    "output": {
      "reasoning": "图像中心是一个巨大的、具有多个孔洞的异形结构（疑似大型海绵或珊瑚礁），目标位于机器人正前方。路径两侧有岩壁，但中心通道开阔。为了执行探测任务并观察孔洞动态，机器人需要对准目标中心，以恒定低速缓慢推进，并在接近过程中保持视觉锁定以捕捉细节变化。",
      "action": [
        "Align with the central structure",
        "Move forward slowly",
        "Maintain constant speed",
        "Focus camera on internal holes",
        "Stabilize for observation",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0132.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左绕过正前方的巨型贝壳状珊瑚，保持匀速，注意避开底部的海胆类生物。",
    "output": {
      "reasoning": "图像正前方占据视野的是一个巨大的贝壳状珊瑚，其底部及周围海床上散布着多个带刺的海胆类生物。为了安全绕行，机器人需要先向左调整航向，同时注意保持垂直高度或精确路径以避开底部的海胆，随后以匀速从左侧空旷区域绕过目标珊瑚。",
      "action": [
        "Turn left",
        "Maintain altitude to clear sea urchins on the seabed",
        "Go straight at constant speed",
        "Arc left to bypass the giant coral",
        "Monitor distance from the coral and bottom organisms",
        "Continue forward once cleared"
      ]
    },
    "images": [
      "data/images/0133.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方绕过正前方的大型绿色珊瑚，保持匀速并注意观察右侧珊瑚群的动态变化。",
    "output": {
      "reasoning": "图像正前方是一个巨大的绿色褶皱状珊瑚，占据了主要航道。右侧有一个圆形的褐色珊瑚及底部的珊瑚群。根据指令，需要先向右侧偏转以避开正前方的绿色珊瑚，随后以恒定速度向右前方航行，同时将视觉重点放在右侧的珊瑚区域以进行动态观察。",
      "action": [
        "Turn right",
        "Maintain constant speed",
        "Go straight to the right-front",
        "Observe right coral group",
        "Keep moving forward"
      ]
    },
    "images": [
      "data/images/0134.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，穿过前方石柱间的空隙，注意观察周围光影的变化。",
    "output": {
      "reasoning": "图像显示前方有三根巨大的石柱，左侧石柱与中心石柱之间存在明显的通行间隙。水面透射的波纹光影和侧方的绿色光束构成了复杂的光影环境。任务要求匀速穿过间隙并进行观察，因此需要调整航向对准空隙，保持稳定动力输出，并在移动过程中持续监测视觉特征的变化。",
      "action": [
        "Maintain constant speed",
        "Adjust heading to the gap between left and center pillars",
        "Go straight",
        "Pass through the gap",
        "Monitor light and shadow changes",
        "Keep cruising"
      ]
    },
    "images": [
      "data/images/0135.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向左前方绕过这根巨大的石柱，去探索后方较远处的遗迹。",
    "output": {
      "reasoning": "图像中心偏右位置有一根巨大的石柱，构成了主要的路径障碍。目标遗迹位于石柱后方的远端背景中。为了执行指令，机器人需要先向左侧偏移以避开石柱，随后保持匀速向左前方行进，待越过石柱侧面后，再调整航向指向远处的遗迹区域。",
      "action": [
        "Maintain constant speed",
        "Turn left",
        "Go straight to bypass the pillar",
        "Adjust heading towards the distant ruins",
        "Move forward",
        "Explore the ruins"
      ]
    },
    "images": [
      "data/images/0136.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方调整航向，绕过近处的绿色条纹障碍物，保持匀速向深水区探索，注意观察左侧珊瑚礁的动态变化。",
    "output": {
      "reasoning": "图像显示，一个巨大的绿色条纹长条形物体（障碍物）占据了画面中心及右侧近景，阻挡了直行路径。左侧分布有珊瑚礁结构，背景为光线渐暗的深水区。为执行指令，机器人需先向左转弯以避开障碍物，随后斜向左前方切入，在保持对左侧珊瑚礁观察的同时，向深水暗区推进。",
      "action": [
        "Turn left",
        "Go straight to bypass the green striped obstacle",
        "Adjust heading to front-left",
        "Maintain constant speed",
        "Monitor the left coral reef",
        "Move towards the deep water area",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0137.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，避开右侧的珊瑚群，并注意上方大型生物的动向。",
    "output": {
      "reasoning": "图像显示机器人正处于光影交错的水下环境。右侧下方分布有明显的彩色珊瑚群，上方有一只大型深色生物正在游动。为了在保持匀速前进的同时避开右侧障碍并监测上方威胁，机器人需要向左侧微调航向以拉开与珊瑚的距离，并保持传感器对上方的持续锁定。",
      "action": [
        "Maintain constant speed forward",
        "Steer slightly left to avoid the coral cluster on the right",
        "Monitor the movement of the large creature above",
        "Keep a safe distance from the seabed",
        "Continue straight path"
      ]
    },
    "images": [
      "data/images/0138.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿管道右侧轨道匀速向前航行，注意观察管道接口状态，保持航向稳定。",
    "output": {
      "reasoning": "图像显示右侧有一条大型管道，下方有两条平行的金属轨道。机器人当前视角位于左侧轨道附近，需向右侧微调以对准管道下方的右侧轨道。随后需保持匀速前进，同时视觉传感器需重点监测右侧管道的环形接口区域，并实时修正航向以确保在轨道上方稳定航行。",
      "action": [
        "Shift right to align with the right track",
        "Set constant speed forward",
        "Focus on pipeline joints",
        "Maintain stable heading",
        "Go straight along the track"
      ]
    },
    "images": [
      "data/images/0139.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着前方的蓝色圆柱体匀速向前推进，注意避开底部的缆线并观察远端的光影变化。",
    "output": {
      "reasoning": "图像中心是一个长条形的蓝色分段圆柱体，向远端延伸。底部地板上有明显的黑色平行缆线/轨道。远端上方有强烈的外部光束射入。为了安全执行指令，机器人需要首先对准圆柱体路径，保持一定高度以避开底部的缆线障碍，随后匀速向前推进，并持续监测远端光影的强度变化以判断环境边界。",
      "action": [
        "Align with the blue cylinder",
        "Ascend slightly to clear floor cables",
        "Move forward at constant speed",
        "Monitor light rays at the far end",
        "Maintain steady heading along the pipe"
      ]
    },
    "images": [
      "data/images/0140.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前探索，注意避开右侧的红色浮标，同时留意近处的环境动态。",
    "output": {
      "reasoning": "图像显示右前方有一个明显的红色球形障碍物（红色浮标），而前方及左侧区域相对开阔。为了执行‘匀速向前’并‘避开右侧浮标’的指令，机器人需要启动前进动力，同时向左侧进行小角度转向以确保安全间距，并持续观察近处复杂的珊瑚礁地形以防碰撞。",
      "action": [
        "Move forward at constant speed",
        "Steer slightly left to avoid the red buoy on the right",
        "Maintain safe distance from right-side obstacles",
        "Continue forward exploration",
        "Monitor nearby coral environment"
      ]
    },
    "images": [
      "data/images/0141.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，注意避开近处隆起的珊瑚礁，向红色的浮标目标靠拢。",
    "output": {
      "reasoning": "红色浮标位于视野正前方偏上位置，而近处左右两侧及下方分布着大型隆起的珊瑚礁。为了避开这些障碍物并向目标靠拢，机器人需要保持稳定航速，微调航向从珊瑚礁之间的空隙穿过，并适当提升高度以接近悬浮的浮标。",
      "action": [
        "Maintain constant speed",
        "Steer slightly left to avoid the large reef on the right",
        "Go straight",
        "Ascend slightly to align with the buoy height",
        "Approach the red buoy",
        "Stop"
      ]
    },
    "images": [
      "data/images/0142.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过前方珊瑚群之间的开阔水域，注意观察近处绿色珊瑚的避障距离。",
    "output": {
      "reasoning": "图像显示前方中央区域有明显的开阔水域（光束汇聚处），两侧分布着深色珊瑚礁。近处右侧有一个显著的绿色球状珊瑚，左上方也有类似的绿色结构。为了安全穿过，机器人需要微调航向对准中央通道，在保持匀速前进的同时，重点监测与右侧近处绿色珊瑚的侧向距离，确保避障安全。",
      "action": [
        "Align with the central open water",
        "Maintain constant speed",
        "Monitor distance to the green coral on the right",
        "Go straight through the gap",
        "Keep moving forward"
      ]
    },
    "images": [
      "data/images/0143.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过两侧珊瑚礁，靠近正前方发光的垂直目标进行观察，注意监测底部光照区域的动态变化。",
    "output": {
      "reasoning": "图像显示正前方有一个垂直的、带有气泡上升的结构化目标，其底部发出强烈的绿色光束。左右两侧分布着高大的珊瑚礁，形成了一个天然通道。根据指令，机器人需要保持航向从通道中心穿过，在接近目标的过程中，需重点关注底部绿色光斑区域的视觉反馈。",
      "action": [
        "Maintain constant speed",
        "Go straight through the reef passage",
        "Monitor the bottom illuminated area",
        "Approach the vertical glowing target",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0144.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近正前方的石柱进行抵近观察，注意保持匀速并留意底部的障碍物。",
    "output": {
      "reasoning": "目标石柱位于视野正前方，距离适中。底部左侧和右侧散布有岩石和珊瑚礁，中间路径相对平坦。为完成抵近观察，机器人需保持直线匀速前进，同时实时监测底部障碍物以防碰撞，最后在安全距离处停止。",
      "action": [
        "Go straight at constant speed",
        "Monitor bottom obstacles",
        "Approach the stone pillar",
        "Slow down near the target",
        "Stop for close observation"
      ]
    },
    "images": [
      "data/images/0145.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前移动，穿过这两根石柱靠近中心的绿色目标物，注意保持匀速并避开地面的凹槽。",
    "output": {
      "reasoning": "视觉分析显示，目标绿色物体位于前方两根巨大石柱的中心位置。然而，正前方的地面上有一个明显的长方形深色凹槽障碍。为了安全到达目标，机器人需要首先保持匀速前进，向右侧（或左侧）小幅转向以绕过该凹槽，随后重新修正航向，穿过两根石柱之间的通道，最终接近目标物。",
      "action": [
        "Move forward at constant speed",
        "Steer right to bypass the floor groove",
        "Adjust heading to center between columns",
        "Pass through the stone columns",
        "Approach the green target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0146.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过近处的珊瑚礁，向那艘绿色的潜艇残骸靠近并进行抵近观察。",
    "output": {
      "reasoning": "图像前景有巨大的深色珊瑚礁阻挡了直接前进的路径，目标绿色潜艇残骸位于中景。为了执行指令，机器人需要先保持匀速并向左或向右侧方移动以绕过近处的珊瑚礁障碍，随后重新对准潜艇残骸并直线靠近，最后减速悬停以进行抵近观察。",
      "action": [
        "Maintain constant speed forward",
        "Steer left to bypass the foreground coral reefs",
        "Adjust heading towards the green submarine wreckage",
        "Go straight towards target",
        "Approach the wreckage",
        "Slow down for close-up observation",
        "Stop and hover"
      ]
    },
    "images": [
      "data/images/0147.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过前方两座大型礁石之间的通道，继续向深处探索，请保持匀速并注意右侧障碍物的距离。",
    "output": {
      "reasoning": "图像显示机器人正前方有两个显著的深色礁石结构，形成了一个天然的狭窄通道。右侧礁石体积较大且边缘距离机器人较近，需要重点关注。通道尽头通向更深、更暗的水域。规划逻辑为：对准通道中心，保持稳定速度直线推进，同时动态监测右侧避障距离，穿过狭窄段后继续下潜探索。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Monitor distance to the right reef",
        "Pass through the channel between reefs",
        "Continue diving deeper",
        "Explore the deep area"
      ]
    },
    "images": [
      "data/images/0148.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方光源处匀速推进，注意避开左侧的管线障碍，保持航向稳定。",
    "output": {
      "reasoning": "图像显示正前方偏上位置有一个明亮的光源作为目标点。左侧区域有明显的绿色环绕式管线结构，构成潜在障碍。为了执行指令，机器人需要首先微调航向以远离左侧管线，随后锁定光源位置，以稳定的速度直线推进，并持续修正姿态以抵消环境干扰。",
      "action": [
        "Adjust heading slightly to the right",
        "Go straight towards the light source",
        "Maintain steady speed",
        "Keep safe distance from left pipeline",
        "Stabilize course",
        "Approach light source",
        "Stop"
      ]
    },
    "images": [
      "data/images/0149.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿着正前方的黄色立柱缓慢下降进行抵近观察，注意避开左侧的珊瑚礁群。",
    "output": {
      "reasoning": "视觉感知显示，正前方有一根明显的黄色分段立柱，左侧分布着大面积的珊瑚礁群。为了安全执行指令，机器人需要首先向右微调位置以确保与左侧珊瑚礁保持安全间距，随后靠近立柱，并以恒定速度垂直下降进行细节观察。",
      "action": [
        "Adjust position slightly to the right to clear left corals",
        "Approach the yellow pillar",
        "Maintain constant speed",
        "Descend slowly along the pillar",
        "Perform close observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0150.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，缓缓向正前方发光的中心枢纽靠近，注意避开周围缠绕的绿色管线。",
    "output": {
      "reasoning": "视觉分析显示，目标发光中心枢纽位于视野正上方，呈现明亮的绿色核心。周围布满了粗壮且具有纹理的绿色管线，这些管线在空间中交错，构成了主要的障碍。为了安全接近目标，机器人需要保持低速匀速前进，并在上升接近的过程中，时刻监测并避开下方和侧面缠绕的管线结构。",
      "action": [
        "Move forward slowly",
        "Maintain constant speed",
        "Adjust pitch upwards to align with the hub",
        "Steer clear of the surrounding green pipelines",
        "Approach the central glowing hub",
        "Stop at a safe distance"
      ]
    },
    "images": [
      "data/images/0151.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向前靠近中心的红色环状目标，保持匀速并注意上方绿色螺旋结构的动态变化。",
    "output": {
      "reasoning": "图像中心是一个带有红色发光环的球形目标，其上方连接着明亮的绿色双螺旋结构。目标位于机器人正前方，路径相对清晰。根据指令，需要通过微调对准目标中心，以低速匀速向前推进，并在移动过程中保持对上方绿色螺旋结构动态的视觉监测，以防环境突变。",
      "action": [
        "Align with the central red ring target",
        "Set speed to slow and constant",
        "Move forward slowly",
        "Monitor the green helical structure above",
        "Approach target",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0152.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，避开右下方的红色管状障碍物，并靠近绿色的目标模块。",
    "output": {
      "reasoning": "图像显示绿色的目标模块位于前方中心位置，而巨大的红色管状障碍物横跨在右下方近景处。为了执行指令，机器人需要首先保持向前移动，同时向左微调航向以确保与右侧红色管道保持安全距离，随后直接驶向绿色模块并在靠近后停止。",
      "action": [
        "Go straight",
        "Steer slightly left to avoid the red pipe",
        "Move forward towards the green module",
        "Slow down",
        "Close to target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0153.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近光束中的红色目标，保持匀速并注意避开右侧的珊瑚礁。",
    "output": {
      "reasoning": "目标红色潜水器位于正前方的光束中，距离适中。右侧存在大型珊瑚礁结构，构成潜在碰撞风险。任务规划需要机器人在保持对准目标的同时，向左侧略微偏移以确保与右侧珊瑚礁的安全间距，随后稳步推进直至靠近目标。",
      "action": [
        "Align with the red target in the light beam",
        "Move forward at a steady speed",
        "Monitor distance to the coral reef on the right",
        "Steer slightly left to maintain safe clearance",
        "Continue approaching the red target",
        "Slow down and stop"
      ]
    },
    "images": [
      "data/images/0154.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近观察正前方的石柱，注意避开底部的岩石堆。",
    "output": {
      "reasoning": "图像中心正前方有一根巨大的石柱，其底部周围散布着多块不规则的岩石堆。为了安全靠近并观察石柱，机器人需要保持匀速直线前进，同时注意维持一定的高度以避开底部的岩石障碍，最后在接近石柱表面时减速并停止。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Keep altitude to avoid bottom rocks",
        "Approach the stone pillar",
        "Slow down",
        "Stop for observation"
      ]
    },
    "images": [
      "data/images/0155.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从右侧带孔柱子和左侧圆形珊瑚之间穿过，注意观察底部障碍物的动态变化。",
    "output": {
      "reasoning": "图像显示正前方有一个明显的通道，右侧是带孔的垂直柱状结构，左侧是一个巨大的圆盘状珊瑚。路径中央区域开阔，适合穿行。机器人需要对准两者之间的间隙，保持匀速直线航行，同时视觉系统需重点监控底部散落的生物和阴影，以防其位置发生动态变化。",
      "action": [
        "Align with the center gap",
        "Move forward at constant speed",
        "Monitor bottom obstacles for movement",
        "Pass between the pillar and the coral",
        "Maintain steady course",
        "Stop after passing"
      ]
    },
    "images": [
      "data/images/0156.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向左前方避开右侧的珊瑚礁，注意观察近处的地形变化。",
    "output": {
      "reasoning": "图像显示机器人右侧及右下方分布有密集的珊瑚礁和岩石结构，而左前方区域相对开阔。为避开右侧障碍物并观察地形，机器人需要向左进行航向修正，同时保持稳定的推进速度，并下调视角或加强对底部沙地与岩石交界处的监测。",
      "action": [
        "Maintain constant speed",
        "Turn left",
        "Move forward-left",
        "Keep distance from right coral reef",
        "Scan seafloor terrain",
        "Continue steady cruise"
      ]
    },
    "images": [
      "data/images/0157.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，一直向前穿过面前的岩石峡谷，注意观察左右两侧的障碍物边缘。",
    "output": {
      "reasoning": "图像显示机器人正处于一个水下峡谷的入口处，左右两侧分布着明显的岩石障碍物，上方有光线射入。路径中心相对开阔，适合直线穿行。为了执行指令，机器人需要保持当前航向，匀速前进，并实时监测两侧岩石边缘的距离以确保安全间距。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Monitor left obstacle edge",
        "Monitor right obstacle edge",
        "Navigate through the canyon",
        "Keep forward"
      ]
    },
    "images": [
      "data/images/0158.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前靠近正前方的绿色立柱，注意避开左侧的橙色框架。",
    "output": {
      "reasoning": "图像显示正前方有一个绿色的立柱结构（目标），左侧近处有橙色的框架结构（障碍物）。为了安全靠近目标，机器人需要首先微调航向以确保与左侧障碍物保持安全距离，随后保持匀速直线前进，直至接近绿色立柱。",
      "action": [
        "Adjust heading slightly to the right",
        "Go straight at constant speed",
        "Keep distance from the orange frame on the left",
        "Approach the green pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0159.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着右侧的绿色管道匀速向前推进，注意规避左侧的珊瑚礁群。",
    "output": {
      "reasoning": "图像显示右侧有一条明显的绿色管道，左侧分布着密集的珊瑚礁。为了执行指令，机器人需要将管道作为导航参照物，保持在管道的右侧路径上，同时确保与左侧珊瑚礁群保持安全距离。由于管道在视野中呈现向右下方的弧度，机器人需要不断微调航向以贴合管道走向并保持匀速推进。",
      "action": [
        "Align with the green pipe",
        "Keep distance from the coral reefs on the left",
        "Go straight at a constant speed",
        "Adjust heading to follow the pipe curvature",
        "Maintain steady forward movement"
      ]
    },
    "images": [
      "data/images/0160.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过前方的条纹珊瑚群，向着上方透光的方向探索，注意观察近处动态。",
    "output": {
      "reasoning": "图像显示机器人正处于一个半封闭的水下空间，前方分布着明显的条纹状珊瑚结构，上方有一个明亮的圆形透光开口。根据指令，机器人需要先匀速穿过这些珊瑚障碍，同时监测近处游动的鱼类（动态目标），最后改变航向向上方的光源处攀升探索。",
      "action": [
        "Go straight at constant speed",
        "Navigate through the striped coral clusters",
        "Monitor nearby moving fish",
        "Pitch up towards the light source",
        "Ascend to the surface opening",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0161.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过前方的光束区域并向最远处的支柱靠拢，注意避开左侧的珊瑚群。",
    "output": {
      "reasoning": "图像左侧前景有明显的珊瑚群，需要避开；前方有数条垂直的光束和支柱。最远处的支柱位于画面中心偏右的深处。规划路径为：首先保持航向并微调向右以远离左侧珊瑚，随后穿过光束区域，最后锁定远端支柱并接近。",
      "action": [
        "Move forward at constant speed",
        "Steer slightly right to avoid the left coral group",
        "Pass through the light beam area",
        "Head towards the furthest pillar in the background",
        "Close to the target pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0162.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过左侧巨大的黄色脑珊瑚，向远处的深色平台靠近，注意观察近处珊瑚的分布。",
    "output": {
      "reasoning": "图像左前方有一个巨大的黄色脑珊瑚（障碍物），正前方中景处有绿色和灰色的珊瑚群，远方背景中可见一个深色的人造平台（目标）。路径规划需要先向右侧微调以避开左侧的脑珊瑚，同时保持平稳速度以便传感器记录近处多样的珊瑚分布，最后直线向深色平台靠拢。",
      "action": [
        "Go straight at constant speed",
        "Steer slightly right to bypass the yellow brain coral",
        "Maintain steady pace to observe nearby corals",
        "Go straight towards the dark platform",
        "Approach the platform",
        "Stop"
      ]
    },
    "images": [
      "data/images/0163.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过前方的珊瑚礁群，注意避开右侧巨大的脑纹珊瑚，并持续观察深水区的环境变化。",
    "output": {
      "reasoning": "图像显示前方分布着密集的珊瑚礁，其中右侧有一株体积较大的绿色脑纹珊瑚。为了执行指令，机器人需要保持稳定的推进速度，航向需稍微向左偏移以避开右侧的脑纹珊瑚，随后穿过珊瑚群之间的缝隙，最终进入背景中光线透射的深水区域进行观察。",
      "action": [
        "Maintain steady speed",
        "Go straight",
        "Slightly steer left to avoid the brain coral on the right",
        "Pass through the coral reef gap",
        "Monitor deep water environment",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0164.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过前方光影交错的区域，注意避开底部的珊瑚并留意左侧柱子的间距。",
    "output": {
      "reasoning": "图像显示左侧有一根巨大的结构柱，底部布满起伏的珊瑚礁，正前方是阳光透射形成的光影区域。为安全执行指令，机器人需要保持稳定的推进速度，航向略微向右偏移以与左侧柱子保持安全间距，同时控制高度以避开底部的珊瑚礁，最终穿过明亮的光影区。",
      "action": [
        "Maintain constant speed",
        "Adjust heading slightly right to keep distance from the left pillar",
        "Maintain altitude to clear bottom coral",
        "Go straight through the light and shadow area",
        "Monitor clearance from the left pillar",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0165.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方带有网格纹理的柱子靠近，保持匀速并注意观察底部的珊瑚礁。",
    "output": {
      "reasoning": "图像正前方有一根具有明显网格纹理的垂直柱子，其底部连接着复杂的珊瑚礁结构。目前机器人正对目标，路径清晰。为了执行指令，机器人需要保持当前航向直线前进，同时需控制动力输出以维持匀速。由于需要观察底部的珊瑚礁，在接近过程中应保持视觉传感器对下方的覆盖，并在接近柱子基座时减速以确保安全。",
      "action": [
        "Align with the textured pillar",
        "Go straight at constant speed",
        "Monitor the bottom coral reefs",
        "Decelerate when approaching the base",
        "Stop at a safe distance"
      ]
    },
    "images": [
      "data/images/0166.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "绕过左侧的珊瑚礁，向前方光照充足的开阔区域匀速推进，注意观察底部地形的动态变化。",
    "output": {
      "reasoning": "图像左侧前景有明显的珊瑚礁/岩石阴影，构成主要障碍。前方及上方有明显的透射光束，指示了光照充足的开阔区域。为了执行指令，机器人需要先向右微调航向以安全绕过左侧珊瑚礁，随后对准光照区域匀速推进，并在移动过程中保持对下方海床地形的传感器监测。",
      "action": [
        "Turn slightly right to bypass the coral reef",
        "Go straight towards the well-lit area",
        "Maintain steady speed",
        "Monitor bottom terrain dynamics",
        "Proceed to the open water"
      ]
    },
    "images": [
      "data/images/0167.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，注意避开下方的珊瑚礁，向右前方的开阔水域探索。",
    "output": {
      "reasoning": "图像显示机器人下方存在明显的珊瑚礁结构，而右前方区域光线较亮且无障碍物，属于开阔水域。为了执行指令，机器人需要首先保持当前高度以避开下方的珊瑚礁，随后向右转弯调整航向，最后匀速驶向目标开阔区域进行探索。",
      "action": [
        "Maintain current depth",
        "Maintain constant speed",
        "Turn right towards open water",
        "Go straight",
        "Explore target area",
        "Monitor surroundings"
      ]
    },
    "images": [
      "data/images/0168.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过前方的光影区域，注意避开左侧突出的珊瑚礁并观察右侧的生物动态。",
    "output": {
      "reasoning": "图像显示左侧有大面积突出的珊瑚礁障碍物，前方中心区域有明显的阳光透射（光影区域），右侧及下方分布着鱼类和类似海龟的生物。为了安全穿过并完成观察任务，机器人需要保持航向微偏右，以避开左侧礁石，同时在匀速推进过程中对右侧视野进行扫描。",
      "action": [
        "Move forward at constant speed",
        "Steer slightly right to avoid left coral reef",
        "Enter the light and shadow area",
        "Scan right side for biological activity",
        "Maintain steady forward motion",
        "Pass through the passage",
        "Stop"
      ]
    },
    "images": [
      "data/images/0169.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过两侧的珊瑚礁，向前方光影中的白色气泡区域靠近，注意观察近处沙地的地形起伏。",
    "output": {
      "reasoning": "图像显示机器人正前方是一条由两侧珊瑚礁构成的天然通道。正前方背景中有垂直上升的白色气泡，且伴有明显的水下光影（耶稣光）。近处沙地呈现出明显的波纹状地形起伏。路径中心区域开阔，无直接障碍物。规划逻辑为：保持航向穿过两侧礁石，同时向下监测沙地地形，最终抵达气泡区域。",
      "action": [
        "Go straight at constant speed",
        "Pass through the gap between coral reefs",
        "Monitor the sandy terrain undulations",
        "Approach the white bubble area in the light",
        "Stop"
      ]
    },
    "images": [
      "data/images/0170.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢靠近那辆沉没的汽车进行抵近观察，保持匀速并注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "目标沉没汽车位于视角的正前方偏右位置。底部散布着黑色的珊瑚礁障碍物。为了安全抵近，机器人需要先微调朝向，避开左下方和正下方的珊瑚礁区域，随后以低速匀速向前推进，直至到达适合观察的近距离位置。",
      "action": [
        "Turn slightly right to align with the car",
        "Maintain safe altitude to clear bottom coral reefs",
        "Move forward slowly at constant speed",
        "Approach the front-right of the car",
        "Slow down and hover for observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0171.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "缓慢向前靠近前方的发光柱状结构进行近距离观察，保持匀速并注意避让下方的珊瑚礁。",
    "output": {
      "reasoning": "视觉分析显示，目标发光柱状结构位于正前方，而图像下方前景处有大面积的绿色珊瑚礁。为了执行指令，机器人需要首先略微提升高度或保持俯仰角以确保越过下方的珊瑚礁障碍，随后以缓慢且稳定的速度向目标推进，并在接近后停止以进行近距离观察。",
      "action": [
        "Ascend slightly to clear the coral reef",
        "Move forward slowly",
        "Maintain constant speed",
        "Approach the glowing columnar structure",
        "Stop and hover for close observation"
      ]
    },
    "images": [
      "data/images/0172.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方航行，穿过红色浮标和左侧篮筐之间的空隙，注意避开右侧的柱子并保持匀速。",
    "output": {
      "reasoning": "图像中红色浮标（红色桶状物）位于中央偏右，篮筐位于左侧，右侧边缘有一根大型柱子。目标间隙位于红色浮标和篮筐之间。机器人需要先向左前方微调航向以对准间隙，同时在移动过程中与右侧柱子保持安全距离，并以稳定的速度穿过该区域。",
      "action": [
        "Turn slightly left to align with the gap",
        "Move forward-left at a constant speed",
        "Keep distance from the pillar on the right",
        "Navigate through the space between the red buoy and the basket",
        "Maintain steady heading and speed",
        "Clear the gap and move forward",
        "Stop"
      ]
    },
    "images": [
      "data/images/0173.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，绕过前方缠绕线缆的管道，注意避开右侧红色的网状障碍物。",
    "output": {
      "reasoning": "图像显示前方中心位置有一个缠绕着蓝白色线缆的管道，其右侧紧邻着红色的网状/线缆障碍物。为了绕过管道并避开右侧障碍，机器人需要向左侧进行小幅度转向以获取安全路径，随后保持匀速推进越过目标区域。",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Maintain steady speed",
        "Pass the wrapped pipe",
        "Keep away from the red net on the right",
        "Straighten course",
        "Stop"
      ]
    },
    "images": [
      "data/images/0174.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，缓慢向前靠近正前方的条纹状立柱，注意观察两侧珊瑚的距离。",
    "output": {
      "reasoning": "图像正中央是一个显著的条纹状立柱目标，左右两侧分布着大型脑纹珊瑚。路径前方开阔，但需严格遵守指令中的‘匀速’和‘缓慢’要求，同时在前进过程中实时监测两侧珊瑚的侧向距离，确保安全通过。目标位于正前方，无需大幅转向。",
      "action": [
        "Maintain current heading",
        "Set constant low speed",
        "Go straight slowly",
        "Monitor lateral distance to corals",
        "Approach the striped pillar",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0175.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从左侧绕过正前方的巨大珊瑚礁，向远处的柱状结构探索。",
    "output": {
      "reasoning": "图像正前方被一个巨大的球状珊瑚礁占据，目标‘柱状结构’位于左侧背景的阴影中。为了执行指令，机器人需要先向左转向以避开正前方的障碍物，在保持匀速的状态下从左侧开阔水域绕行，随后重新对准远处的柱状目标并前进探索。",
      "action": [
        "Maintain constant speed",
        "Turn left",
        "Go straight to bypass the large coral reef",
        "Adjust heading towards the distant columnar structure",
        "Move forward to explore the target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0176.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，注意避开右侧的生物群落，并与左侧的珊瑚礁保持安全距离。",
    "output": {
      "reasoning": "图像显示，左侧有明显的黄色珊瑚礁群，右侧有一群类似三叶虫的生物群落。为了在保持匀速前进的同时避开两侧障碍，机器人需要微调航向至中心空旷区域，并维持稳定的推进动力，确保与左侧珊瑚和右侧生物群保持平衡的避障距离。",
      "action": [
        "Adjust heading to center",
        "Set constant cruising speed",
        "Go straight",
        "Keep distance from left coral reef",
        "Avoid right biological community",
        "Maintain forward propulsion"
      ]
    },
    "images": [
      "data/images/0177.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前穿过珊瑚礁群，注意避开右侧的障碍物并实时监测环境变化。",
    "output": {
      "reasoning": "图像显示机器人正处于光影交错的珊瑚礁区域，前方有清晰的通行空间。右侧分布有密集的珊瑚礁结构，构成潜在障碍。为实现匀速穿过并避障，机器人需保持稳定的动力输出，向左微调航向以远离右侧珊瑚，并利用视觉传感器实时扫描前方及侧方环境。",
      "action": [
        "Maintain constant speed",
        "Slightly steer left to avoid right coral reef",
        "Go straight through the gap",
        "Monitor environment for dynamic changes",
        "Continue forward movement"
      ]
    },
    "images": [
      "data/images/0178.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右绕过前方的垂直结构，匀速跟随那枚带有黄色尾翼的潜航器，注意避开下方的线缆。",
    "output": {
      "reasoning": "图像中，前方左侧存在一个显著的黑色垂直结构，目标（带有黄色尾翼的潜航器）位于右侧并向右前方航行。下方海床分布有明显的线缆。为了安全执行指令，机器人需要先向右转向以绕过垂直结构，同时保持或略微提升高度以避开下方线缆，最后调整速度与目标潜航器保持同步。",
      "action": [
        "Turn right",
        "Maintain altitude to clear cables below",
        "Go straight to bypass the vertical structure",
        "Accelerate to match the target's speed",
        "Follow the AUV with yellow fins",
        "Maintain constant speed"
      ]
    },
    "images": [
      "data/images/0179.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢行驶，避开左侧的缆绳网，抵近观察前方的青色目标物，注意保持匀速。",
    "output": {
      "reasoning": "视觉分析显示，图像左侧近处存在大面积的黄色缆绳网（障碍物），目标青色潜水器状物体位于前方偏右位置。为了避开左侧障碍并抵近目标，机器人需要先向右微调航向，随后以恒定低速向右前方推进，最后在接近目标时减速并悬停观察。",
      "action": [
        "Turn slightly right to avoid the cable net on the left",
        "Move forward-right at a slow and constant speed",
        "Approach the cyan target object",
        "Maintain steady speed while closing in",
        "Decelerate and hover for close observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0180.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，抵近观察前方的红色探测器，同时注意避开右侧的大型珊瑚群。",
    "output": {
      "reasoning": "图像中心可见红色的圆柱形探测器，这是目标物体；右侧存在一个巨大的条纹状珊瑚群，属于需要避开的障碍物。为了安全抵近，机器人需要先微调航向以确保与右侧珊瑚保持安全距离，随后以匀速向前推进，在接近探测器时减速并最终停止以进行观察。",
      "action": [
        "Align with the red detector",
        "Keep distance from the coral on the right",
        "Go straight at constant speed",
        "Approach the red detector",
        "Slow down",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0181.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "请保持匀速向前推进，绕过正前方的红色浮标柱并向深处探索，同时注意避开底部两侧的珊瑚礁。",
    "output": {
      "reasoning": "图像显示正前方有一个显著的红色浮标柱，底部左右两侧分布着密集的珊瑚礁。为执行指令，机器人需先保持匀速前进，在接近红色浮标时向右侧（或左侧）小幅度转向以绕过障碍，同时需严格控制下潜深度以避开底部的珊瑚礁群，绕过浮标后重新修正航向，向深水区域直线推进探索。",
      "action": [
        "Maintain constant speed forward",
        "Steer slightly to the right to bypass the red buoy",
        "Maintain current depth to avoid bottom corals",
        "Straighten course after passing the buoy",
        "Go straight towards the deeper area",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0182.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，抵近观察正前方的大型人造礁石结构，注意避开底部散落的珊瑚礁。",
    "output": {
      "reasoning": "视觉分析显示，正前方是一个大型的箱型人造礁石结构，其表面已生长部分生物。底部和侧前方散落着多处珊瑚礁和岩石障碍物。为了安全抵近，机器人需要保持正向航向，同时略微抬升高度以避开底部的珊瑚礁，最后减速并悬停在目标结构前进行观察。",
      "action": [
        "Maintain constant speed forward",
        "Slightly ascend to clear bottom coral reefs",
        "Go straight towards the artificial reef structure",
        "Slow down as approaching the target",
        "Stop and hover for close observation"
      ]
    },
    "images": [
      "data/images/0183.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过两侧的礁石区域，靠近正前方的发光目标并注意观察周围的动态变化。",
    "output": {
      "reasoning": "图像显示机器人正处于由两侧大型礁石构成的通道中，正前方有一个明显的发光目标，并伴有上升的气泡。路径基本清晰，但底部中心有一块较小的球状礁石。机器人需要保持航向，匀速穿过礁石区域，并在接近目标的同时进行环境扫描以观察动态变化。",
      "action": [
        "Go straight at constant speed",
        "Pass between the left and right reefs",
        "Align with the central glowing target",
        "Scan surroundings for dynamic changes",
        "Approach the light source",
        "Slow down and stop near target"
      ]
    },
    "images": [
      "data/images/0184.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速航行，穿过前方的石柱拱门，注意观察底部沙地的起伏。",
    "output": {
      "reasoning": "图像中心显示一个巨大的石柱拱门，路径前方无明显障碍物。左侧有一辆蓝色水下航行体，需保持当前航线以避免干扰。为了穿过拱门并观察沙地起伏，机器人需要调整姿态对准拱门中心，保持稳定的低速推进，并向下倾斜传感器或视角以监测底部地形。",
      "action": [
        "Adjust heading to align with the center of the archway",
        "Maintain constant speed",
        "Go straight towards the archway",
        "Tilt camera down slightly to observe seabed",
        "Pass through the pillars",
        "Continue forward while monitoring bottom terrain"
      ]
    },
    "images": [
      "data/images/0185.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，抵近观察沉车开启的车门内部，注意避开左侧底部的珊瑚群。",
    "output": {
      "reasoning": "图像显示沉车位于前方，其左侧车门处于开启状态。左下方有明显的珊瑚群。为执行指令，机器人需要保持匀速前进，同时航向需略微向右偏移以确保与左侧珊瑚群保持安全距离，随后对准开启的车门位置进行抵近观察。",
      "action": [
        "Maintain constant speed forward",
        "Steer slightly right to clear the corals on the left",
        "Align with the open car door",
        "Approach the door interior",
        "Stop and hover for observation"
      ]
    },
    "images": [
      "data/images/0186.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过前方光影交错的珊瑚礁群，注意避开左侧的潜航器并观察底部障碍物。",
    "output": {
      "reasoning": "视觉分析显示：左侧中层水域有一台绿色潜航器，前方是受上方光线照射的珊瑚礁群，底部分布着密集的脑纹状珊瑚。为完成指令，机器人需要保持匀速，向右微调以避开左侧潜航器，随后对准光影交汇的礁石缝隙穿行，同时俯视监测底部珊瑚的高度以确保安全间距。",
      "action": [
        "Maintain constant speed",
        "Steer slightly right to avoid the submersible on the left",
        "Go straight through the gap in the coral reefs",
        "Monitor bottom clearance and observe obstacles",
        "Continue forward through the light rays"
      ]
    },
    "images": [
      "data/images/0187.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢推进，深入探索洞穴内部，注意保持与底部礁石的安全距离。",
    "output": {
      "reasoning": "视觉分析显示，机器人当前位于洞穴入口附近，正下方有一块巨大的礁石，右侧和前方是深色的洞穴内部空间。根据指令，机器人需要向右前方移动。为确保与底部礁石的安全距离，首先应微调高度或保持当前悬浮高度，然后向右转动以对准洞穴延伸方向，最后缓慢推进。由于环境较暗，需持续利用自带灯光进行避障。",
      "action": [
        "Maintain altitude or ascend slightly",
        "Turn right towards the cave opening",
        "Go straight slowly",
        "Monitor distance to bottom reef",
        "Proceed deeper into the cave"
      ]
    },
    "images": [
      "data/images/0188.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，直接穿过前方黑色立柱底部的贝壳状开口，注意避开两侧密集的珊瑚群。",
    "output": {
      "reasoning": "图像显示正前方是一个带有贝壳状开口的黑色立柱，开口两侧分布着密集的珊瑚群。为了安全完成穿行任务，机器人需要首先调整姿态对准开口中心，以确保与两侧珊瑚保持安全距离，随后以恒定速度直行穿过该区域。",
      "action": [
        "Align with the center of the shell-like opening",
        "Maintain constant speed",
        "Go straight",
        "Keep distance from corals on both sides",
        "Pass through the opening",
        "Stop"
      ]
    },
    "images": [
      "data/images/0189.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "一直向前穿过两侧的珊瑚群，抵近观察正前方的绿色节状柱体，请保持匀速并注意近处的动态变化。",
    "output": {
      "reasoning": "图像显示目标绿色节状柱体位于视野正中央，左右两侧分布着大型珊瑚状结构，形成了一条天然通道。路径前方开阔，无明显静态障碍物阻挡。根据指令，机器人需要保持直线航行穿过珊瑚群，同时维持恒定速度并实时监测水流或生物带来的动态干扰，最终抵近目标进行观察。",
      "action": [
        "Go straight",
        "Pass through the coral groups",
        "Maintain constant speed",
        "Monitor dynamic environment",
        "Approach the green segmented pillar",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0190.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前方那根高耸的柱状目标靠近，注意观察周围珊瑚群的动态变化。",
    "output": {
      "reasoning": "图像中心视野中有一根显著的高耸柱状目标，其表面具有条纹纹理。底部和右侧分布着密集的珊瑚群。为了安全接近目标并观察珊瑚，机器人需要保持稳定的巡航速度，在垂直方向上维持安全高度以避开底部的复杂地形，并直线向柱状目标推进。",
      "action": [
        "Maintain stable depth",
        "Go straight at constant speed",
        "Monitor coral dynamics",
        "Approach the central pillar",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0191.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前靠近中心绿色发光基座，注意观察底部支架的结构，并在接近后停止。",
    "output": {
      "reasoning": "目标绿色发光基座位于视野正前方，路径清晰且无明显障碍物。任务要求匀速接近并观察底部支架，因此需要维持直线航行，同时将视觉焦点锁定在基座底部的多足支撑结构上以进行细节观察，最后在接近目标的安全距离内停止。",
      "action": [
        "Go straight at constant speed",
        "Focus on bottom support structure",
        "Maintain forward movement",
        "Approach the green base",
        "Stop"
      ]
    },
    "images": [
      "data/images/0192.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过前方的红色浮标并向右侧开阔水域探索，注意观察周围珊瑚礁的分布。",
    "output": {
      "reasoning": "图像显示前方正中央的珊瑚礁平台上有一个明显的红色浮标。为了执行指令，机器人需要先保持匀速前进，在接近浮标时向左侧微调以实现‘绕过’动作，随后向右转入光照充足的开阔水域。在整个移动过程中，需要保持视觉传感器对周边珊瑚礁群落的扫描与记录。",
      "action": [
        "Go straight at constant speed",
        "Steer left to bypass the red buoy",
        "Turn right towards the open water",
        "Go straight",
        "Scan and observe surrounding coral reefs",
        "Maintain exploration"
      ]
    },
    "images": [
      "data/images/0193.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过前方的珊瑚礁峡谷，注意保持与底部岩石的安全距离，匀速向深处探索。",
    "output": {
      "reasoning": "图像显示前方两侧分布着大型珊瑚礁和岩石，形成了一个明显的峡谷通道。机器人目前处于通道入口，底部散落着一些较小的岩石障碍物。为了执行指令，机器人需要保持在通道中央，微调高度以确保与底部岩石的安全间距，并稳步向光线较暗的深处峡谷推进。",
      "action": [
        "Maintain center position in the canyon",
        "Adjust altitude to keep safe distance from bottom rocks",
        "Go straight at constant speed",
        "Pass through the coral reef canyon",
        "Continue exploring deeper into the dark area",
        "Monitor surroundings"
      ]
    },
    "images": [
      "data/images/0194.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持当前航向，匀速靠近正前方的方形石柱，注意观察其表面的结构细节。",
    "output": {
      "reasoning": "图像显示正前方有一座高大的方形石柱，位于视野中心，路径上无明显障碍物。指令要求保持航向并靠近以观察细节。规划逻辑为：维持当前对准状态，直线推进，在接近目标时减速以确保观察质量，最后悬停进行细节扫描。",
      "action": [
        "Go straight",
        "Maintain heading towards the pillar",
        "Keep target centered",
        "Slow down as approaching",
        "Approach the pillar surface",
        "Hover and observe structural details",
        "Stop"
      ]
    },
    "images": [
      "data/images/0195.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方的石塔靠近，注意避开右侧的红色动态目标，保持匀速前进。",
    "output": {
      "reasoning": "视觉分析显示，目标石塔位于画面中心正前方，而红色的动态目标（鱼群）位于画面右侧。为了安全靠近石塔并避开右侧障碍，机器人需要锁定正前方目标，在前进过程中保持航向略微偏左以拉开与红色目标的距离，并控制动力输出以维持匀速。",
      "action": [
        "Lock target on the stone tower ahead",
        "Maintain steady speed",
        "Slightly steer left to keep distance from red targets on the right",
        "Go straight towards the tower",
        "Approach the tower base",
        "Stop"
      ]
    },
    "images": [
      "data/images/0196.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右绕过正前方的巨型脑珊瑚，保持匀速，注意观察周围鱼类的动态变化。",
    "output": {
      "reasoning": "图像正前方是一个巨大的脑珊瑚，占据了中心视野并阻挡了直行路径。左侧和左下方观察到有条纹鱼类活动。为了向右绕过目标，机器人需要先向右调整航向，在保持恒定推进速度的同时，利用视觉传感器持续追踪周围鱼类的动态，确保在绕行过程中不发生碰撞。",
      "action": [
        "Set constant speed",
        "Turn right",
        "Move forward to bypass the brain coral",
        "Monitor fish movements on both sides",
        "Maintain safe distance from coral surface",
        "Observe right-side environment",
        "Complete bypass and stabilize"
      ]
    },
    "images": [
      "data/images/0197.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右绕过正前方的大型褶皱珊瑚，注意观察后方的开阔水域。",
    "output": {
      "reasoning": "图像正前方占据视野的是一个巨大的褶皱珊瑚，构成了直接障碍。根据指令，机器人需要保持匀速并从右侧绕行。路径规划为：先向右转向避开珊瑚边缘，随后直线行驶完成绕路，最后调整姿态以观察珊瑚后方的开阔水域。",
      "action": [
        "Maintain constant speed",
        "Turn right",
        "Go straight to bypass the coral",
        "Turn left slightly to face the rear area",
        "Observe the open water",
        "Stop"
      ]
    },
    "images": [
      "data/images/0198.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着面前的绿色长管结构向右前方匀速推进，注意保持与管壁的安全距离并观察底部的阴影区域。",
    "output": {
      "reasoning": "图像中心显示一根绿色的长管状结构，从左前方延伸至右后方。为了执行指令，机器人需要将航向调整至右前方，与管道走向保持平行。由于需要观察底部的阴影区域（位于管道右侧的沙地上），机器人应保持在管道右侧的安全距离处推进，并微调视角向下观察。",
      "action": [
        "Adjust heading to the front-right",
        "Move slightly right to maintain safe distance from the pipe",
        "Move forward at a constant speed",
        "Tilt camera down to observe the shadow area",
        "Maintain steady propulsion along the pipe"
      ]
    },
    "images": [
      "data/images/0199.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着主管道缓慢向前推进，注意右侧珊瑚礁，保持匀速。",
    "output": {
      "reasoning": "图像显示主管道占据了视野的左侧和中心区域，并向远方延伸。右下角有一处明显的珊瑚礁。为了安全地沿着管道推进，机器人需要首先校准航向与管道平行，并向左侧保持微小偏移以避开右侧的珊瑚礁，随后以低速、恒定的动力向前巡航。",
      "action": [
        "Align with the main pipe axis",
        "Shift slightly to the left to clear the coral reef",
        "Move forward at low speed",
        "Maintain constant velocity",
        "Monitor distance to the pipe and reef"
      ]
    },
    "images": [
      "data/images/0200.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "",
    "output": {
      "reasoning": "目标白色皮卡车清晰可见，位于视野的左下方，停放在水底的结构化平台上。机器人当前处于斜上方位置，路径上无明显大型障碍物。规划动作为：首先向左微调航向对准目标，然后下降高度并向前推进，最终靠近皮卡车并停止。",
      "action": [
        "Turn left",
        "Descend",
        "Go straight",
        "Close to the white pickup truck",
        "Stop"
      ]
    },
    "images": [
      "data/images/0201.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方匀速前进，抵近观察下方的珊瑚礁，注意避开上方的大型结构体。",
    "output": {
      "reasoning": "图像显示上方有一个巨大的黑色结构体，占据了左上部空间；下方分布着珊瑚礁。根据指令，机器人需要向右前方移动以避开上方的障碍物，并逐渐降低高度以抵近观察底部的珊瑚礁。路径规划应先保持水平向右前位移，确认远离上方结构后再进行下潜。",
      "action": [
        "Turn right slightly",
        "Move forward at constant speed",
        "Keep distance from the overhead structure",
        "Descend slowly towards the coral reef",
        "Approach the coral reef",
        "Hover for observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0202.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "绕过正前方的红色浮标，匀速向远处那个巨大的红色结构体靠近，注意观察周围光影的变化。",
    "output": {
      "reasoning": "视觉感知显示，正前方近处有一个红色浮标障碍物，远处背景中有一个巨大的红色多孔结构体。为了绕过浮标并靠近目标，机器人需要先向右侧（或左侧）进行侧向位移或转向以避开障碍，随后重新对准远处结构体，保持稳定速度前进，并持续监测水下光影的动态变化。",
      "action": [
        "Turn right slightly",
        "Go straight to bypass the red buoy",
        "Turn left to realign with the large red structure",
        "Go straight at constant speed",
        "Monitor surrounding light and shadow changes",
        "Approach the large structure",
        "Stop"
      ]
    },
    "images": [
      "data/images/0203.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方转向避开红色的浮标障碍，保持匀速向深处探索，注意观察近处岩石区域的动态变化。",
    "output": {
      "reasoning": "图像中心显著位置存在一个红色的浮标状结构，被识别为指令中需要避开的障碍物。右侧和下方分布着大片岩石区域。为了执行任务，机器人需要首先执行右转动作以绕过红色浮标，随后进入背景中的开阔深水区域进行探索。在移动过程中，需保持恒定航速，并将视觉注意力集中在右侧岩石区，以监测可能的动态生物或环境变化。",
      "action": [
        "Turn right-forward",
        "Bypass the red buoy obstacle",
        "Maintain constant speed",
        "Go straight towards deep water",
        "Monitor nearby rock area",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0204.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方转向以避开右侧的大型结构体，保持匀速并注意观察下方珊瑚礁的动态变化。",
    "output": {
      "reasoning": "图像右侧有一个巨大的圆柱形结构体（疑似潜航器或大型管道），占据了显著空间。为执行避障指令，机器人需要向左前方开阔水域转向。下方分布有密集的珊瑚礁，在移动过程中需保持稳定的航速，以便视觉系统能够持续且清晰地监测珊瑚礁的动态。",
      "action": [
        "Turn left",
        "Go forward",
        "Maintain constant speed",
        "Keep distance from the right structure",
        "Monitor coral reefs below"
      ]
    },
    "images": [
      "data/images/0205.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右绕过前方的管道支架，保持匀速并注意避开底部的珊瑚礁。",
    "output": {
      "reasoning": "图像显示前方正中心偏左位置有一个蓝色的垂直管道支架，底部散布着黄色和灰色的珊瑚礁。为了向右绕过支架，机器人需要先向右调整航向以获得安全间隙，同时必须保持高度以防撞击底部的珊瑚，随后匀速向前推进完成绕行。",
      "action": [
        "Turn right",
        "Maintain altitude to clear coral reefs",
        "Go straight at constant speed",
        "Bypass the pipe support",
        "Keep clear of bottom obstacles",
        "Stop"
      ]
    },
    "images": [
      "data/images/0206.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近中心的发光柱状目标进行观测，注意避开底部密集的珊瑚礁。",
    "output": {
      "reasoning": "图像中心有一个显著的发光柱状目标，其顶部呈绿色发光状。图像底部和前景布满了密集的彩色珊瑚礁，属于需要避开的障碍物。为了安全靠近目标，机器人需要保持匀速前进，同时控制俯仰角或高度以越过底部的珊瑚区域，最后在目标前减速悬停进行观测。",
      "action": [
        "Go straight at constant speed",
        "Maintain altitude to clear bottom corals",
        "Approach the central glowing target",
        "Slow down",
        "Hover for observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0207.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿着中心支架向最上方的红色浮标推进，注意避开两侧的线缆。",
    "output": {
      "reasoning": "图像显示目标红色浮标位于中心支架的最远端（视野上方）。中心支架由底部的红白组件和中部的青色圆柱体组成，两侧对称分布着两条粗大的黄色包裹线缆，形成了狭窄的通道。为了安全到达目标，机器人需要保持在中心轴线上，匀速推进并实时监控与两侧线缆的距离。",
      "action": [
        "Align with the central axis",
        "Maintain constant speed",
        "Go straight along the central support",
        "Keep safe distance from left and right cables",
        "Approach the top red buoy",
        "Stop at target"
      ]
    },
    "images": [
      "data/images/0208.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方的石柱靠近，注意避开左侧的缆绳区域。",
    "output": {
      "reasoning": "图像中心位置有一个显著的石柱作为目标，左侧区域存在明显的缆绳结构。为了执行指令，机器人需要锁定正前方的石柱，在保持匀速前进的同时，航向需稍微偏向石柱右侧或保持中心对准，以确保与左侧缆绳保持安全距离。",
      "action": [
        "Align with the central stone pillar",
        "Maintain constant speed",
        "Go straight while keeping distance from the left ropes",
        "Close to the stone pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0209.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方航行，靠近那根较远的柱子，保持匀速并注意观察底部岩石的分布。",
    "output": {
      "reasoning": "图像中可见两根巨大的石柱，左侧柱子在视觉上比右侧稍小且位置偏后，判定为指令中提到的‘较远的柱子’。目标位于当前视角的左前方。路径底部布满了不规则的岩石，航行时需兼顾避障与观察。计划首先调整航向指向左侧柱子，随后以恒定速度推进，并持续监测底部地形分布。",
      "action": [
        "Turn left",
        "Set constant speed",
        "Go straight towards the farther column",
        "Observe and scan bottom rock distribution",
        "Approach the base of the left column",
        "Stop"
      ]
    },
    "images": [
      "data/images/0210.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，密切关注前方海底的地形变化，注意避开近处的珊瑚礁。",
    "output": {
      "reasoning": "图像显示机器人正前方和下方分布着起伏不平的海底地形及珊瑚礁。为了执行指令，机器人需要保持稳定的前进动力，同时通过实时地形扫描识别障碍物，并采取微调高度或转向的方式避开近处的珊瑚礁，确保安全推进。",
      "action": [
        "Maintain constant speed",
        "Scan seabed topography ahead",
        "Ascend slightly to clear nearby coral reefs",
        "Go straight",
        "Monitor for terrain changes",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0211.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方发光的红色目标点缓慢推进，注意避开右侧的珊瑚群并观察周围水流变化。",
    "output": {
      "reasoning": "图像显示发光的红色目标点位于机器人的右前方，且伴有气泡上升。右侧近处分布有大型珊瑚群。为了安全接近目标，机器人需要向右微调航向，同时保持与右侧珊瑚的安全距离，以低速推进并持续监测气泡轨迹以感知水流变化。",
      "action": [
        "Turn right slightly",
        "Keep distance from coral on the right",
        "Go straight slowly",
        "Observe bubbles to monitor water current",
        "Approach the red glowing target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0212.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右绕过面前的橙绿相间立柱，保持匀速，并注意避开右侧的网状障碍物。",
    "output": {
      "reasoning": "图像中心是一个垂直的橙绿相间立柱，右侧紧邻一个绿色的网状障碍物。为了向右绕过立柱，机器人需要先向右微调方向，进入立柱与网状障碍物之间的空隙，随后保持匀速直线前进，同时持续监测右侧距离以防触碰网状物。",
      "action": [
        "Move forward",
        "Turn right slightly",
        "Maintain constant speed",
        "Go straight between the pillar and the mesh",
        "Keep distance from the right mesh obstacle",
        "Bypass the pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0213.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近正前方巨大的青色珊瑚结构，注意保持匀速，并留意左侧粉色珊瑚的位置。",
    "output": {
      "reasoning": "图像中心偏右位置有一个巨大的青色珊瑚结构，这是主要目标；左下角有一个明显的粉色珊瑚结构。路径前方地面平坦，无明显障碍物。机器人需要执行直线前进动作，同时通过视觉反馈维持恒定低速，并持续监测左侧粉色珊瑚的距离以防碰撞。",
      "action": [
        "Move forward slowly",
        "Maintain constant speed",
        "Monitor pink coral on the left",
        "Approach cyan coral structure",
        "Stop"
      ]
    },
    "images": [
      "data/images/0214.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过两侧岩石构成的狭窄通道，直接向远处的绿光光源靠拢，注意观察近处地面的起伏变化。",
    "output": {
      "reasoning": "图像显示前方是一个由两侧岩石构成的狭窄通道，尽头有明显的绿色垂直光源。近处地面呈现出明显的波纹状起伏。为了安全穿过该区域并到达目标，机器人需要保持在通道中心以避开两侧岩石，同时实时监测地面高度变化以维持稳定的离地间隙，最终向绿光中心靠拢。",
      "action": [
        "Maintain constant speed forward",
        "Enter the narrow channel between rocks",
        "Keep center position to avoid side obstacles",
        "Monitor ground undulations and adjust altitude",
        "Move directly towards the green light source",
        "Stop at the target"
      ]
    },
    "images": [
      "data/images/0215.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿着右侧发光的管线持续向前探索，注意观察左侧岩壁的距离。",
    "output": {
      "reasoning": "图像显示右侧有一条带有绿色发光环的显著管线，左侧为紧邻的粗糙岩壁。目前机器人位于两者之间的通道中。为了执行指令，机器人需要首先对齐管线的延伸方向，设定稳定的推进速度，并在前进过程中通过视觉反馈维持与左侧岩壁的安全距离，确保不发生碰撞并保持航向平行于管线。",
      "action": [
        "Align with the glowing pipeline",
        "Set constant forward speed",
        "Move forward along the pipeline",
        "Monitor distance to the left rock wall",
        "Adjust heading to maintain clearance from left wall",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0216.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，缓慢向前推进，绕过正前方的蓝色脑珊瑚并向那根冒泡的立柱靠近，同时注意避开左侧的黄色珊瑚礁。",
    "output": {
      "reasoning": "视觉分析显示，图像正前方有一个显著的蓝色脑珊瑚，左下角分布着黄色珊瑚礁，而目标（冒泡的立柱）位于背景中央。为了安全到达目标，机器人需要采取‘前进-右偏-修正’的路径：首先缓慢前进，通过向右侧微调方向来同时避开左侧的黄色珊瑚和正前方的蓝色珊瑚，随后重新对准立柱并接近。",
      "action": [
        "Move forward slowly at constant speed",
        "Steer slightly right to bypass the blue brain coral",
        "Keep distance from the yellow coral reef on the left",
        "Align with the bubbling pillar",
        "Approach the pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0217.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方的蓝色柱状物靠近，注意避开右侧的杯状珊瑚。",
    "output": {
      "reasoning": "图像中心是一个显著的蓝色柱状物，即为目标。右侧前景分布着数个黄褐色的杯状珊瑚，属于需要避开的障碍物。路径规划应保持直线或微向左偏，以确保与右侧珊瑚保持安全距离，并遵循指令要求匀速推进。",
      "action": [
        "Maintain constant speed",
        "Go straight towards the blue pillar",
        "Keep distance from the cup coral on the right",
        "Approach target",
        "Stop"
      ]
    },
    "images": [
      "data/images/0218.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方缓慢推进，绕过右侧的大型橙色结构并靠近红色的圆柱浮标，注意保持高度并观察线缆位置。",
    "output": {
      "reasoning": "图像右侧占据大部分视野的是大型橙色结构，目标红色圆柱浮标位于左前方。浮标顶部连接有一条延伸至上方的线缆。为了安全绕过右侧障碍并靠近目标，机器人需要向左修正航向，缓慢推进以保持稳定，同时在接近过程中密切监视线缆的相对位置，防止发生缠绕，并维持当前作业高度。",
      "action": [
        "Turn left",
        "Go straight slowly",
        "Maintain current altitude",
        "Monitor cable position on the left",
        "Approach the red cylindrical buoy",
        "Stop at a safe distance"
      ]
    },
    "images": [
      "data/images/0219.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向左前方航行以避开右侧的橙色浮标，并靠近观察前方的绿色舱体。",
    "output": {
      "reasoning": "视觉分析显示，右侧近处有一个显著的橙色浮标，前方中心位置是一个大型绿色舱体。根据指令，机器人需要避开右侧障碍物并靠近目标。因此，首先需要向左前方调整航向以拉开与浮标的安全距离，随后保持匀速航行接近绿色舱体，最后减速并停在目标前进行观察。",
      "action": [
        "Maintain constant speed",
        "Turn left",
        "Go straight to bypass orange buoy",
        "Approach green hull",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0220.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过两侧的岩石区域，注意观察上方光线投射处的环境变化。",
    "output": {
      "reasoning": "图像显示机器人当前处于一个由两侧岩石（左侧为大型深色礁石，右侧为珊瑚礁区域）构成的水下通道中心。前方路径开阔且平坦，上方有强烈的自然光束投射。为了完成指令，机器人需要保持当前航向匀速前进，穿过通道，同时传感器需重点监测上方光照区域的亮度与环境波动。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Pass through the rocky channel",
        "Monitor light rays from above",
        "Observe environmental changes",
        "Keep steady heading"
      ]
    },
    "images": [
      "data/images/0221.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方光亮处缓慢前进，穿过两侧的黑色柱状障碍物，注意保持航向平稳。",
    "output": {
      "reasoning": "图像显示机器人正处于两个巨大的黑色柱状障碍物之间，正前方上方有明显的强光束。路径中心区域开阔，无直接碰撞风险。根据指令，需要保持在两侧障碍物的中心线上，缓慢且平稳地向光源方向推进。",
      "action": [
        "Align with the center gap between the black columns",
        "Move forward slowly",
        "Maintain stable heading while passing obstacles",
        "Continue towards the light source",
        "Stop"
      ]
    },
    "images": [
      "data/images/0222.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近沉没的绿色车辆进行抵近观察，注意避开左侧的珊瑚礁并保持高度稳定。",
    "output": {
      "reasoning": "图像显示目标绿色车辆位于前方中心区域，左侧近处有明显的黄色珊瑚礁障碍物。为安全抵近，机器人需要先保持当前高度稳定，向右微调航向以确保与左侧珊瑚礁保持安全距离，随后以低速向前推进，最终在靠近车辆的位置悬停观察。",
      "action": [
        "Maintain stable altitude",
        "Turn slightly right to clear the left coral reef",
        "Move forward slowly",
        "Keep monitoring the distance to the left obstacles",
        "Approach the green vehicle",
        "Stop and hover for close observation"
      ]
    },
    "images": [
      "data/images/0223.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前靠近那辆沉没的汽车，注意避开左侧近处的珊瑚礁并留意上方的光影变化。",
    "output": {
      "reasoning": "目标沉船汽车位于视野正前方，左侧近处有大片珊瑚礁，路径规划需向右微调以保持安全距离。上方水面有强烈的阳光折射，需在前进过程中持续监测环境光影变化以确保视觉定位稳定。",
      "action": [
        "Adjust heading slightly to the right",
        "Maintain constant speed",
        "Go straight towards the car",
        "Monitor overhead light patterns",
        "Keep distance from left coral reefs",
        "Slow down and approach",
        "Stop"
      ]
    },
    "images": [
      "data/images/0224.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过珊瑚丛和右侧橙色长条障碍物之间的空隙，保持匀速，注意观察前方光影下的动态变化。",
    "output": {
      "reasoning": "图像显示左侧和底部有数个橙黄色珊瑚丛，右侧有一条巨大的橙色带状障碍物。两者之间存在一个明显的通道，且上方有强烈的放射状光影。根据指令，机器人需要对准该间隙，在保持匀速的同时穿过，并持续监测前方光影区域的动态。",
      "action": [
        "Align with the gap between corals and orange obstacle",
        "Move forward at constant speed",
        "Navigate through the passage",
        "Monitor dynamic light and shadow changes ahead",
        "Maintain steady movement",
        "Stop after clearing the gap"
      ]
    },
    "images": [
      "data/images/0225.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方移动，靠近那辆沉没的卡车进行观察，注意保持匀速并留意底部的障碍物。",
    "output": {
      "reasoning": "目标沉没卡车位于当前视野的左前方，距离适中。底部沙地上散布着珊瑚礁和岩石（障碍物）。为了安全靠近，机器人需要先向左调整航向，随后在保持一定离地高度以避开底部障碍物的前提下，匀速向卡车推进并最终悬停观察。",
      "action": [
        "Turn left",
        "Go straight",
        "Maintain altitude to clear bottom obstacles",
        "Approach the sunken truck at constant speed",
        "Slow down",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0226.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "穿过前方两侧的障碍物，向着光亮处持续前进，注意保持在通道中央并观察近处地形的变化。",
    "output": {
      "reasoning": "视觉图像显示，前方左右两侧各有一个大型的波浪状障碍物，形成了一个天然的通道。通道尽头有明显的光束射入，作为导航目标。底部地形较为复杂且不平整。为了安全通过，机器人需要首先微调姿态以对准通道中心，然后保持直线航行，同时利用视觉传感器持续监测下方地形的起伏，确保不发生碰撞。",
      "action": [
        "Align with the center of the channel",
        "Go straight towards the light source",
        "Pass through the obstacles on both sides",
        "Monitor bottom terrain variations",
        "Maintain forward momentum",
        "Keep centered in the passage"
      ]
    },
    "images": [
      "data/images/0227.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，从左侧绕过前方的大型结构，向远处的开阔水域探索，注意避开底部的珊瑚群。",
    "output": {
      "reasoning": "图像显示右侧有一个巨大的沉船或人工结构，底部布满了密集的珊瑚群。为了安全绕行并到达远处的开阔水域，机器人需要首先保持高度以避开底部的珊瑚，随后向左侧转向以绕过右侧的结构障碍，最后对准前方光线较亮的区域直线推进。",
      "action": [
        "Maintain constant speed",
        "Ascend slightly to avoid bottom corals",
        "Turn left to bypass the large structure",
        "Go straight towards the open water",
        "Keep monitoring for obstacles while exploring"
      ]
    },
    "images": [
      "data/images/0228.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速推进，穿过面前的珊瑚缝隙，注意避开右侧的大型橙色珊瑚丛。",
    "output": {
      "reasoning": "图像显示，前方正中偏左位置有一个明显的珊瑚缝隙，而右侧近处分布着大型的橙色分枝状珊瑚丛。为执行指令，机器人需要先微调航向向左以对准缝隙，随后保持匀速直线推进，在通过过程中需时刻监控右侧传感器数据以确保与橙色珊瑚保持安全距离。",
      "action": [
        "Move forward at constant speed",
        "Slightly steer left to align with the gap",
        "Maintain steady propulsion",
        "Keep clear of the orange coral on the right",
        "Pass through the gap",
        "Stabilize position"
      ]
    },
    "images": [
      "data/images/0229.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向左前方绕过石柱，去探索远处那几个沉没的箱体，注意观察底部环境变化。",
    "output": {
      "reasoning": "图像右侧是一个高大的雕刻石柱，作为主要障碍物；目标是左前方远处的几个沉没箱体。路径规划需要先向左微调以绕开石柱基座，随后保持稳定速度向目标直线推进，同时需保持相机下倾或利用感知算法监测底部地形变化。",
      "action": [
        "Turn left slightly",
        "Maintain steady speed",
        "Go straight towards the sunken boxes",
        "Scan the seabed environment",
        "Approach the target boxes",
        "Stop"
      ]
    },
    "images": [
      "data/images/0230.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过左侧的网状结构并抵近观察正前方的堆叠立柱，注意避开上方悬垂的线缆。",
    "output": {
      "reasoning": "图像显示目标堆叠立柱位于正前方，左侧存在大面积绿色网状结构，上方有数条悬垂的线缆。为了安全抵近，机器人需要保持匀速前进，通过向右微调航向来绕开左侧障碍物，同时需维持当前深度以避开上方线缆，最后减速抵近立柱。",
      "action": [
        "Go straight at constant speed",
        "Steer slightly right to bypass the left mesh structure",
        "Maintain current depth to avoid overhead cables",
        "Approach the stacked pillars",
        "Slow down and stop for observation"
      ]
    },
    "images": [
      "data/images/0231.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前穿过线缆拱廊，注意避开底部的发光生物，向光线深处继续探索。",
    "output": {
      "reasoning": "图像显示机器人正处于一个由线缆构成的拱廊入口处。路径底部有数个发光的球形生物（避障目标），而光束从前方深处射入，指引了探索方向。为了安全穿过并避开底部生物，机器人需要保持在拱廊的中上层空间匀速直行。",
      "action": [
        "Align with the center of the cable arcade",
        "Maintain altitude to stay above bioluminescent creatures",
        "Go straight at constant speed",
        "Pass through the arcade",
        "Move towards the light rays",
        "Continue exploration into the depths"
      ]
    },
    "images": [
      "data/images/0232.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从左侧绿色目标与右侧珊瑚礁之间穿过，注意观察前方光影变化。",
    "output": {
      "reasoning": "图像左侧有一个明显的绿色圆柱形目标，右侧是大型红褐色珊瑚礁，两者之间存在一个清晰的可通行区域。为了执行指令，机器人需要调整航向对准中央间隙，在保持恒定推进速度的同时，直线穿过该区域，并持续监测前方水域的光影波动以判断环境变化。",
      "action": [
        "Align with the gap between the green target and the coral reef",
        "Maintain constant speed",
        "Go straight through the passage",
        "Observe light and shadow changes ahead",
        "Keep moving forward"
      ]
    },
    "images": [
      "data/images/0233.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前探索，注意避开右侧巨大的岩石障碍，并观察前方开阔水域的情况。",
    "output": {
      "reasoning": "图像显示右侧有一个巨大的、带有孔洞的岩石（或大型海绵状生物）障碍物，左侧也有较小的礁石。目前机器人位于两者之间的通道。为了执行指令，需要向左前方微调航向以远离右侧巨大障碍物，随后保持匀速直线行驶穿过当前区域，最终进入前方光照充足的开阔水域进行观察。",
      "action": [
        "Turn slightly left",
        "Move forward at constant speed",
        "Keep distance from the right rock",
        "Proceed to open water",
        "Observe forward environment",
        "Maintain steady pace"
      ]
    },
    "images": [
      "data/images/0234.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前匀速航行，靠近并观察正前方的石塔，注意避开底部的障碍物并保持高度稳定。",
    "output": {
      "reasoning": "图像中心是一个高大的石塔，底部海床上散布着若干石块障碍物。为了安全靠近，机器人需要维持当前的中层水域高度以避开底部障碍物，保持匀速直线航行，并在接近目标后减速悬停以进行观察。",
      "action": [
        "Maintain current depth",
        "Go straight at constant speed",
        "Keep distance from bottom obstacles",
        "Slow down",
        "Approach stone tower",
        "Stop and hover for observation"
      ]
    },
    "images": [
      "data/images/0235.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过这排石柱，保持匀速，注意观察左侧游动的生物。",
    "output": {
      "reasoning": "图像显示前方由多根装饰性石柱构成了清晰的通道，左侧水域有明显的鱼类生物活动。为了完成任务，机器人需要对准石柱间的路径直线前进，同时将视觉注意力分配给左侧区域以观察生物，并保持稳定的动力输出以实现匀速移动。",
      "action": [
        "Align with the corridor between pillars",
        "Go straight",
        "Maintain constant speed",
        "Track swimming creatures on the left",
        "Pass through the row of pillars",
        "Stop"
      ]
    },
    "images": [
      "data/images/0236.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正上方的主结构靠近，注意避开近处的线缆。",
    "output": {
      "reasoning": "图像显示正上方是一个巨大的圆柱形主结构，而底部近处有数条粗大的带肋线缆（障碍物）。为了安全执行指令，机器人需要先微调位置以确保与这些线缆保持安全距离，然后设定恒定速度，沿垂直路径向上方的主结构靠拢。",
      "action": [
        "Adjust position to clear foreground cables",
        "Set constant speed",
        "Ascend towards main structure",
        "Maintain safety distance from cables",
        "Approach target structure",
        "Stop"
      ]
    },
    "images": [
      "data/images/0237.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿着蓝色主管道结构一直向前航行，注意避开上方的垂直立柱并观察管壁细节。",
    "output": {
      "reasoning": "图像显示蓝色主管道结构位于视野中心并向远方延伸，上方有一根明显的垂直立柱。为执行指令，机器人需要保持航向与管道平行，在接近立柱时微调高度或水平位置以确保安全间距，同时靠近管壁以获取清晰的细节图像。",
      "action": [
        "Align with the blue pipe structure",
        "Maintain constant speed and go straight",
        "Keep a safe distance from the vertical pillar above",
        "Approach the pipe wall for detailed observation",
        "Continue forward along the pipe",
        "Monitor wall details"
      ]
    },
    "images": [
      "data/images/0238.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，抵近观察正前方的石柱，注意避开底部散落的障碍物。",
    "output": {
      "reasoning": "图像中心是一个巨大的石柱目标，底部散落着小型岩石障碍物。为了执行指令，机器人需要保持航向向正前方匀速推进，同时需要略微提升高度或调整路径以确保不触碰底部的散落障碍物，最后在靠近石柱的位置停止进行观察。",
      "action": [
        "Go straight at constant speed",
        "Maintain safe altitude to avoid bottom obstacles",
        "Approach the stone pillar",
        "Slow down",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0239.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，紧跟前方的蓝色潜航器，同时注意避开下方的珊瑚礁群。",
    "output": {
      "reasoning": "视觉分析显示，目标蓝色潜航器位于前方偏右位置，且正向右前方行驶。机器人下方存在明显的珊瑚礁群。为了完成指令，机器人需要保持当前深度以避开下方障碍，同时向右微调航向以对准目标，并设定恒定推进速度进行跟随。",
      "action": [
        "Maintain current depth",
        "Turn slightly right",
        "Set constant forward speed",
        "Follow the blue submersible",
        "Keep safety distance from reefs",
        "Maintain steady tracking"
      ]
    },
    "images": [
      "data/images/0240.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，跟随前方潜航器进入母舰库口，注意避开右侧的珊瑚礁并留意光影变化。",
    "output": {
      "reasoning": "图像显示蓝色潜航器正位于前方，正对母舰巨大的库口（形似张开的巨口）。右侧可见明显的珊瑚礁群，上方有强烈的下射光束。机器人需要保持当前航向，微调位置以远离右侧珊瑚礁，并跟随前车进入光影复杂的库口区域。",
      "action": [
        "Maintain constant speed forward",
        "Follow the lead submersible's trajectory",
        "Keep distance from the coral reefs on the right",
        "Monitor light intensity changes",
        "Enter the mothership docking bay",
        "Stabilize and stop"
      ]
    },
    "images": [
      "data/images/0241.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，穿过前方的圆形通道，注意监测底部起伏的地形。",
    "output": {
      "reasoning": "图像显示前方有一个发光的巨大圆形通道，底部为起伏不平的波浪状地形。机器人当前位于通道入口前方，需要先对准通道中心，随后保持恒定动力输出向前推进，同时传感器需实时监测下方地形高度以维持安全间距。",
      "action": [
        "Align with the center of the circular passage",
        "Maintain constant forward speed",
        "Go straight",
        "Monitor bottom terrain clearance",
        "Adjust altitude to follow terrain contours",
        "Pass through the circular passage"
      ]
    },
    "images": [
      "data/images/0242.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向上方拉升以越过底部的绿色珊瑚群，随后抵近前方那根直达水面的垂直柱子，注意观察周围环境的动态变化。",
    "output": {
      "reasoning": "图像显示底部有密集的绿色珊瑚群，目标垂直柱子位于正前方。为了安全越过障碍物并抵达目标，机器人需要首先向上拉升高度，随后保持匀速直线航行，同时在接近过程中持续监测周围环境的动态。",
      "action": [
        "Maintain constant speed",
        "Go up to clear the green corals",
        "Go straight towards the vertical pillar",
        "Monitor environment dynamics",
        "Approach the vertical pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0243.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前穿过前方的珊瑚障碍群，注意观察两侧近处的环境变化。",
    "output": {
      "reasoning": "图像显示机器人正前方有一条由左右两侧大型珊瑚礁构成的天然通道。路径前方视线较清晰，但两侧障碍物距离较近。为了执行指令，机器人需要保持当前航向，以恒定速度切入通道，并开启侧向感知以实时监控环境变化。",
      "action": [
        "Maintain current heading",
        "Set constant forward speed",
        "Enter the gap between coral structures",
        "Activate side-view monitoring",
        "Proceed through the obstacle group",
        "Keep stable movement"
      ]
    },
    "images": [
      "data/images/0244.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方的红白色浮标缓慢靠近，保持匀速，同时注意观察左侧珊瑚群的距离。",
    "output": {
      "reasoning": "图像显示红白色浮标位于视野正中心，距离适中。左侧有大片绿色片状珊瑚群，且距离机器人较近。为执行指令，机器人需要保持当前航向，以低速匀速向前推进，同时视觉系统需实时监测左侧珊瑚的边缘距离，防止发生侧向刮擦。",
      "action": [
        "Maintain current heading",
        "Move forward at low constant speed",
        "Monitor distance to left coral group",
        "Slowly approach the red and white buoy",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0245.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右侧绕过正前方巨大的脑纹珊瑚，注意观察近处珊瑚缝隙中的变化。",
    "output": {
      "reasoning": "图像中心被一个巨大的脑纹珊瑚占据，左侧有较小的黄色珊瑚群。根据指令，需要向右侧绕行，路径应避开正前方的障碍物。同时，由于需要观察近处缝隙的变化，机器人应保持平稳的低速移动，以便视觉传感器捕捉细节。",
      "action": [
        "Maintain constant speed",
        "Turn right slightly",
        "Go straight along the right side of the brain coral",
        "Scan nearby coral crevices",
        "Keep distance from the coral surface",
        "Complete bypass"
      ]
    },
    "images": [
      "data/images/0246.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前航行，靠近正前方的石柱进行抵近观察，同时注意避开右侧的蓝色残骸。",
    "output": {
      "reasoning": "图像中心正前方有一根巨大的石柱（目标），右侧海床上有一辆蓝色的汽车残骸（障碍物）。机器人需要保持直线航向，在确保与右侧蓝色残骸维持安全距离的前提下，匀速接近石柱，并在抵达观察距离时减速停止。",
      "action": [
        "Go straight at constant speed",
        "Keep distance from blue wreckage on the right",
        "Approach the stone pillar",
        "Slow down",
        "Stop for close observation"
      ]
    },
    "images": [
      "data/images/0247.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过正前方的石柱并抵近观察那辆蓝色的小车，注意避开左侧的机械残骸。",
    "output": {
      "reasoning": "视觉分析显示：正前方有一根巨大的石柱作为主要障碍物，目标蓝色小车位于石柱的右后方。左侧存在大型六角形机械残骸。为执行指令，机器人需保持匀速，向右侧偏移以避开左侧残骸并绕过石柱，随后转向并接近目标小车进行观察。",
      "action": [
        "Go straight at constant speed",
        "Turn right to bypass the stone pillar",
        "Move forward while keeping distance from the left wreckage",
        "Turn left towards the blue car",
        "Approach the car",
        "Stop and observe"
      ]
    },
    "images": [
      "data/images/0248.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方缓慢推进，避开近处的珊瑚礁群，注意观察上方岩壁的距离并保持匀速。",
    "output": {
      "reasoning": "图像显示机器人位于一个狭窄的水下洞穴或通道中。下方分布着多种形态的珊瑚礁（近处障碍物），左侧和上方则是粗糙的岩壁。为了向左前方推进并避开珊瑚，机器人需要先微调航向向左，同时通过浮力补偿或垂直推进器保持与上方岩壁的安全高度，最后以低功率输出维持匀速前进。",
      "action": [
        "Turn slightly left",
        "Adjust pitch and depth to maintain distance from overhead rock",
        "Slowly increase forward thrust",
        "Steer clear of bottom coral reefs",
        "Maintain constant speed",
        "Monitor distance to left rock wall"
      ]
    },
    "images": [
      "data/images/0249.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方修正航向，避开右侧突出的珊瑚礁，匀速向前方透光的石柱群探索，注意观察近处地形的起伏。",
    "output": {
      "reasoning": "图像右侧有大型且突出的珊瑚礁（紫色带白点及荧光绿部分），前方远处可见透光的石柱群和光束。为避开右侧障碍并抵达目标，机器人需要先向左前方微调航向，随后保持匀速直线航行，同时需降低或调整视角以监测底部起伏的岩石和珊瑚地形。",
      "action": [
        "Turn slightly left",
        "Move forward-left to bypass the coral reef",
        "Maintain constant speed",
        "Adjust pitch to observe bottom terrain",
        "Go straight towards the light-transmitting pillars",
        "Keep distance from the right-side obstacles",
        "Continue exploration"
      ]
    },
    "images": [
      "data/images/0250.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右前方穿过网状障碍物之间的空隙，注意避开垂下的缆绳。",
    "output": {
      "reasoning": "图像显示前方左侧有一个大型绿色圆柱状网状结构，右侧存在明显的空隙。空隙右侧边缘有垂直垂下的缆绳。为了安全穿过，机器人需要向右前方调整航向，对准网状结构与缆绳之间的中心通道，在保持匀速的同时，精确控制路径以避开两侧障碍物。",
      "action": [
        "Adjust heading to the right front",
        "Maintain constant speed",
        "Go straight towards the gap",
        "Keep distance from the left net obstacle",
        "Steer clear of the hanging cable on the right",
        "Pass through the gap",
        "Continue forward"
      ]
    },
    "images": [
      "data/images/0251.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前穿过网状障碍区，抵近前方发光的中心立柱，注意观察两侧缆绳的间距。",
    "output": {
      "reasoning": "图像显示目标发光立柱位于视野正中心，路径被两侧的绿色网状障碍物和垂直缆绳环绕，形成了一个半封闭的通道。为了安全抵达目标，机器人需要保持航向居中以避开两侧缆绳，并以稳定的速度穿过网状区域。",
      "action": [
        "Maintain constant speed",
        "Go straight towards the glowing pillar",
        "Monitor distance to side cables",
        "Keep centered in the mesh tunnel",
        "Approach the central pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0252.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左绕过正前方的红色浮标，保持匀速向远处那台绿色的潜航设备靠近，注意观察右侧礁石的距离。",
    "output": {
      "reasoning": "视觉分析显示：正前方近处有一个巨大的红色球形浮标（障碍物），左上方远处有一台绿色的潜航设备（目标），右侧和左侧均有大型礁石区域。根据指令，需要先向左转向以绕过红色浮标，随后调整航向指向绿色潜航器，并在靠近过程中实时监测右侧礁石的侧向距离以防碰撞。",
      "action": [
        "Turn left",
        "Go straight to bypass the red buoy",
        "Turn right to face the green submersible",
        "Go straight at constant speed",
        "Monitor distance from the reef on the right",
        "Approach the green submersible",
        "Stop"
      ]
    },
    "images": [
      "data/images/0253.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右侧绕过前方的红色浮标，保持匀速前进，并注意避开底部的礁石区域。",
    "output": {
      "reasoning": "图像中心显著位置为一个红色的圆柱形浮标，构成了前方的直接障碍。底部和侧面分布有深色的礁石区域。为执行指令，机器人需要向右侧偏移以避开浮标，同时需精确控制高度以确保不触碰底部的礁石，并最终恢复匀速直线航行。",
      "action": [
        "Turn right slightly",
        "Maintain altitude to clear bottom reefs",
        "Go straight at constant speed",
        "Bypass the red buoy on its right side",
        "Adjust heading back to center",
        "Keep moving forward"
      ]
    },
    "images": [
      "data/images/0254.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近正前方的多孔立柱，注意观察底部缠绕的线缆并保持安全距离。",
    "output": {
      "reasoning": "视觉感知显示正前方有一个高大的多孔立柱，其基座处有一条明显的带条纹线缆。目标位于机器人正前方，路径清晰。机器人需要保持直线匀速航行，在接近过程中重点观察底部的线缆分布，并确保在触及线缆或立柱前停止，以保持安全作业距离。",
      "action": [
        "Go straight at constant speed",
        "Approach the porous pillar",
        "Monitor the cables at the base",
        "Slow down as distance decreases",
        "Stop at a safe distance"
      ]
    },
    "images": [
      "data/images/0255.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过前方的线缆群并靠近中心立柱，注意避开右侧的大型柱状体。",
    "output": {
      "reasoning": "图像右侧占据了大量空间的是大型柱状体，前方则是密集的线缆群。中心立柱位于线缆群后方的中景位置。为安全执行指令，机器人需要保持向前的动力，同时航向略微向左偏移以避开右侧障碍物，随后谨慎穿过线缆区域，最终减速靠近目标立柱。",
      "action": [
        "Maintain steady forward speed",
        "Steer slightly left to avoid the large column on the right",
        "Navigate through the cable group",
        "Approach the central pillar",
        "Stop and hover"
      ]
    },
    "images": [
      "data/images/0256.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前方球形珊瑚的中空区域靠近进行细节探测，注意观察内部的动态变化。",
    "output": {
      "reasoning": "图像正前方是一个巨大的球形珊瑚，其左侧有一个明显的半开放中空区域，内部可见绿色的褶皱状结构。路径前方开阔，无明显障碍物。任务要求保持匀速靠近该开口并进行细节观察。",
      "action": [
        "Maintain constant speed",
        "Go straight",
        "Adjust heading to face the hollow opening",
        "Slowly approach the interior structure",
        "Hover and observe dynamic changes",
        "Stop"
      ]
    },
    "images": [
      "data/images/0257.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持低速向前，靠近观察正前方的绿色珊瑚结构，注意避开底部的凸起物。",
    "output": {
      "reasoning": "图像正前方是一个巨大的绿色珊瑚结构，目标明确。在机器人与目标之间的底部路径上存在明显的珊瑚礁凸起物（障碍物）。规划逻辑为：首先以低速启动并保持直行，同时微调高度或路径以确保避开底部的凸起，随后减速接近目标珊瑚，最后悬停进行观察。",
      "action": [
        "Go straight at low speed",
        "Adjust altitude to clear bottom protrusions",
        "Approach the green coral structure",
        "Slow down",
        "Stop and hover for observation"
      ]
    },
    "images": [
      "data/images/0258.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着绿色管道继续向前匀速巡检，注意观察管壁连接处并保持与管道的安全距离。",
    "output": {
      "reasoning": "图像显示绿色管道向右前方延伸，管壁连接处（黑色环状结构）清晰可见。机器人当前位于管道左侧，光照充足，路径无障碍。为了执行巡检，需要先微调航向与管道轴线平行，随后保持恒定的侧向安全距离匀速前进，并在经过每个连接处时进行重点视觉特征提取。",
      "action": [
        "Align with the pipe direction",
        "Maintain safe lateral distance",
        "Go straight at constant speed",
        "Identify and inspect the pipe joint",
        "Continue forward tracking the pipe"
      ]
    },
    "images": [
      "data/images/0259.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，沿右侧大型柱状结构进行抵近观察，注意保持安全距离并留意左前方舱门处的动态变化。",
    "output": {
      "reasoning": "当前视野右侧为大型黑色柱状结构，左前方远处可见一处开启的舱门。机器人当前位于结构左侧的平坦底座上。为了执行指令，需要保持匀速前进，同时向右微调航向以靠近柱状结构进行观察，并持续监测左前方舱门的动态。",
      "action": [
        "Move forward at constant speed",
        "Slightly steer right to approach the cylindrical structure",
        "Maintain parallel movement along the right structure",
        "Keep safe distance from the cylinder surface",
        "Monitor the front-left hatch area",
        "Continue steady advancement"
      ]
    },
    "images": [
      "data/images/0260.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近锈蚀车辆，同时注意避开右侧的球形机械障碍物，保持匀速前进。",
    "output": {
      "reasoning": "视觉分析显示，目标锈蚀车辆位于视野正前方偏左位置，而球形机械障碍物位于右侧。为了安全执行指令，机器人需要微调航向以确保与右侧障碍物保持安全距离，随后以恒定慢速向前推进，最终靠近目标车辆。",
      "action": [
        "Adjust heading slightly to the left",
        "Move forward slowly",
        "Maintain constant speed",
        "Keep distance from the spherical obstacle on the right",
        "Approach the rusted vehicle",
        "Stop at a safe distance"
      ]
    },
    "images": [
      "data/images/0261.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近前方带绿色锈迹的圆筒结构，准备进入其内部进行抵近观察，注意保持航行平稳。",
    "output": {
      "reasoning": "图像中心是一个带有绿色锈迹的大型圆筒状结构，开口正对相机视角，路径清晰无障碍。为实现平稳进入并抵近观察，机器人需要首先精确对准圆筒中心轴线，随后以低速推进，同时实时补偿水流影响以保持航行平稳，最后进入内部并悬停。",
      "action": [
        "Align with the center of the cylinder opening",
        "Move forward slowly",
        "Maintain steady depth and attitude",
        "Enter the cylindrical structure",
        "Hover for close observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0262.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近绿色的半球形设备，保持匀速，注意避开左侧的黄色海绵。",
    "output": {
      "reasoning": "图像中心偏右位置有一个绿色的半球形设备（目标），左下角存在一个明显的黄色海绵（障碍物）。为执行指令，机器人需要先向右微调以确保与左侧黄色海绵保持安全距离，随后匀速直线前进靠近绿色设备，并在接近目标后停止。",
      "action": [
        "Adjust heading slightly to the right",
        "Go straight at constant speed",
        "Keep distance from the yellow sponge on the left",
        "Approach the green hemispherical device",
        "Stop"
      ]
    },
    "images": [
      "data/images/0263.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方黄色目标缓慢靠近，注意避开左侧上方的岩壁阴影，保持匀速前进并观察周边动态。",
    "output": {
      "reasoning": "视觉感知显示，黄色目标物位于机器人正前方，路径相对开阔。图像左上方存在明显的岩壁及其投射的深色阴影，需在前进时保持航向偏右以确保安全间距。光照条件良好但存在强光直射，需匀速慢速接近以维持视觉定位稳定并观察周边环境。",
      "action": [
        "Adjust heading slightly to the right",
        "Move forward slowly",
        "Maintain steady speed",
        "Keep distance from left rock shadow",
        "Monitor surroundings",
        "Approach yellow target",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0264.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方发光的石柱抵近，保持匀速并注意观察左侧上方悬挂物的状态。",
    "output": {
      "reasoning": "视觉分析显示，发光石柱位于机器人正前方，路径相对开阔。左上方有一处巨大的深色悬挂岩石结构。根据指令，机器人需要保持直线航行以接近石柱，同时在移动过程中将视觉注意力或传感器监测范围覆盖至左上方悬挂物，并确保推进速度平稳。",
      "action": [
        "Go straight towards the glowing pillar",
        "Maintain constant speed",
        "Monitor the hanging object on the upper left",
        "Approach the pillar",
        "Stop near the target"
      ]
    },
    "images": [
      "data/images/0265.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "匀速向前靠近那根发光的绿色石柱，注意观察其表面细节并保持姿态稳定。",
    "output": {
      "reasoning": "目标发光绿色石柱位于视野正前方，距离适中，路径上无明显障碍物。为了满足指令中‘观察细节’和‘姿态稳定’的要求，机器人需要保持直线匀速航行，并在接近过程中逐渐减速以减少动态模糊，确保视觉传感器能捕捉到清晰的表面纹理。",
      "action": [
        "Go straight",
        "Maintain constant speed",
        "Approach the green pillar",
        "Slow down for detailed observation",
        "Keep posture stable",
        "Stop at a safe distance"
      ]
    },
    "images": [
      "data/images/0266.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近前方的绿色目标进行侦察，注意避开下方的珊瑚礁并留意上方光线的变化。",
    "output": {
      "reasoning": "视觉分析显示，绿色目标（潜航器状物体）位于前方视野中心偏右位置。下方存在密集的珊瑚礁群，构成主要障碍。上方有强烈的折射光线，可能影响视觉感知。规划路径需在保持深度的同时向目标靠拢，并保持匀速以确保侦察质量。",
      "action": [
        "Go straight at constant speed",
        "Maintain current depth to clear coral reefs",
        "Adjust heading towards the green target",
        "Approach target slowly",
        "Perform reconnaissance scan",
        "Monitor ambient light intensity",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0267.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方缓慢推进，避开近处的大型植被，抵近侦察潜艇的指挥塔，注意保持姿态稳定。",
    "output": {
      "reasoning": "图像显示左侧和近景处有大面积的绿色叶片状植被，构成主要障碍。目标潜艇位于中景，其指挥塔（帆罩）垂直向上延伸。为执行指令，机器人需先向右微调以避开左侧植被，随后以低速向右前方推进，并在接近指挥塔时保持动力平衡以确保姿态稳定。",
      "action": [
        "Turn slightly right to clear the vegetation",
        "Move forward slowly towards the submarine",
        "Adjust pitch and roll to maintain stability",
        "Approach the command tower",
        "Hover and perform reconnaissance",
        "Stop"
      ]
    },
    "images": [
      "data/images/0268.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，注意避让左侧近处的珊瑚，直接向前方圆柱形残骸的开口处靠近并进行内部探测。",
    "output": {
      "reasoning": "图像左前方近处存在黄色珊瑚礁，属于障碍物；正前方是目标圆柱形残骸，其开口清晰可见。任务要求避开左侧障碍物并靠近开口。规划路径为：首先保持恒定航速，小幅向右修正航向以避开左侧珊瑚，随后对准残骸开口直线航行，接近后切换至探测模式。",
      "action": [
        "Maintain constant speed",
        "Slightly turn right to avoid left coral",
        "Go straight towards the wreckage opening",
        "Close to the opening",
        "Enter for internal detection",
        "Stop"
      ]
    },
    "images": [
      "data/images/0269.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着前方的长形结构一直向前航行，保持匀速，注意观察表面的孔洞细节。",
    "output": {
      "reasoning": "图像中心显示一个带有大量规则孔洞的长形圆柱状结构，该结构向前方延伸，路径清晰且无明显障碍物。为了观察表面细节并保持匀速航行，机器人需要对准结构的纵向轴线，维持稳定的推进动力，并保持与结构表面的适当距离以确保视觉观测质量。",
      "action": [
        "Align with the longitudinal axis of the structure",
        "Move forward at a constant speed",
        "Maintain a stable distance from the surface",
        "Scan and observe surface holes",
        "Continue straight along the structure"
      ]
    },
    "images": [
      "data/images/0270.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向右绕过左侧巨大的珊瑚礁，向光线透出的深处水域探索，注意避开底部的发光障碍物。",
    "output": {
      "reasoning": "图像左侧有一个巨大的纹理复杂的珊瑚礁，底部散布着多个发光的圆形障碍物。上方中心区域有明显的光束透出。根据指令，机器人需要向右侧偏移以绕过左侧巨大的珊瑚，同时需要控制高度或路径避开底部的发光体，最后向光亮处匀速推进。",
      "action": [
        "Maintain constant speed",
        "Turn right to bypass the large coral reef",
        "Adjust altitude to clear bottom glowing obstacles",
        "Go straight towards the light source",
        "Explore deep water area",
        "Keep monitoring obstacles"
      ]
    },
    "images": [
      "data/images/0271.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近那个带有绿色亮点的巨大结构，注意保持匀速并观察近处的动态变化。",
    "output": {
      "reasoning": "图像正前方是一个带有多个绿色发光孔洞的巨大结构，这是指令中的目标。前景中有一个发光的圆盘状物体，在前进过程中需要注意避让或保持高度以防碰撞。为了满足‘缓慢’、‘匀速’和‘观察’的要求，动作序列将分解为对准、低速推进和持续的环境监测。",
      "action": [
        "Align with the large structure",
        "Set speed to slow and constant",
        "Move forward slowly",
        "Monitor the foreground glowing object",
        "Scan for dynamic changes in the environment",
        "Maintain steady approach",
        "Stop at a safe distance from the structure"
      ]
    },
    "images": [
      "data/images/0272.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前缓慢靠近中心位置的红色浮标，注意避开右侧的大型结构，保持匀速前进。",
    "output": {
      "reasoning": "图像中心显示一个悬挂的红色浮标（目标），右侧有一个巨大的红色圆柱形结构（障碍物）。为了安全靠近目标，机器人需要保持航向对准中心浮标，同时在前进过程中略微向左偏移或保持中轴线行驶，以确保与右侧大型结构保持安全距离，并以匀速、缓慢的节奏接近目标。",
      "action": [
        "Align with the central red buoy",
        "Keep distance from the large structure on the right",
        "Go straight slowly",
        "Maintain constant speed",
        "Slow down as approaching the buoy",
        "Stop"
      ]
    },
    "images": [
      "data/images/0273.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近中心的小型圆柱目标，保持匀速并注意避开上方的大型网状结构。",
    "output": {
      "reasoning": "图像中心清晰可见一个小型圆柱形目标，而上方覆盖着一个大型的带红色条纹的网状结构。为了安全执行指令，机器人需要保持当前深度（或略微下潜）以确保与上方障碍物留有安全间距，随后匀速直线前进靠近目标。",
      "action": [
        "Maintain current depth to avoid overhead mesh structure",
        "Go straight at a constant speed",
        "Approach the small cylindrical target in the center",
        "Slow down and stop in front of the target"
      ]
    },
    "images": [
      "data/images/0274.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前穿过两根石柱之间的开阔地带，保持匀速，同时密切关注右侧光照区域的动态变化。",
    "output": {
      "reasoning": "视觉感知显示前方左右两侧各有一根巨大的雕刻石柱，中间区域开阔且无障碍物。右侧石柱正处于上方投射的强光束中，是指令要求的重点监控区域。机器人需要对准中心路径，在保持匀速前进的同时，将视觉注意力或传感器偏向右侧光照区进行实时监测。",
      "action": [
        "Align with the center of the two pillars",
        "Move forward at a constant speed",
        "Monitor the illuminated area on the right",
        "Pass through the gap",
        "Stop"
      ]
    },
    "images": [
      "data/images/0275.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，抵近观察正前方的石柱，同时注意避开右侧的礁石堆。",
    "output": {
      "reasoning": "图像中心正前方有一根受上方光照的石柱，为主要观察目标。右侧可见明显的礁石堆，需在前进过程中保持左偏航向以确保避障安全。路径规划为：匀速启动，向左微调避开右侧礁石，随后直线抵近石柱并停止观察。",
      "action": [
        "Move forward at constant speed",
        "Steer slightly left to avoid right-side reef",
        "Continue straight toward the stone pillar",
        "Slow down and approach target",
        "Stop for observation"
      ]
    },
    "images": [
      "data/images/0276.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右绕过前方的绿色大型结构体并继续向前探索，注意保持匀速并避开上方的缆绳。",
    "output": {
      "reasoning": "图像前方占据主要视野的是一个绿色大型圆柱形结构体，其右上方有一条明显的斜向缆绳。为了执行指令，机器人需要先向右转向以避开结构体主体，同时在移动过程中保持深度或稍微下潜，以确保与上方缆绳保持安全距离。绕过结构体边缘后，需修正航向并保持匀速向前探测。",
      "action": [
        "Turn right",
        "Maintain depth and constant speed",
        "Move forward to bypass the green structure",
        "Avoid the cable above",
        "Turn left to correct heading",
        "Go straight for exploration"
      ]
    },
    "images": [
      "data/images/0277.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方的编织塔结构靠近，并注意避开右侧的弧形网状障碍。",
    "output": {
      "reasoning": "视觉分析显示，目标编织塔结构位于视野中心偏左位置，而右侧有一片大面积的弧形网状障碍物。为了执行指令，机器人需要首先微调航向以确保与右侧障碍物保持安全距离，随后保持恒定速度向塔基方向直线推进。",
      "action": [
        "Adjust heading slightly to the left",
        "Maintain constant speed",
        "Go straight towards the woven tower",
        "Monitor distance from the right mesh obstacle",
        "Approach the tower base",
        "Stop and hover"
      ]
    },
    "images": [
      "data/images/0278.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着主缆绳匀速向前推进，避开两侧的珊瑚结构，向前方发绿光的基站靠近，注意保持高度稳定。",
    "output": {
      "reasoning": "图像显示主缆绳从视野下方延伸至远方，两侧分布着明显的黄色珊瑚结构。前方背景中心可见发出绿光的基站。路径相对清晰，机器人需要首先锁定主缆绳方向并稳定高度，随后沿缆绳中轴线匀速推进，同时与两侧的珊瑚保持安全距离，最终抵达绿光基站。",
      "action": [
        "Maintain stable altitude",
        "Align with the main cable",
        "Go straight along the cable at constant speed",
        "Keep distance from corals on both sides",
        "Approach the green-lit base station",
        "Slow down and hover near the station"
      ]
    },
    "images": [
      "data/images/0279.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，沿着正前方的黑色线缆向光源处探索，注意避开近处底部的绿色珊瑚礁。",
    "output": {
      "reasoning": "图像显示正前方有两条平行的黑色线缆延伸至远方的强光源。近处底部（左下及正下方）分布有密集的绿色发光珊瑚礁，构成直接的路径障碍。为了安全执行指令，机器人需要先略微上浮以越过近处的珊瑚礁，随后对准线缆中心，保持恒定速度向光源方向推进。",
      "action": [
        "Ascend slightly to clear the green corals",
        "Align with the central black cable",
        "Move forward at constant speed",
        "Follow the cable path towards the light source",
        "Maintain altitude to avoid seabed obstacles"
      ]
    },
    "images": [
      "data/images/0280.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右侧绕过前方的彩色浮标，保持匀速前进，同时注意避开底部的珊瑚礁，向深水区继续探索。",
    "output": {
      "reasoning": "图像显示彩色浮标位于机器人正前方偏左位置，底部散布着绿色的珊瑚礁结构。为了向右绕过浮标，机器人需要先向右调整航向。同时，为了避开底部的珊瑚礁，需要保持当前的悬浮高度或略微上升。绕过障碍物后，应指向背景中光线较暗的深水区域，保持稳定速度继续航行。",
      "action": [
        "Turn right",
        "Maintain altitude to avoid bottom corals",
        "Go straight at constant speed",
        "Bypass the buoy",
        "Head towards deep water area"
      ]
    },
    "images": [
      "data/images/0281.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向右前方航行，避开红色的浮标和底部的珊瑚群，保持匀速前进，注意观察前方管道的结构变化。",
    "output": {
      "reasoning": "图像显示前方有一根巨大的绿色管道，其左下方悬挂着一个红色浮标，底部则分布着密集的珊瑚群。为实现向右前方航行并避开障碍，机器人需要先向右转向以远离红色浮标，同时保持或提升高度以跨越底部的珊瑚群，随后沿管道平行方向匀速前进，并持续监测管道表面的结构细节。",
      "action": [
        "Turn right",
        "Ascend slightly to clear corals",
        "Move forward at constant speed",
        "Keep distance from the red buoy",
        "Scan pipe structure",
        "Continue sailing front-right"
      ]
    },
    "images": [
      "data/images/0282.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方巨大的黑色岩石结构靠近，保持匀速并注意避开左侧的浅色珊瑚礁。",
    "output": {
      "reasoning": "图像中心显示一个巨大的黑色分形状岩石结构，这是目标物。左下角存在明显的浅色珊瑚礁，需作为障碍物避开。路径规划应先向右微调航向以远离左侧珊瑚礁，随后保持匀速直线推进，并在接近目标结构时减速停止。",
      "action": [
        "Adjust heading slightly to the right",
        "Move forward at constant speed",
        "Keep clear of the light-colored coral reef on the left",
        "Go straight towards the black rock structure",
        "Slow down as approaching",
        "Stop"
      ]
    },
    "images": [
      "data/images/0283.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，直接向正前方的异形柱状结构靠近，注意避开两侧的珊瑚礁石并观察其基座情况。",
    "output": {
      "reasoning": "图像中心是一个明显的异形柱状结构，目标明确。左右两侧分别有大型的珊瑚礁石（左侧为浅色多孔状，右侧为深色岩石状），形成了天然的狭窄通道。为了执行指令，机器人需要保持航向居中以避开两侧障碍，匀速向前方推进，并在接近目标基座时进行细节观察。",
      "action": [
        "Align with the central pillar structure",
        "Maintain constant speed and go straight",
        "Keep a safe distance from the left coral reef",
        "Keep a safe distance from the right rock formation",
        "Approach the base of the pillar",
        "Slow down and observe the base condition",
        "Stop"
      ]
    },
    "images": [
      "data/images/0284.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "绕过正前方的石柱向深处的遗迹群探索，保持匀速并注意观察右侧的生物动态。",
    "output": {
      "reasoning": "图像中心有一座巨大的石柱阻挡了直接向前的路径，深处可见散落的遗迹群。右侧有一条鱼类生物正在游动。为了执行指令，机器人需要先向右侧偏移以绕过石柱，这样既能避开障碍，又能方便观察右侧的生物动态，随后再修正航向指向深处的遗迹。整个过程需保持动力输出平稳以维持匀速。",
      "action": [
        "Move forward at steady speed",
        "Turn right slightly to bypass the central pillar",
        "Observe the creature on the right side",
        "Go straight to pass the pillar",
        "Turn left to face the distant ruins",
        "Go straight towards the ruins",
        "Maintain steady speed and continue exploration"
      ]
    },
    "images": [
      "data/images/0285.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从左侧齿轮残骸与右侧高塔之间的空隙穿过，注意避开底部的礁石并向深处探索。",
    "output": {
      "reasoning": "图像显示左前方有一个巨大的齿轮状残骸，右侧是一座高耸的圆柱形石塔，两者之间形成了一个清晰的通道。底部散布着零星的礁石和碎石。为了执行指令，机器人需要首先调整高度以确保避开底部的礁石，随后对准齿轮与高塔之间的间隙，保持稳定速度穿过，并继续向光线延伸的深水区域航行。",
      "action": [
        "Adjust altitude to clear bottom rocks",
        "Align with the gap between the gear wreckage and the tower",
        "Go straight at constant speed",
        "Pass through the gap",
        "Move forward into the deep area",
        "Maintain forward exploration"
      ]
    },
    "images": [
      "data/images/0286.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方的光源中心点直线穿行，注意避开周围密集的管状障碍物。",
    "output": {
      "reasoning": "图像显示正前方有一个明亮的光源中心点，周围环绕着大量带有黄绿条纹的管状障碍物。目前中心路径相对开阔，机器人需要锁定该光源作为导航目标，保持当前航向并以恒定速度直线前进，同时需实时监测与周围管状物边缘的距离以确保安全避障。",
      "action": [
        "Lock on the central light source",
        "Set constant forward speed",
        "Go straight towards the center",
        "Monitor distance to surrounding tubular obstacles",
        "Maintain heading and speed",
        "Reach the light source center"
      ]
    },
    "images": [
      "data/images/0287.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向正前方发绿光的线缆簇中心缓慢靠近，注意避开两侧密集的网状结构。",
    "output": {
      "reasoning": "图像中心显示一个发绿光的线缆簇目标，其位于机器人的正前方。左右两侧分布着密集的绿色网状结构，构成了狭窄的通行空间。为了安全执行任务，机器人需要保持航向对准发光中心，以恒定低速前进，并实时监测与两侧网状障碍物的距离。",
      "action": [
        "Align heading with the center of the green cable cluster",
        "Set constant low speed",
        "Go straight forward",
        "Monitor distance to left and right net structures",
        "Slowly approach the target center",
        "Stop at safe distance"
      ]
    },
    "images": [
      "data/images/0288.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，靠近中心发绿光的柱状体进行探测，注意避开左侧的缆绳网并留意近处的动态变化。",
    "output": {
      "reasoning": "图像中心是一个发绿光的柱状目标，左侧有大面积的缆绳网障碍物，右下方可见一条条纹鱼（动态物体）。为了安全执行任务，机器人需要保持匀速，路径需略微向右偏移以远离左侧缆绳网，并在接近过程中持续监测右侧生物的动态。",
      "action": [
        "Maintain constant speed",
        "Slightly steer right to clear the left cable net",
        "Go straight towards the green glowing pillar",
        "Monitor nearby dynamic objects",
        "Approach the pillar",
        "Stop and stabilize for detection"
      ]
    },
    "images": [
      "data/images/0289.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方沉没的红色车辆靠近进行抵近侦察，保持匀速并注意观察周边环境的动态变化。",
    "output": {
      "reasoning": "图像显示沉没的红色车辆位于视野正前方海底，路径相对开阔。上方有一个大型红色圆形悬浮结构，在下潜靠近过程中需注意其位置变化。为了完成抵近侦察，机器人需要平稳下潜并向前推进，同时保持对周边水域的动态观察。",
      "action": [
        "Move forward and descend slowly",
        "Maintain constant speed towards the red car",
        "Scan surroundings for dynamic changes",
        "Approach the vehicle front",
        "Hover for close reconnaissance",
        "Stop"
      ]
    },
    "images": [
      "data/images/0290.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前方带有海藻的红色浮标靠近，保持匀速，注意观察左侧红色残骸的边缘。",
    "output": {
      "reasoning": "图像中心位置有一个带有海藻向上生长的红色浮标，这是主要目标。左侧近处有一个巨大的红色金属残骸。路径规划需要机器人在向中心浮标匀速前进的同时，保持与左侧残骸的安全距离并对其边缘进行视觉监控。",
      "action": [
        "Adjust heading to center red buoy",
        "Move forward at constant speed",
        "Monitor the edge of the red wreckage on the left",
        "Maintain safe distance from left obstacle",
        "Slow down as approaching the buoy",
        "Stop in front of target"
      ]
    },
    "images": [
      "data/images/0291.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从正前方石柱的左侧绕过去，探索后方的建筑群，注意观察近处石块的分布。",
    "output": {
      "reasoning": "图像中心有一根巨大的石柱作为主要障碍物，左后方可见明显的建筑群遗迹。近处沙地上散落着数块岩石。按照指令，机器人需要保持匀速，首先识别并避开近处的岩石，随后向左前方转向以绕过中心石柱，最后直线前往后方的建筑区域进行探索。",
      "action": [
        "Maintain constant speed forward",
        "Scan and monitor nearby rocks",
        "Turn slightly left",
        "Go straight to bypass the central pillar",
        "Approach the background architectural complex",
        "Slow down and observe",
        "Stop"
      ]
    },
    "images": [
      "data/images/0292.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，穿过石柱阵列，向前方光影下的蓝色珊瑚区探索，注意保持与右侧大型石柱的距离。",
    "output": {
      "reasoning": "图像显示前方是一个由石柱构成的阵列，右侧有一个巨大的石柱占据了显著的近景空间。目标蓝色珊瑚区位于画面中心偏右、光影投射的深处。为了安全穿过阵列并到达目标，机器人需要保持匀速前进，同时在路径规划上略微偏左以避开右侧巨大的石柱，随后穿过石柱间的缝隙，最终接近蓝色珊瑚区进行探索。",
      "action": [
        "Maintain constant speed forward",
        "Slightly adjust heading to the left to keep distance from the right pillar",
        "Navigate through the pillar array",
        "Head towards the blue coral area under the light rays",
        "Approach the target area",
        "Start exploration"
      ]
    },
    "images": [
      "data/images/0293.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，绕过正前方巨大的珊瑚礁群，向深处的光源方向探索。",
    "output": {
      "reasoning": "图像显示正前方有一个巨大的、具有复杂纹理的珊瑚礁群，完全阻挡了直线路径。上方和背景处有明显的光源。为了绕过障碍物并向光源探索，机器人需要先保持匀速接近，然后向一侧（如左侧）转向以避开珊瑚礁中心，绕行后再重新调整航向对准光源方向推进。",
      "action": [
        "Go straight at constant speed",
        "Turn left to bypass the central reef",
        "Go straight along the reef edge",
        "Turn right to realign with the light source",
        "Move forward towards the light",
        "Maintain exploration speed"
      ]
    },
    "images": [
      "data/images/0294.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向左前方绕过中心的大型纹理结构，朝着远处的亮光处缓慢前进，注意保持航行平稳。",
    "output": {
      "reasoning": "视野中心被一个巨大的、具有波纹状纹理的暗色结构占据，该结构阻挡了直接通往远处光源的路径。光源位于左前方的隧道深处。为了绕过障碍物并保持平稳，机器人需要先向左侧转向，在避开中心结构边缘后，重新校准航向对准光源，并以低速推进以确保航行稳定性。",
      "action": [
        "Turn left",
        "Move forward to bypass the central structure",
        "Adjust heading towards the distant light",
        "Move forward slowly",
        "Maintain steady buoyancy and speed",
        "Keep approaching the light source"
      ]
    },
    "images": [
      "data/images/0295.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过左侧的卷曲珊瑚，向右侧那根高大的石柱靠近。",
    "output": {
      "reasoning": "图像左前方有一个明显的卷曲状珊瑚，右侧背景中有一根垂直的高大石柱。根据指令，机器人需要先保持匀速前进，向右侧微调航向以绕过左侧的珊瑚障碍，随后重新对准右侧石柱并执行接近动作。",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Bypass the curled coral",
        "Turn right",
        "Approach the tall stone pillar",
        "Stop"
      ]
    },
    "images": [
      "data/images/0296.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着左侧的线缆管道匀速向前推进，注意避开底部散落的小型珊瑚群，向深处的亮光区域进行探索。",
    "output": {
      "reasoning": "图像左侧可见明显的粗大线缆管道，底部散布着数个深色的珊瑚或岩石簇，右上角有强烈的点状光源。为了完成任务，机器人需要以左侧管道为参照物，在保持安全高度（避开底部珊瑚）的同时，向画面深处的亮光区域匀速推进。",
      "action": [
        "Align with the left cable pipe",
        "Maintain altitude to avoid bottom coral clusters",
        "Move forward at constant speed",
        "Navigate towards the bright light source",
        "Stop"
      ]
    },
    "images": [
      "data/images/0297.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前推进，避开左侧茂密的珊瑚丛，向光线透射的开阔区域探索，注意观察近处环境的细微变化。",
    "output": {
      "reasoning": "图像左侧近景存在密集的鹿角状珊瑚，属于避障区域；光线从上方和中心偏右区域透射，指示了开阔水域的方向。机器人需要向右前方微调航向以绕开左侧珊瑚，同时保持恒定速度向光源方向推进，并维持对近处环境细节的传感器监测。",
      "action": [
        "Maintain constant speed forward",
        "Steer slightly right to avoid left coral clusters",
        "Move towards the light beams",
        "Scan nearby environment for subtle changes",
        "Explore towards open water"
      ]
    },
    "images": [
      "data/images/0298.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，从右侧分叉珊瑚的左侧缝隙穿过去，注意观察近处珊瑚礁的动态变化。",
    "output": {
      "reasoning": "图像右侧有一株明显的蓝色分叉珊瑚，其左侧与中心区域的珊瑚礁之间形成了一个可通行的缝隙。为了执行指令，机器人需要锁定该缝隙作为航路点，在保持匀速前进的同时，实时监测近处（底部及左侧）珊瑚礁的边缘动态，确保安全通过。",
      "action": [
        "Maintain constant speed",
        "Align with the gap on the left of the blue branching coral",
        "Move forward steadily",
        "Monitor nearby coral reefs for dynamic changes",
        "Pass through the gap",
        "Keep cruising"
      ]
    },
    "images": [
      "data/images/0299.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向前靠近中心的发光珊瑚体，保持匀速并注意底部的绿色基座，准备进行近距离观察。",
    "output": {
      "reasoning": "图像中心是一个巨大的粉紫色发光珊瑚体，底部带有明显的绿色荧光六角形基座。目标位于机器人正前方，路径清晰无障碍。根据指令，需要执行直线航行，同时通过视觉反馈持续监测与绿色基座的距离，确保匀速接近并在安全距离内停止以进行观察。",
      "action": [
        "Go straight at constant speed",
        "Monitor the green base distance",
        "Slow down as approaching the base",
        "Adjust position to center the coral",
        "Stop and hover for close-up observation"
      ]
    },
    "images": [
      "data/images/0300.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "向正前方的大型发光珊瑚礁抵近观察，保持匀速并注意避开底部的礁石群。",
    "output": {
      "reasoning": "图像中心是一个大型的球形发光珊瑚礁，目标明确且位于正前方。底部区域分布着较暗的礁石群，属于潜在障碍。规划路径时需保持当前航向，微调垂直高度以确保与底部礁石保持安全间距，随后匀速推进并在接近目标时减速悬停。",
      "action": [
        "Adjust depth to clear bottom reefs",
        "Maintain heading towards glowing coral",
        "Go straight at constant speed",
        "Slow down as approaching target",
        "Hover for observation",
        "Stop"
      ]
    },
    "images": [
      "data/images/0301.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，从左侧巨大的石柱旁绕过，注意避开底部的岩石，向深处的光影交汇处继续探索。",
    "output": {
      "reasoning": "图像显示正前方偏左位置有一根巨大的绿色方块状石柱，底部及右侧地面散布着岩石障碍。深处右上角有明显的光影交汇区域。根据指令，机器人需要先向左偏移以绕过石柱，同时保持匀速并注意避开底部的岩石，最后调整航向指向深处的光源方向。",
      "action": [
        "Go straight at constant speed",
        "Turn left to bypass the giant pillar",
        "Keep distance from bottom rocks",
        "Pass the pillar on the left side",
        "Turn right towards the light intersection",
        "Continue exploration into the deep"
      ]
    },
    "images": [
      "data/images/0302.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，绕过左侧巨大的石柱，向前方光影交汇处的岩石群靠近，注意观察近处底部的环境变化。",
    "output": {
      "reasoning": "图像左侧近处有一根巨大的石柱（障碍物），目标岩石群位于前方光影交汇的深处。机器人需要保持匀速，向右前方微调航向以绕过石柱，同时在移动过程中俯视观察底部沙石环境的变化，最终抵达光影处的岩石区域。",
      "action": [
        "Go straight at constant speed",
        "Turn slightly right to bypass the left pillar",
        "Tilt camera down to observe seabed environment",
        "Move toward the illuminated rock group ahead",
        "Approach the target rocks",
        "Stop"
      ]
    },
    "images": [
      "data/images/0303.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前方开阔区域推进，注意避开底部的零星礁石，并留意上方水面的光影变化。",
    "output": {
      "reasoning": "图像显示前方海域视野较为开阔，水深适中。潜水器位于沙质底质上方，底部可见零星暗色礁石。上方水面波纹清晰，光影投射明显。为了执行指令，机器人需要微调高度以确保与底部礁石保持安全距离，随后设定恒定功率向前推进，并开启上方传感器监测光照波动。",
      "action": [
        "Maintain safe altitude above seabed",
        "Set constant forward speed",
        "Go straight towards open water",
        "Scan and bypass bottom reefs",
        "Monitor surface light patterns",
        "Maintain steady cruise"
      ]
    },
    "images": [
      "data/images/0304.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速向前，缓慢进入前方绿色潜航器残骸的开口处进行内部探测，注意观察舱内环境变化。",
    "output": {
      "reasoning": "目标绿色潜航器残骸位于正前方，其开口（舱门）正对机器人，路径清晰无障碍。为执行内部探测，机器人需要首先精确对准开口中心，随后按照指令保持低速匀速前进，安全穿过开口进入舱体内部，并在进入后稳定姿态进行环境观察。",
      "action": [
        "Align with the center of the green submersible opening",
        "Move forward at a constant slow speed",
        "Enter the hatch slowly",
        "Stabilize and hover inside the cabin",
        "Observe and scan the internal environment"
      ]
    },
    "images": [
      "data/images/0305.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "保持匀速，向前穿过前方的蓝色管道，同时注意观察左侧珊瑚礁的动态变化。",
    "output": {
      "reasoning": "图像显示正前方有一个带有黄色条纹的蓝色管道入口，路径清晰。左侧分布着大面积的珊瑚礁。根据指令，机器人需要保持稳定速度直线前进以穿过管道，同时在移动过程中持续监测左侧珊瑚礁区域的状况。",
      "action": [
        "Maintain constant speed",
        "Go straight towards the blue pipe entrance",
        "Monitor the coral reef on the left",
        "Enter the blue pipe",
        "Pass through the pipe",
        "Stop"
      ]
    },
    "images": [
      "data/images/0306.png"
    ]
  },
  {
    "instruction": "# Role\n你是一个先进的水下机器人纯视觉-语言-动作（VLA）任务规划模型。你的核心能力是根据单目RGB图像（视觉输入）和用户的自然语言指令（instruction），将一个宏观的复杂任务拆解为一系列逻辑严密、可执行的自然语言原子步骤列表。\n\n# Context & Constraints\n1. **环境**：水下环境，可能存在光照不均、浑浊或动态障碍物。\n2. **输入**：\n    - 当前视角的单目 RGB 图像（你需要分析图像中的物体位置、朝向、障碍物）。\n    - 用户的宏观指令（例如：“避开红色障碍物向右移动”或“接近侧面的岩石”）。\n3. **输出限制**：\n    - 输出必须符合提供的 JSON Schema 格式。\n    - action 字段必须是一个字符串列表 (List of Strings)。\n    - 步骤描述应简洁、自然、动作性强（例如：\"Turn right\", \"Go straight\"）。\n\n# Workflow\n在生成输出前，请在内心进行以下推理：\n1. 视觉感知：识别图像中的关键目标（Target）和障碍物（Obstacles）。\n2. 空间推理：判断机器人与目标的相对位置（左侧？前方？距离远近？）。\n3. 规划拆解：根据相对位置，规划路径并将其切分为连续动作。\n\n# Examples\n## Example 1\nUser Instruction: \"接近侧面的岩石\"\nImage Analysis: 岩石位于右前方，无障碍。\nOutput: \n{\n  \"reasoning\": \"目标岩石在右前方约2米处，路径无障碍，首先转向并接近。\",\n  \"action\": [\"Turn right\", \"Go straight\", \"Close to target\", \"Stop\"]\n}\n\n## Example 2\nUser Instruction: \"寻找并检查黄色的管道\"\nImage Analysis: 当前视野未见管道，左侧有模糊阴影。\nOutput:\n{\n  \"reasoning\": \"视野内未发现目标，左侧阴影疑似目标，建议左转搜索。\",\n  \"action\": [\"Turn left\", \"Search for yellow pipe\", \"Approach pipe\", \"Stop\"]\n}\n\n# Task\n请根据提供的图像和指令，生成对应的推理过程和动作拆解列表。\n",
    "input": "沿着主管道继续向前探索，注意保持匀速并避开下方的绿色支撑结构。",
    "output": {
      "reasoning": "图像显示主管道横跨视野并向前方延伸，下方紧邻一个绿色的支撑结构。为了安全地沿着管道探索，机器人需要首先调整姿态确保与管道平行，保持当前深度以越过下方的支撑物，然后以恒定速度向前推进。",
      "action": [
        "Maintain current depth",
        "Align with the pipeline orientation",
        "Move forward at constant speed",
        "Keep clearance from the green support structure below",
        "Continue following the pipeline"
      ]
    },
    "images": [
      "data/images/0307.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green striped cylindrical object on the seabed while maintaining a safe distance from the coral formations on either side.",
    "output": {
      "reasoning": "The large green striped cylindrical object lies horizontally on the seabed, centered in the image. Coral formations are visible on both left and right sides—left side has a dense cluster near the front-left, right side has branching corals extending toward the object’s midsection. To approach safely, the robot should first align centrally relative to the object while avoiding lateral drift into corals. Given the robot’s current position (above and slightly ahead of the object, as inferred from perspective and bubble trail), the optimal path is: (1) descend gently to seabed level to reduce vertical clearance risk, (2) move forward while maintaining central alignment, (3) adjust slight left/right as needed to keep equal distance from both coral clusters. No immediate turn is required since the object is directly ahead; however, minor course corrections may be needed during forward motion.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward steadily toward the green striped cylinder",
        "Maintain central alignment between left and right coral formations",
        "If drifting left, correct slightly right; if drifting right, correct slightly left"
      ]
    },
    "images": [
      "./data/images/temp_90_0045_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central dark rock with yellow bands and inspect its top surface for embedded sensors or markings.",
    "output": {
      "reasoning": "From the image, the central dark rock with yellow bands is clearly visible in the foreground, slightly left of center. It has a distinct layered structure and yellow banding, matching the description. The robot appears to be positioned at a moderate distance (~1–2 m) and slightly elevated (due to water surface reflection and lighting). To approach it for inspection of its top surface, the robot should first move forward toward the rock while maintaining alignment. Once near, a slight upward tilt or gentle ascent may be needed to position the camera directly above the top surface for optimal inspection. No immediate obstacles block the direct path. Therefore, the plan is: (1) advance straight toward the rock, (2) fine-tune position to center over it, (3) ascend slightly to view the top surface.",
      "action": [
        "Go straight toward the central dark rock with yellow bands",
        "Slow down and center position over the rock",
        "Gently ascend to inspect the top surface"
      ]
    },
    "images": [
      "./data/images/temp_90_0080_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge cluster on the seabed while maintaining a safe distance from the large coral structure above.",
    "output": {
      "reasoning": "From the image, the yellow sponge cluster is visible on the seabed in the lower-right foreground, near white-tipped coral formations. A large, dark coral structure looms above and slightly behind the robot’s current position (center-left to upper-middle of frame), posing a collision risk if the robot ascends or drifts upward. To safely approach the sponge cluster: (1) The robot must first descend slightly to increase vertical clearance from the overhead coral; (2) Then move forward and slightly right toward the sponge cluster, staying low near the seabed; (3) Avoid any upward pitch or ascent during transit. The current heading is roughly aligned with the target but requires lateral correction to the right and controlled descent.",
      "action": [
        "Descend slightly",
        "Turn right 15 degrees",
        "Go straight slowly",
        "Maintain depth while approaching sponge cluster"
      ]
    },
    "images": [
      "./data/images/temp_90_0011_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine positioned at the cave entrance, maintaining a safe distance from surrounding rocks and coral formations.",
    "output": {
      "reasoning": "The submarine is centered at the cave entrance, illuminated by light rays from above. It is flanked by rocks and coral on both sides and slightly ahead. To approach safely: (1) The robot must first align centrally to avoid left/right obstacles; (2) Then advance straight while monitoring proximity to side formations; (3) Stop before entering the narrowest part of the cave to maintain safe distance. The current view shows the submarine directly ahead, so no lateral repositioning is needed initially—only controlled forward motion with awareness of surrounding terrain.",
      "action": [
        "Go straight slowly",
        "Maintain central alignment with the submarine",
        "Stop when 2 meters away from the submarine"
      ]
    },
    "images": [
      "./data/images/temp_90_0056_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-tipped cylindrical pipe while avoiding the overhead net structure and the nearby submersible vehicle.",
    "output": {
      "reasoning": "From the image, the yellow-tipped cylindrical pipe is in the lower-right foreground, extending diagonally upward toward the center. An overhead net structure spans the upper portion of the scene, and a submersible vehicle is positioned to the left of the pipe, near the center. To approach the pipe safely: (1) The robot must avoid ascending into the net—so maintain depth below it; (2) The submersible is on the left, so approach from the right side of the pipe or slightly behind to avoid collision; (3) The pipe’s yellow tip is visible ahead and slightly to the right, indicating the target direction. Therefore, the safest path is to first move right to clear the submersible, then proceed forward while staying low under the net, and finally adjust to align with the pipe’s axis toward the yellow tip.",
      "action": [
        "Turn right slightly",
        "Go straight while maintaining depth below the net",
        "Approach the yellow-tipped pipe from its right side",
        "Align with the pipe and move forward toward the yellow tip"
      ]
    },
    "images": [
      "./data/images/temp_90_0066_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral formation from the front while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The image shows a large, prominent green coral formation centered in the frame, directly ahead of the robot's current position. The coral is well-lit and occupies most of the mid-to-near field, indicating it is relatively close. To approach it from the front while maintaining a safe distance, the robot should proceed straight but at a controlled, cautious speed—avoiding any lateral movement that might risk contact. Given the clear frontal alignment and absence of immediate obstacles in the direct path, a single 'Go straight' action is sufficient for this step; subsequent steps (not required here) would involve stopping at a predetermined safe distance (e.g., 1–2 meters) based on sensor feedback.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_90_0040_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green SUV and inspect its front right wheel area for signs of entanglement or damage.",
    "output": {
      "reasoning": "The green SUV is visible in the center-left of the image, resting on the seabed. Its front right wheel is partially visible from the current viewpoint (robot is slightly ahead and to the left of the vehicle). To inspect the front right wheel area, I need to: (1) move closer to the vehicle while maintaining a safe distance to avoid disturbing sediment; (2) reposition to get a clear view of the front right wheel—likely requiring a slight rightward translation and/or rotation to align with that side; (3) stabilize and hover for inspection. The seabed is uneven with debris, so careful forward motion is needed. No immediate obstacles block the path to the vehicle’s front right side.",
      "action": [
        "Go straight slowly toward the green SUV",
        "Turn slightly right to align with the front right wheel",
        "Move forward gently to get closer to the front right wheel area",
        "Hover steadily for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0006_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green spherical object with circular indentations on the seabed and inspect its surface texture.",
    "output": {
      "reasoning": "The image shows a green spherical object with circular indentations on the seabed, located centrally in the mid-ground. The robot (with glowing green eyes) is positioned to the left and slightly behind it. To approach and inspect its surface texture, the robot must first move forward toward the object, then possibly adjust laterally to align closely for detailed inspection. Given the relatively clear path and no immediate obstacles between the robot and the target, the plan is: (1) advance straight toward the green sphere, (2) fine-tune position to get closer (e.g., slight right turn if needed to center), (3) move very close for surface inspection without contact.",
      "action": [
        "Go straight toward the green spherical object",
        "Slightly turn right to align with the object's center",
        "Move forward slowly to get within close inspection distance",
        "Hover steadily to inspect surface texture"
      ]
    },
    "images": [
      "./data/images/temp_90_0028_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible device suspended among the vertical pipes and inspect its lower attachment point.",
    "output": {
      "reasoning": "The green cylindrical submersible device is visible in the center-right of the image, suspended among vertical pipes. It is positioned slightly above the seafloor and surrounded by rock-like structures. The lower attachment point—likely the orange conical component hanging beneath the cylinder—is partially visible but not yet clearly inspectable from the current viewpoint. To inspect it closely, the robot must first approach the device while avoiding nearby obstacles (e.g., left-side rock formation and foreground boulders). Given the device’s position relative to the robot (appears front-right), the optimal path is: (1) move forward cautiously to reduce distance, (2) adjust rightward to align with the device’s central axis, (3) descend slightly to bring the lower attachment into clear view, and (4) stabilize for inspection. No immediate red obstacles are present; primary constraints are spatial clearance and orientation.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Descend gently",
        "Hover and inspect the lower attachment point"
      ]
    },
    "images": [
      "./data/images/temp_90_0085_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater instrument suspended vertically near the coral reef and inspect its lower sensor housing.",
    "output": {
      "reasoning": "The red cylindrical instrument is clearly visible, suspended vertically in the center of the frame, with its lower sensor housing (black conical section) facing the camera. The robot appears to be already aligned with the instrument’s axis and at a moderate distance (~2–3 m). To inspect the lower sensor housing closely, the robot should first close the distance while maintaining alignment, then position itself slightly below or directly in front for optimal visual inspection. No immediate obstacles block the path; coral reefs are on both sides but not obstructing the central approach. Since the instrument is stationary and vertically oriented, a straight-forward approach followed by a slight downward adjustment (if needed) will suffice.",
      "action": [
        "Go straight toward the red cylindrical instrument",
        "Slow down as you near the lower sensor housing",
        "Position yourself directly in front of the lower sensor housing for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0078_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue corrugated pipe lying on the seabed and inspect its surface for damage or marine growth, while avoiding the net draped over it.",
    "output": {
      "reasoning": "The image shows a large blue corrugated pipe lying horizontally on the seabed, partially covered by a net draped over its upper surface. The robot is currently positioned at a distance and slightly to the left of the pipe (from the viewer’s perspective), with sunlight filtering from above creating shadows and highlights. To inspect the pipe’s surface for damage or marine growth while avoiding the net, the robot should: (1) approach from the side (not from above or through the net), ideally along the exposed lower or lateral surface; (2) maintain safe clearance from the net, which hangs loosely and may shift; (3) move parallel to the pipe to enable systematic visual inspection. Given the pipe extends diagonally from lower-left to upper-right in the frame, the optimal path is to first move forward toward the near end of the pipe, then align laterally to its right side (unobstructed by net), and proceed alongside it.",
      "action": [
        "Go straight toward the near end of the blue pipe",
        "Turn right to align parallel to the pipe’s right side",
        "Move slowly alongside the pipe while maintaining 0.5 m clearance",
        "Inspect the pipe surface visually for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_90_0065_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the left while maintaining a safe distance from the large rock formation on the right.",
    "output": {
      "reasoning": "The yellow sponge is visible on the left side of the image, near the bottom-left corner. A large dark rock formation occupies the right side of the scene, extending vertically. The robot is currently centered in the frame, facing forward. To approach the sponge while avoiding the rock, I need to: (1) gently steer left to align toward the sponge, (2) maintain forward motion at a safe distance from the right-side rock (i.e., avoid drifting right), and (3) possibly adjust depth or fine-tune heading as it gets closer. Since the sponge is relatively close and low, a slight left turn followed by straight advance should suffice without risking collision with the rock on the right.",
      "action": [
        "Turn slightly left",
        "Go straight",
        "Maintain slight leftward heading to stay clear of right rock"
      ]
    },
    "images": [
      "./data/images/temp_90_0077_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, ribbed green structure in the center while avoiding the overgrown column on the left.",
    "output": {
      "reasoning": "The image shows two prominent vertical structures: on the left, an overgrown, irregularly textured column (to be avoided), and in the center, a tall, ribbed green structure with clear vertical ridges (the target). The robot is positioned slightly in front and to the left of the target, with the obstructive column occupying the left visual field. To approach the central structure safely, the robot must first shift rightward to clear the left-side obstacle, then proceed straight toward the target. Given the underwater lighting and visibility, small incremental movements are safer. Therefore, the plan is: (1) turn slightly right to align away from the overgrown column, (2) move forward cautiously toward the central structure, maintaining clearance from the left obstacle.",
      "action": [
        "Turn right slightly",
        "Go straight toward the ribbed green structure"
      ]
    },
    "images": [
      "./data/images/temp_90_0063_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, weathered columnar structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a tall, weathered columnar structure centrally located in the scene, with visible marine growth (e.g., corals and encrustations) on its surface and surrounding seabed. The robot appears to be positioned at a moderate distance (~2–3 meters) in front of the structure, slightly below it (given upward-looking perspective). To approach and inspect its surface, the robot should first move forward toward the base, then circle or adjust laterally to examine different facets—especially since the current view shows only the front-right side. Given the structure’s height and potential shadowing, a slight upward pitch may help inspect upper sections after closing range. No immediate obstacles block the direct path forward.",
      "action": [
        "Go straight toward the columnar structure",
        "Slow down as you near the base (within 1 meter)",
        "Circle clockwise around the structure at close range",
        "Tilt camera upward slightly to inspect upper sections",
        "Pause briefly at each quadrant to observe surface details"
      ]
    },
    "images": [
      "./data/images/temp_90_0030_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green-painted pipeline straight ahead while maintaining a safe distance from the adjacent yellow-banded cables on the left.",
    "output": {
      "reasoning": "The image shows a green-painted pipeline running straight ahead along the seabed, serving as the primary navigation reference. To the left of the green pipeline, there is a bundle of yellow-banded cables—these are obstacles that must be avoided by maintaining a safe lateral distance. The robot is currently aligned with the green pipeline and positioned slightly to its right, which is appropriate given the hazard on the left. To follow the instruction precisely, the robot should continue moving forward while subtly adjusting rightward (if needed) to ensure it stays centered relative to the green pipeline and avoids drifting toward the yellow cables. No turns are required; only steady forward motion with minor course correction to preserve safe clearance.",
      "action": [
        "Go straight",
        "Slightly adjust right to maintain safe distance from yellow-banded cables"
      ]
    },
    "images": [
      "./data/images/temp_90_0090_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe structure on the left and inspect the circular end cap marked with the number '1'.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe structure on the left side of the frame, with multiple circular end caps visible. The cap marked with the number '1' is clearly in view and positioned closest to the robot, slightly below and to the left. To approach and inspect it, the robot should first move forward toward the structure while maintaining alignment with the target cap. Since the cap is already within visual range and relatively close, no major lateral correction is needed—just a slight leftward adjustment may help center the cap in the field of view before closing in. After approaching, a final small forward motion will bring the robot near enough for detailed inspection.",
      "action": [
        "Move forward slightly",
        "Turn left slightly to align with the cap marked '1'",
        "Move forward toward the circular end cap marked '1'",
        "Stop and hover for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0070_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green mechanical structure on the seabed and inspect its central cylindrical component for signs of biofouling or damage.",
    "output": {
      "reasoning": "The green mechanical structure is clearly visible in the center-right of the image, resting on the seabed. It features a prominent central cylindrical component with radial supports and circular ports—this is the target for inspection. The robot (submersible in upper left) is currently at a distance and slightly elevated; to approach and inspect the central cylinder, it must first descend toward the seabed, then move forward (straight) toward the structure, and finally position itself close and centered relative to the cylinder for detailed visual inspection. No immediate obstacles block the direct path, though some coral/rock formations are nearby—maintaining a safe lateral margin is advisable. Given the lighting from above, approaching from the front-left will optimize illumination on the cylinder’s surface.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward straight toward the green mechanical structure",
        "Adjust left slightly to center on the central cylindrical component",
        "Hover steadily 1–2 meters in front of the cylinder",
        "Perform slow clockwise rotation to inspect full circumference"
      ]
    },
    "images": [
      "./data/images/temp_90_0003_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular metallic structure on the left while avoiding the coral clusters on the seabed.",
    "output": {
      "reasoning": "The image shows a large circular metallic structure (likely a pipe or tunnel section) on the left side of the frame, partially embedded in the seabed. Coral clusters are visible on the seabed in the foreground and midground, especially near the center and right. To approach the structure while avoiding corals, the robot should first move slightly right to clear the nearest coral cluster in front, then proceed forward along the left side where the seabed appears clearer, maintaining safe distance from corals. The structure is at moderate distance and angled away, so a gentle leftward trajectory after initial clearance will align with it.",
      "action": [
        "Move slightly right to avoid the nearest coral cluster",
        "Go straight while keeping the metallic structure on the left",
        "Gently turn left to align with the entrance of the circular structure",
        "Approach the structure steadily at low speed"
      ]
    },
    "images": [
      "./data/images/temp_90_0048_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large dark rock formation on the right while maintaining a safe distance from the textured coral-covered boulders in the foreground.",
    "output": {
      "reasoning": "The image shows a large dark rock formation on the right side of the frame, extending vertically and occupying much of the right half. In the foreground (lower-left to center), there are textured, coral-covered boulders that appear close to the robot’s current position. To safely approach the large dark rock on the right while avoiding the foreground coral boulders, the robot should first move slightly left to clear the immediate coral obstacles, then proceed forward while gently turning right to align toward the target rock, maintaining lateral clearance. Given the depth and lighting, a cautious forward motion with minor rightward correction is appropriate—avoid sharp turns near obstacles.",
      "action": [
        "Move slightly left",
        "Go forward",
        "Turn slightly right",
        "Continue forward while maintaining distance from foreground coral"
      ]
    },
    "images": [
      "./data/images/temp_90_0021_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the underwater cave opening toward the sunlit area while avoiding the large rocks on the seabed.",
    "output": {
      "reasoning": "The image shows an underwater cave with a clear opening ahead, where sunlit rays penetrate from above—indicating the target destination. Large rocks are scattered on the seabed, particularly on the left and right sides near the entrance, but a central corridor appears navigable. The robot is currently positioned inside the cave, facing the opening. To safely reach the sunlit area: (1) First, maintain forward motion while staying centered to avoid the rocks flanking the path; (2) As the opening widens, slightly adjust right to bypass a prominent rock on the left mid-path; (3) Continue straight toward the brightest sunlit zone, ensuring clearance from the large rock cluster on the right seabed. No sharp turns are needed—smooth forward progression with minor lateral corrections suffices.",
      "action": [
        "Go straight while staying centered",
        "Slightly turn right to avoid the left-side rock",
        "Continue straight toward the sunlit area"
      ]
    },
    "images": [
      "./data/images/temp_90_0008_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle on the seabed and inspect its front illuminated viewport.",
    "output": {
      "reasoning": "The submarine-like vehicle is positioned on the seabed to the right of the current view, with its front illuminated viewport clearly visible and facing slightly toward the camera. There are rocks in the foreground (left and bottom) that must be navigated around. To approach the front viewport safely, I should first move forward while slightly adjusting right to clear the left-side rock, then continue straight toward the vehicle until close enough for inspection. The lighting from the viewport helps localize the target. No immediate obstacles block the direct path once the initial rocks are bypassed.",
      "action": [
        "Move forward slightly while turning right to avoid the left foreground rock",
        "Proceed straight toward the submarine-like vehicle",
        "Stop when aligned with and near the front illuminated viewport for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0083_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car between the two large columns and inspect its front left wheel area for marine growth or damage.",
    "output": {
      "reasoning": "The submerged car is positioned between two large columns — one on the left (a multi-column structure) and one on the right (a single thick column). From the current viewpoint, the car is slightly to the right of center and partially obscured by the right column. To approach it safely and position for inspection of the front left wheel, I need to: (1) move forward while slightly adjusting left to clear the right column and align with the car’s midline; (2) continue forward until close enough to inspect; (3) then maneuver to the car’s front-left side, ensuring stable positioning for visual inspection of the wheel area (which is likely covered in marine growth or corrosion given the environment).",
      "action": [
        "Move forward slightly while turning left to align with the car between the columns",
        "Continue forward until within 2 meters of the car",
        "Turn left and approach the front-left corner of the car",
        "Hover steadily and inspect the front left wheel area for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0054_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the right while avoiding the tall dark coral structures on the left.",
    "output": {
      "reasoning": "From the image, the robot is currently oriented forward with tall dark coral structures visible on the left side of the frame, posing a collision risk. A yellow sponge (target) is located on the right side, partially illuminated and near the bottom-right region. To approach it safely, the robot must first steer right to clear the left-side corals, then proceed forward while maintaining a safe lateral offset from the left obstacles. The lighting suggests the sponge is relatively close, so a moderate turn followed by straight movement should suffice.",
      "action": [
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_90_0050_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object with green circular feature on the seabed and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The red dome-shaped object with a green circular feature is clearly visible on the seabed in the foreground, slightly to the right of center. The robot appears to be positioned above and slightly behind it (based on perspective and shadow). To approach and inspect its surface, I need to: (1) descend gently to get closer to the seabed level; (2) move forward toward the object; (3) position the robot directly in front or slightly above the object for optimal visual inspection; (4) possibly circle slightly to view multiple angles if needed, but initial priority is stable close approach. No immediate obstacles block the direct path. Lighting is sufficient for visual inspection.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward toward the red dome-shaped object",
        "Stop at ~0.5m distance from the object",
        "Adjust orientation to face the object directly",
        "Perform slow clockwise rotation to inspect surface"
      ]
    },
    "images": [
      "./data/images/temp_90_0060_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient stone column and inspect its surface for inscriptions or structural damage.",
    "output": {
      "reasoning": "The image shows a tall, ancient stone column centered in the frame, standing upright on the seabed. It is surrounded by smaller rock formations and marine growth (e.g., green algae or coral), but there is a clear open path from the robot’s current position toward the column’s base. The column appears to be ~5–7 meters away based on perspective and scale relative to surrounding features. To inspect its surface for inscriptions or damage, I must first approach it closely—ideally within 1–2 meters—while maintaining stable orientation to enable high-resolution visual inspection. Since the column is directly ahead, the optimal initial action is to move straight forward. Once near, a slow circumferential movement will allow full surface coverage.",
      "action": [
        "Go straight toward the stone column",
        "Slow down as you near the base (within 2 meters)",
        "Orbit the column clockwise at close range to inspect all sides",
        "Pause briefly at each quadrant to capture detailed imagery"
      ]
    },
    "images": [
      "./data/images/temp_90_0088_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and white striped buoy anchored on the seabed, maintaining a safe distance from the surrounding coral and rocks.",
    "output": {
      "reasoning": "The red and white striped buoy is centrally located in the image, anchored on the seabed. Coral formations are visible to the left and right of the buoy, and a large rock structure is on the right side. To approach safely, the robot should first align directly toward the buoy while maintaining central positioning to avoid the nearby coral (left) and rock (right). Given the clear frontal view and moderate distance, the safest initial action is to move straight forward cautiously. As it gets closer, fine adjustments can be made—but for now, 'Go straight' is the optimal first step to initiate approach while staying in the clearest corridor.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_90_0071_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the left green columnar structure and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows two prominent green columnar structures; the left one is closer and more clearly visible, with circular openings and surface texture suggesting possible biofouling or damage. The robot is currently positioned slightly in front and to the right of the left column, with clear water path toward it. To approach and inspect its surface, I need to: (1) move leftward to align with the column’s midline, (2) advance straight toward it to reach inspection distance (~0.5–1 m), and (3) possibly circle slightly or hover to enable close visual inspection of surface features. Given the current view, a gentle left turn followed by forward motion is safest and most direct.",
      "action": [
        "Turn left slightly",
        "Go straight toward the left green column",
        "Hover near the column surface for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0064_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car between the two stone pillars and inspect its front right side for damage or debris.",
    "output": {
      "reasoning": "The green car is positioned centrally between two stone pillars, slightly angled with its front right side facing the viewer. From the current perspective, the robot appears to be at a moderate distance in front of the car, with clear visibility of the target area. To inspect the front right side, the robot must first approach the car directly, then maneuver to position itself adjacent to and slightly ahead of the front right wheel/fender region. The path appears unobstructed; no immediate obstacles block forward movement. Step-by-step: (1) Move straight toward the car to reduce distance; (2) Once near, turn slightly right to align with the front right side; (3) Advance gently to get a close-up view for inspection.",
      "action": [
        "Go straight toward the green car",
        "Turn slightly right",
        "Move forward to inspect the front right side"
      ]
    },
    "images": [
      "./data/images/temp_90_0073_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object on the seabed while avoiding the large rocks on either side.",
    "output": {
      "reasoning": "The red dome-shaped object is clearly visible on the seabed, centered in the lower part of the image. Two large rocks flank it: one on the left (larger, textured) and one on the right (also large, partially shadowed). The robot appears to be positioned slightly above and behind the target, with a clear forward path toward the red dome if it stays centered between the two rocks. To avoid collision, the robot should maintain a central trajectory—neither veering left nor right—and descend gently while moving forward. The water column above is relatively clear, so no vertical obstacles interfere. Step-by-step: first align forward toward the dome, then proceed straight while maintaining safe lateral clearance from both rocks, possibly with minor course corrections if needed (but none are required in current frame).",
      "action": [
        "Go straight toward the red dome-shaped object",
        "Maintain center alignment between the left and right large rocks",
        "Descend gently to reach seabed level near the red dome"
      ]
    },
    "images": [
      "./data/images/temp_90_0042_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spherical coral structure from the front-left to inspect its surface texture and openings.",
    "output": {
      "reasoning": "The large spherical coral is centered in the image, slightly right of frame center, resting on the seabed. The robot appears to be positioned front-left relative to the coral (based on perspective and lighting), with clear open space in that approach quadrant. To inspect its surface texture and openings from the front-left, I need to: (1) move forward toward the coral while maintaining a slight left bias to stay on the intended approach vector; (2) close distance carefully to enable detailed visual inspection without disturbing sediment; (3) stabilize at optimal inspection range (~1–2 m) facing the front-left quadrant of the coral. No immediate obstacles block this path.",
      "action": [
        "Move forward-left slightly",
        "Advance steadily toward the coral",
        "Stop at ~1.5 meters distance, aligned with front-left side",
        "Stabilize position for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0031_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car parked beside the moss-covered concrete pillar and inspect its driver-side door for damage.",
    "output": {
      "reasoning": "The green car is visible to the right of the moss-covered concrete pillar, parked parallel to it. The driver-side door (left side of the car from our perspective, since the car faces roughly toward the camera) is partially visible but not yet close enough for detailed inspection. To approach and inspect it: (1) First, move forward toward the car while staying clear of the pillar and the large spherical object on the left; (2) Then, position the robot alongside the driver’s side—this requires a slight leftward lateral movement once near the front of the car; (3) Finally, halt and orient the camera toward the driver-side door for close inspection. No immediate obstacles block the path, but care must be taken to avoid disturbing sediment or drifting into the pillar.",
      "action": [
        "Go straight toward the green car",
        "Move slightly left to align with the driver-side door",
        "Stop and face the driver-side door for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0082_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-lit underwater vehicle positioned centrally among the coral formations.",
    "output": {
      "reasoning": "The yellow-lit underwater vehicle is clearly visible in the center of the image, surrounded by coral formations on both sides and slightly ahead. Sunlight rays indicate depth and orientation; the vehicle faces the robot directly and appears at moderate distance (~2–3 m). No immediate obstacles block the direct path. To approach it safely and precisely, I should first align straight toward it, then close the distance gradually while maintaining central alignment to avoid colliding with nearby corals.",
      "action": [
        "Go straight toward the yellow-lit vehicle",
        "Maintain central alignment with the vehicle",
        "Slowly advance until near the vehicle"
      ]
    },
    "images": [
      "./data/images/temp_90_0046_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged pickup truck and inspect its front-left wheel area for damage or entanglement.",
    "output": {
      "reasoning": "The submerged pickup truck is clearly visible in the center of the image, oriented diagonally with its front facing slightly leftward. The front-left wheel is on the near side (left from our perspective) and partially obscured by sediment and nearby yellow cylindrical objects (possibly pipes or buoys). To inspect the front-left wheel area, I need to: (1) approach the vehicle from the front-left quadrant to get a clear view; (2) maintain safe distance initially to avoid disturbing sediment; (3) maneuver closer once aligned; (4) position the camera for close-up inspection of the wheel well and tire area. The lighting from above creates glare but also illuminates the target area adequately. No immediate obstacles block the direct path to the front-left side.",
      "action": [
        "Move forward slowly toward the front-left side of the truck",
        "Turn slightly left to align with the front-left wheel",
        "Descend gently to lower operating depth near the wheel level",
        "Advance cautiously to within 1 meter of the front-left wheel",
        "Hover and adjust orientation to face the wheel well directly",
        "Perform close visual inspection of the front-left wheel and surrounding area"
      ]
    },
    "images": [
      "./data/images/temp_90_0058_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted green car from the front-left side and inspect the open driver’s door while avoiding the scattered circular debris on the seabed.",
    "output": {
      "reasoning": "The rusted green car is positioned diagonally on the seabed, with its open driver’s door facing toward the left side of the frame. From the current robot perspective (likely slightly ahead and to the right of the car), the front-left approach requires moving forward while angling left to reach the driver’s door without crossing the scattered circular debris—these are concentrated in the foreground and near the car’s front-right wheel area. To avoid them, I should first move forward cautiously, then veer left to align with the front-left quadrant of the car, and finally advance closer to inspect the open driver’s door. The debris appears mostly in the lower half of the image; keeping a slight upward trajectory (i.e., maintaining depth) may help clear them, but lateral avoidance is primary.",
      "action": [
        "Go straight slowly",
        "Turn left slightly",
        "Go straight toward the front-left corner of the car",
        "Approach the open driver's door carefully",
        "Hover and inspect the open driver's door"
      ]
    },
    "images": [
      "./data/images/temp_90_0020_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-eyed submersible robot hovering near the center of the frame, maintaining a safe distance from the surrounding coral formations.",
    "output": {
      "reasoning": "The yellow-eyed submersible robot is centrally located in the frame, slightly elevated and facing the camera. Coral formations are present on the left, right, and bottom foreground, forming a semi-enclosed corridor. To approach safely, I must avoid lateral drift into corals. The robot appears stationary, so a controlled forward advance with slight central alignment is appropriate. First, I will gently move forward while keeping the target centered; then fine-tune heading to maintain clearance from nearby corals (especially the large one on the lower right and the cluster on the left). No sharp turns are needed—only minor corrections if drifting.",
      "action": [
        "Go straight slowly",
        "Slightly adjust left to center the target",
        "Maintain steady forward motion while monitoring coral proximity"
      ]
    },
    "images": [
      "./data/images/temp_90_0055_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical coral structure in the center foreground and inspect its surface for signs of marine growth or damage.",
    "output": {
      "reasoning": "The image shows a green cylindrical coral structure in the center foreground, slightly to the left of the robot's current heading. Sunlight streams from above, illuminating the scene and highlighting textures on the corals. The target coral is clearly visible and accessible; no immediate obstacles block the path. To inspect its surface closely, I need to: (1) move forward toward it while maintaining alignment, (2) possibly adjust laterally to center it in the field of view, and (3) get close enough for detailed visual inspection (within ~0.5–1 m). Given its position directly ahead and slightly left, a slight left correction followed by forward motion is optimal to approach head-on.",
      "action": [
        "Turn slightly left",
        "Go straight toward the green cylindrical coral",
        "Stop when ~0.5 meters away",
        "Hover and inspect the surface for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0004_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red ribbed lantern on the seabed while avoiding the nearby dark rock formations.",
    "output": {
      "reasoning": "The red ribbed lantern is clearly visible in the center-left of the image, resting on the seabed. A dark rock formation is directly beneath and slightly to the right of the lantern, casting a shadow. Another dark rock (possibly part of a larger formation) is visible to the far left, partially obscuring the view. To approach the lantern safely while avoiding rocks: (1) First, move slightly right to clear the left-side rock and align with the lantern’s open side; (2) Then proceed straight toward the lantern, maintaining a safe lateral distance from the dark rock directly under it; (3) Finally, close in gently to reach the lantern without contacting surrounding obstacles.",
      "action": [
        "Turn slightly right",
        "Go straight toward the red lantern",
        "Approach the lantern carefully from its left side"
      ]
    },
    "images": [
      "./data/images/temp_90_0023_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure on the left and inspect its front opening for debris or damage.",
    "output": {
      "reasoning": "The large green cylindrical structure is clearly visible on the left side of the image, with its front opening facing the camera. The robot appears to be at a moderate distance and slightly angled toward it. To approach and inspect the front opening, the robot should first align itself directly in front of the structure, then move forward carefully to get a closer view of the opening for debris or damage assessment. Since the opening is already in view, minimal lateral adjustment is needed—primarily forward motion with possible slight left correction to center the opening.",
      "action": [
        "Turn slightly left to center the green cylindrical structure's front opening in the field of view",
        "Go straight slowly toward the front opening",
        "Stop when within 1 meter of the opening for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0033_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar while avoiding the floating debris and illuminated green mesh structures above.",
    "output": {
      "reasoning": "The central stone pillar is directly ahead and well-lit, serving as the primary target. Floating debris appears as small, scattered bright spots in the mid- to far-field (e.g., lower right and center), and the illuminated green mesh structures form an overhead canopy—suggesting the robot must maintain a safe vertical clearance while advancing. Since the pillar is centered and the path appears unobstructed at ground level, the safest approach is to proceed straight while slightly adjusting upward or maintaining neutral pitch to avoid the overhead meshes, and subtly correcting laterally if debris drifts into the path. Given no immediate close-range obstacles directly in front, the first action is to go straight toward the pillar, monitoring for debris encroachment.",
      "action": [
        "Go straight toward the central stone pillar",
        "Maintain slight upward pitch to avoid overhead green mesh",
        "Monitor for floating debris and make minor lateral adjustments if needed"
      ]
    },
    "images": [
      "./data/images/temp_90_0044_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical structure secured by yellow ropes and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The image shows a large submerged cylindrical structure (likely a pipeline or buoy) secured by visible yellow ropes. It is positioned diagonally across the frame, with its nearest end on the right side and extending toward the upper left. The robot appears to be currently located to the left and slightly in front of the structure, with netting in the lower-left foreground possibly indicating proximity to a support platform or mooring area. To approach the cylinder for inspection, the robot should first move forward to reduce distance, then adjust laterally (rightward) to align parallel to the cylinder’s axis, ensuring close but safe proximity. Once aligned, it can proceed along the length of the cylinder while maintaining a consistent standoff distance for visual inspection. No immediate obstacles block the direct path, though care must be taken near the yellow ropes to avoid entanglement.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Turn right to align parallel to the cylinder",
        "Move slowly alongside the cylinder while maintaining ~1m distance",
        "Inspect surface continuously for cracks, corrosion, or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_90_0036_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater passage between the left rocky wall and the right sponge-covered rock formation, heading toward the sunlit opening ahead.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage flanked by a dark rocky wall on the left and a sponge-covered rock formation on the right. Sunlight streams from an opening ahead, indicating the goal direction. To safely navigate this constrained corridor, the robot must maintain central alignment—avoiding contact with either side—while progressing forward toward the light. Given the perspective, the passage appears to widen slightly ahead, so steady forward motion with minor lateral corrections is appropriate. No immediate obstacles block the path, but precise heading control is essential due to the narrow clearance.",
      "action": [
        "Go straight while maintaining center alignment between left rock wall and right sponge-covered formation",
        "Slightly adjust left if drifting toward right sponge formation",
        "Slightly adjust right if drifting toward left rocky wall",
        "Continue forward toward the sunlit opening"
      ]
    },
    "images": [
      "./data/images/temp_90_0014_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge-like organism on the left while maintaining a safe distance from the red robotic structure in the foreground.",
    "output": {
      "reasoning": "The image shows a red robotic structure in the foreground (right-lower quadrant), partially obstructing the view. A yellow sponge-like organism is visible on the left side, illuminated by sun rays, and appears to be at mid-depth. To approach it safely, I must first avoid the red structure—likely requiring a leftward movement to clear it. Since the target is already on the left, a gentle left turn followed by forward motion should align the robot toward the sponge while keeping lateral clearance from the red structure. Given the lighting and depth cues, the sponge is not extremely far, so a short forward advance after repositioning should suffice. No immediate obstacles lie directly between the current position and the sponge once the red structure is bypassed on its left side.",
      "action": [
        "Turn left slightly",
        "Move forward slowly",
        "Maintain slight leftward heading to keep distance from red structure"
      ]
    },
    "images": [
      "./data/images/temp_90_0072_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the illuminated opening in the cave while avoiding the rocky outcrops on the left and right.",
    "output": {
      "reasoning": "The image shows an underwater cave with a bright illuminated opening ahead, serving as the target destination. Rocky outcrops are present on both left and right sides—especially prominent on the right (large textured rock mass) and left (irregular protrusions near the cave wall). The path to the opening appears relatively clear in the center but requires careful lateral positioning to avoid collisions. To safely navigate, the robot should first center itself within the passage, then proceed forward while maintaining clearance from both sides. Given the perspective, the opening is directly ahead but slightly elevated; thus, a slight upward tilt may help align with the exit plane, though primary motion is forward. No immediate turn is needed since the opening lies straight ahead, but minor corrections may be required mid-transit. For atomic steps: (1) ensure centered alignment, (2) advance steadily toward the light, (3) monitor proximity to obstacles and adjust if needed—but since we must output fixed steps without real-time feedback, we assume initial centering suffices for safe transit.",
      "action": [
        "Center yourself in the cave passage",
        "Go straight toward the illuminated opening",
        "Maintain steady forward motion while keeping equal distance from left and right rocky outcrops"
      ]
    },
    "images": [
      "./data/images/temp_90_0022_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its upper ornamental structure for signs of marine growth or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned slightly to the right of center and at a moderate distance from the robot's current location. The upper ornamental structure (capitals with carved details) is illuminated by overhead light, making it inspectable. To inspect it closely for marine growth or damage, I need to: (1) move forward toward the column while maintaining alignment, (2) adjust lateral position if needed to center the column in view, and (3) ascend slightly to bring the upper ornamental section into optimal inspection range (since the camera appears level or slightly low relative to the capital). No immediate obstacles block the direct path, though debris on the seabed suggests cautious forward motion.",
      "action": [
        "Go straight toward the central stone column",
        "Slightly adjust left to center the column in view",
        "Ascend gently to align camera with the upper ornamental structure",
        "Hover and inspect the capital for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0005_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green ribbed cylindrical object with the red fin-like protrusion and inspect its surface for附着物 or damage.",
    "output": {
      "reasoning": "The green ribbed cylindrical object with a red fin-like protrusion is visible on the right side of the image, partially adjacent to a large spotted rock. It is at moderate distance and slightly angled away from the robot’s current heading. To approach it for inspection, I need to first reorient toward it (turn right), then close the distance (go straight), and finally position closely and stably for surface inspection (adjust forward/backward and possibly slight yaw). No immediate obstacles block the path, though small fish are present but non-interfering. The goal is to get within ~0.5–1 m for clear visual inspection of附着物 (biofouling) or damage.",
      "action": [
        "Turn right",
        "Go straight",
        "Move forward slowly",
        "Hover and stabilize"
      ]
    },
    "images": [
      "./data/images/temp_90_0086_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green striped buoy with the orange cone and inspect its surface for damage or attached sensors.",
    "output": {
      "reasoning": "The green striped buoy with an orange cone is clearly visible in the center-left of the image, resting on the seabed and partially supported by a rusted circular structure. The robot (camera platform) is positioned above and slightly behind it. To approach and inspect its surface, I need to: (1) move forward and slightly downward to reach the buoy’s level; (2) close in laterally to position the camera near the buoy’s side; (3) circle or adjust orientation to inspect the full surface, especially around the orange cone and possible sensor mounts (e.g., holes or protrusions on the buoy). No immediate obstacles block the direct path, though the rusted structure lies beneath—so maintain safe clearance (~0.5 m) to avoid contact.",
      "action": [
        "Move forward and descend slightly toward the green striped buoy",
        "Approach laterally to position within 1 meter of the buoy’s side",
        "Circle slowly around the buoy while maintaining close proximity",
        "Inspect surface details near the orange cone and any visible ports or attachments"
      ]
    },
    "images": [
      "./data/images/temp_90_0062_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front grille for marine growth or damage.",
    "output": {
      "reasoning": "The submerged car is clearly visible in the center-right of the image, resting on the seabed. Its front grille faces slightly toward the camera but angled leftward. The robot appears to be positioned at a moderate distance (~3–4 m) and slightly to the left-front of the car. To inspect the front grille closely, the robot must: (1) move forward to reduce distance, (2) adjust laterally (slightly right) to align directly with the grille’s centerline, and (3) approach until close enough for detailed visual inspection (within ~1 m). No immediate obstacles block the path; minor marine growth is already visible on the grille, but closer inspection is required per instruction.",
      "action": [
        "Go straight toward the car",
        "Slightly turn right to center on the front grille",
        "Continue forward slowly until within 1 meter of the grille",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0052_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge located on the seabed near the center-left of the frame, avoiding the large overhanging rock formation above and the submarine to the left.",
    "output": {
      "reasoning": "The yellow sponge is not directly visible in the current frame, but based on the instruction, it should be near the center-left seabed. The large overhanging rock formation dominates the upper-right portion of the image and casts a shadow; we must stay below it but avoid getting too close to prevent collision. A submarine is positioned on the left side, partially behind a rock—so we must steer rightward initially to avoid it. The seabed appears relatively clear in the center-left region, suggesting a safe path forward after clearing the submarine. Therefore, the plan is: (1) move slightly right to clear the submarine’s position, (2) proceed straight while maintaining depth to stay under the overhang, (3) adjust leftward gently as we approach the target zone to center on the sponge.",
      "action": [
        "Turn slightly right",
        "Go straight at constant depth",
        "Turn slightly left",
        "Approach the seabed gently"
      ]
    },
    "images": [
      "./data/images/temp_90_0007_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated yellow net structure while avoiding the suspended cables and the fish swimming nearby.",
    "output": {
      "reasoning": "The illuminated yellow net structure is the primary target, located centrally and slightly below the current viewpoint. Suspended cables are visible in the upper-left quadrant, extending diagonally downward—these must be avoided by steering right or maintaining clearance. A fish is swimming near the left side of the net, but it appears at a safe distance and moving away; no immediate evasive action needed. The robot is currently positioned somewhat above and slightly left of the net’s entrance. To approach safely: (1) first adjust rightward to clear the cables, (2) then descend gently while maintaining central alignment with the net opening, (3) proceed forward into the net structure without sudden movements to avoid disturbing the fish or entanglement.",
      "action": [
        "Turn right slightly",
        "Descend slowly",
        "Go straight toward the net entrance",
        "Enter the illuminated yellow net structure"
      ]
    },
    "images": [
      "./data/images/temp_90_0015_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column while avoiding the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, illuminated by light rays from above and positioned slightly to the right of center. Scattered debris (small rocks and fragments) lies on the seabed, particularly in the foreground and left side. To approach the column safely, I must navigate forward while steering clear of the debris field. The clearest path appears to be slightly right of center—avoiding the left-side debris—and maintaining a straight but cautious trajectory toward the column’s base. Since the column is directly ahead but offset right, a slight rightward correction early on will align the robot with the safest approach corridor.",
      "action": [
        "Turn slightly right",
        "Go straight slowly",
        "Adjust left slightly to avoid nearby debris",
        "Continue straight toward the base of the central stone column"
      ]
    },
    "images": [
      "./data/images/temp_90_0043_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater canyon, staying centered between the two rocky walls while avoiding the suspended debris and maintaining depth above the seabed coral formations.",
    "output": {
      "reasoning": "The robot is currently positioned in a narrow underwater canyon with rocky walls on both left and right. Sunlight penetrates from above, illuminating the central path. A suspended cylindrical debris object is visible directly ahead, slightly below the robot’s current depth. The seabed is covered with coral formations, requiring the robot to maintain sufficient altitude to avoid collision. To stay centered, the robot must balance lateral position relative to both walls; the left wall appears closer than the right in the current frame, suggesting a slight rightward drift correction may be needed. The debris lies along the central corridor, so a slight upward adjustment followed by forward motion will help clear it safely. After passing the debris, continued centering and depth maintenance are required.",
      "action": [
        "Slightly ascend to clear suspended debris",
        "Move forward while gently correcting right to stay centered",
        "Maintain steady depth above coral seabed",
        "Continue forward, monitoring wall proximity for centering"
      ]
    },
    "images": [
      "./data/images/temp_90_0047_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical perforated pipe structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The vertical perforated pipe structure is directly in front of the robot, centered in the field of view, and appears within close to moderate range. The lighting highlights its surface, suggesting good visibility for inspection. To inspect its surface thoroughly, the robot should first approach it closely (but safely) to resolve fine details like marine growth or damage. Since the pipe is upright and dominates the scene, a straight-forward approach is optimal. After reaching an appropriate inspection distance (~1–2 m), a slow circumferential movement (e.g., gentle right turn while maintaining distance) will allow full surface coverage. However, the initial step is to close the distance safely.",
      "action": [
        "Go straight toward the pipe structure",
        "Slow down as you near the pipe (within ~1.5 m)",
        "Begin a slow rightward arc to inspect the pipe surface"
      ]
    },
    "images": [
      "./data/images/temp_90_0087_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall textured pillar on the right while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The image shows an underwater scene with a prominent tall textured pillar on the right side, matching the target described. Surrounding it are irregular rock formations—especially to the left and near the foreground—that must be avoided. The robot appears to be positioned slightly left of center, facing forward, with the pillar visible at a moderate distance (~3–5 m) to the right. To approach safely: (1) First, align the robot toward the pillar by turning right slightly (not too sharply, to avoid drifting into nearby rocks). (2) Then proceed forward cautiously while monitoring proximity to adjacent rocks—especially the large formation on the immediate left and the rounded rocks in the lower right. (3) Maintain a lateral offset (e.g., ~1 m) from the pillar’s base to avoid collision with its irregular surface or nearby debris. Given the lighting and visibility, small incremental movements are safest.",
      "action": [
        "Turn right slightly",
        "Go straight slowly",
        "Maintain slight rightward offset while advancing",
        "Pause and reassess distance to pillar and nearby rocks"
      ]
    },
    "images": [
      "./data/images/temp_90_0026_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green and purple striped cylindrical object on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large green and purple striped cylindrical object lies horizontally on the seabed in the foreground, slightly to the left of center. The robot is currently positioned above and behind it (mid-water), with clear visibility and no immediate obstacles blocking approach. To inspect its surface, I need to: (1) descend toward the seabed to reach the object’s level, (2) move forward slowly to close distance, (3) position the robot parallel and close to the cylinder for detailed visual inspection—ideally along its length to check for damage or marine growth. Given the object’s orientation (left-to-right across frame), approaching from the front-left and then moving rightward along its side is optimal. No red obstacles are present; minor rocks nearby do not impede path.",
      "action": [
        "Descend vertically toward the seabed",
        "Move forward slowly toward the cylindrical object",
        "Position parallel to the cylinder, ~0.5 m away",
        "Translate rightward along the cylinder’s length while maintaining proximity",
        "Pause briefly at mid-section to inspect surface details"
      ]
    },
    "images": [
      "./data/images/temp_90_0027_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue pickup truck and inspect its front right wheel for damage or entanglement.",
    "output": {
      "reasoning": "The blue pickup truck is visible in the center-right of the image, resting on the seabed between two large columns. Its front right wheel is partially visible and appears accessible from the robot’s current position (assumed to be slightly left and forward of the truck, based on perspective). To inspect the front right wheel, I need to: (1) move closer to the truck while avoiding the left column and seabed obstacles; (2) position the robot to the front-right side of the truck for optimal view of that wheel; (3) stabilize and focus on the wheel for inspection. Given the lighting and clarity, a direct approach with slight rightward correction should suffice.",
      "action": [
        "Go straight toward the blue pickup truck",
        "Turn slightly right to align with the front right side of the truck",
        "Move forward slowly to get within 1 meter of the front right wheel",
        "Hover and stabilize for visual inspection of the front right wheel"
      ]
    },
    "images": [
      "./data/images/temp_90_0053_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange cylindrical pipe lying on the seabed and inspect its open end for blockages or damage.",
    "output": {
      "reasoning": "The large orange cylindrical pipe is clearly visible on the seabed, oriented diagonally with its open end facing roughly toward the camera’s right side. The robot is currently positioned at a moderate distance (~3–4 m) and slightly to the left of the pipe’s axis. To inspect the open end, the robot must first approach the pipe while maintaining alignment with the open end (to avoid occlusion or glancing views). Given the pipe’s orientation, a slight rightward correction followed by forward motion will bring the robot into optimal inspection position. Once near (~1 m), a final fine adjustment may be needed to center the open end in the field of view for detailed visual inspection.",
      "action": [
        "Turn right slightly",
        "Go straight toward the pipe",
        "Adjust position to center the open end in view",
        "Hover and inspect the open end for blockages or damage"
      ]
    },
    "images": [
      "./data/images/temp_90_0010_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green mesh pipeline and inspect the red-and-black flexible hose attached to it.",
    "output": {
      "reasoning": "The image shows a green mesh pipeline lying on the seabed, with a red-and-black flexible hose visibly attached to it on the right side. The robot is currently positioned slightly left and above the pipeline, facing roughly toward it. To approach the pipeline and inspect the hose, the robot should first move forward (straight) to reduce distance, then adjust laterally (right) to align with the hose attachment point. Given the hose is coiled and prominent on the right side of the pipeline, a slight right turn after approaching will position the robot for close inspection. No immediate obstacles block the path.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Move forward toward the red-and-black hose",
        "Hover and inspect the hose connection"
      ]
    },
    "images": [
      "./data/images/temp_90_0076_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted orange submersible vehicle and inspect its front green-rimmed viewport for damage or obstructions.",
    "output": {
      "reasoning": "The rusted orange submersible vehicle is directly in front of the robot, centered in the frame, with its green-rimmed viewport clearly visible and facing the camera. The distance appears moderate—close enough to approach without major navigation obstacles, but not yet at inspection range. To inspect the viewport for damage or obstructions, I need to: (1) move forward to reduce distance, (2) stabilize position directly in front of the viewport, and (3) possibly adjust pitch/yaw slightly to center the viewport in the field of view if needed. Given the clear line of sight and absence of immediate obstacles, a straightforward forward motion followed by fine positioning is optimal.",
      "action": [
        "Go straight",
        "Go straight",
        "Hover and center the green-rimmed viewport in view"
      ]
    },
    "images": [
      "./data/images/temp_90_0061_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted submarine hull on the right and inspect the visible porthole and surrounding corrosion.",
    "output": {
      "reasoning": "The image shows a rusted submarine hull on the right side of the frame, with a visible porthole near its conning tower and extensive corrosion along its surface. The robot is currently positioned at a moderate distance and slightly to the left of the hull. To approach and inspect the porthole and surrounding corrosion, the robot must first reposition to the right to align with the hull, then move forward steadily while maintaining proximity for detailed visual inspection. Given the underwater environment and potential drift, small, controlled movements are preferred. No immediate obstacles block the path to the hull.",
      "action": [
        "Turn right slightly",
        "Move forward slowly toward the submarine hull",
        "Adjust position to center the porthole in view",
        "Hover and inspect the porthole and surrounding corrosion"
      ]
    },
    "images": [
      "./data/images/temp_90_0038_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the right while maintaining a safe distance from the seabed and avoiding the scattered smaller corals.",
    "output": {
      "reasoning": "The robot is currently facing forward, with a large textured coral formation visible on the right side of the frame. Smaller scattered corals are present on the seabed in front and to the right, near the base of the large coral. To approach the large coral while avoiding smaller ones and maintaining safe altitude above the seabed, the robot should: (1) gently turn right to align toward the large coral without drifting into nearby small corals; (2) advance cautiously while keeping a consistent height (avoid descending); (3) fine-tune heading to stay parallel to the large coral’s edge at a safe lateral distance (~1–2 m), ensuring no collision with protruding branches or nearby small corals. The lighting suggests good visibility, supporting precise maneuvering.",
      "action": [
        "Turn right slightly",
        "Ascend slightly to maintain safe altitude above seabed",
        "Go straight slowly toward the large coral formation",
        "Adjust heading left slightly to maintain safe lateral distance from the large coral and avoid smaller corals"
      ]
    },
    "images": [
      "./data/images/temp_90_0067_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large perforated brown sponge in the foreground while avoiding the rocky outcrop to the right.",
    "output": {
      "reasoning": "The large perforated brown sponge is clearly visible in the foreground, centered-left in the image. A rocky outcrop occupies the right side of the frame, partially shadowed and extending vertically — this must be avoided. The robot appears to be positioned slightly behind and above the sponge, with open water/sandy bottom between. To approach the sponge safely: first, ensure lateral clearance from the right-side rock by moving slightly left; then proceed straight toward the sponge; finally, close in carefully to avoid collision due to perspective or current drift.",
      "action": [
        "Move slightly left to clear the rocky outcrop on the right",
        "Go straight toward the large perforated brown sponge",
        "Slowly approach and stop near the sponge"
      ]
    },
    "images": [
      "./data/images/temp_90_0041_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral formation in the center while avoiding the dense green seaweed patch to its right.",
    "output": {
      "reasoning": "The large green ridged coral formation is centrally located in the image, with distinct vertical ridges and a prominent arch-like structure. To its right (from the robot’s perspective), there is a dense patch of fine, bright green seaweed that must be avoided. The robot appears to be slightly left of the coral’s centerline and at a moderate distance. To approach safely: first, maintain a slight leftward offset to stay clear of the seaweed; then proceed straight toward the coral; if needed, make a minor right correction as it nears to center on the formation without drifting into the seaweed.",
      "action": [
        "Turn slightly left",
        "Go straight toward the central green ridged coral",
        "Adjust slightly right as you near the coral to center on it"
      ]
    },
    "images": [
      "./data/images/temp_90_0001_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater vehicle illuminated by sunbeams and inspect its front sensor array.",
    "output": {
      "reasoning": "The green underwater vehicle is directly ahead and centered in the frame, illuminated by sunbeams from above, indicating it is the primary target. Its front sensor array (a prominent circular yellow-lit aperture) faces the robot. The environment appears clear with no immediate obstacles between the robot and the vehicle. To inspect the front sensor array, the robot must first close the distance safely, then position itself for a close-up view—ideally slightly below or in front at a stable distance to avoid occlusion and ensure optimal lighting. Given the current alignment, the shortest safe path is straight ahead, followed by a slight forward adjustment to reach inspection range.",
      "action": [
        "Go straight toward the green vehicle",
        "Slow down as you near the vehicle",
        "Position yourself 1–2 meters in front of the vehicle, centered on the front sensor array",
        "Hover steadily to inspect the front sensor array"
      ]
    },
    "images": [
      "./data/images/temp_90_0018_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater vehicle lying horizontally among the coral formations, maintaining a safe distance to avoid disturbing the reef.",
    "output": {
      "reasoning": "The large cylindrical underwater vehicle is lying horizontally in the mid-to-lower portion of the image, surrounded by coral formations on both sides and below. The robot appears to be positioned slightly above and behind the vehicle (based on perspective and lighting). To approach it safely without disturbing the reef, I should: (1) descend gently to align with the vehicle’s vertical level while staying clear of the foreground corals; (2) move forward slowly along a path that keeps the vehicle centered and avoids close proximity to nearby coral clusters; (3) maintain lateral clearance—especially from the dense coral on the left—and adjust heading slightly right if needed to stay in open water above the reef bed. No immediate obstacles block a direct but cautious approach.",
      "action": [
        "Descend slightly to match the vehicle's depth",
        "Move forward slowly toward the vehicle",
        "Maintain slight rightward offset to avoid left-side coral",
        "Hold steady at a safe distance (~2–3 meters) once aligned"
      ]
    },
    "images": [
      "./data/images/temp_90_0012_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow and black patterned pipe on the right while maintaining a safe distance from the two submerged cylindrical robots on the left.",
    "output": {
      "reasoning": "The image shows the robot's current view: on the right is a prominent yellow-and-black patterned pipe (target), and on the left are two submerged cylindrical robots (obstacles). The pipe lies roughly along the forward-right direction, slightly ahead. To approach it safely, I must avoid drifting left toward the cylindrical robots. First, I need to align toward the pipe without moving too close to the left obstacles — a slight rightward correction is appropriate. Then, proceed forward while maintaining lateral clearance from the left-side robots. Since the pipe is already in view and relatively close, only a short sequence of controlled movements is needed.",
      "action": [
        "Turn slightly right",
        "Go straight slowly",
        "Maintain distance from left cylindrical robots"
      ]
    },
    "images": [
      "./data/images/temp_90_0068_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible vehicle lying on the seabed and inspect its front hatch and sensor array.",
    "output": {
      "reasoning": "The green cylindrical submersible is lying on the seabed, angled slightly with its front (hatch and sensor array) facing toward the upper-right of the image. From the robot’s current perspective (lower-left), the vehicle is diagonally ahead and to the right. To inspect the front hatch and sensor array, I must first approach it head-on—i.e., align with its longitudinal axis and close the distance. The seabed appears relatively clear of immediate obstacles near the target, though some small rocks and marine growth are present; thus, a controlled forward trajectory with slight rightward correction is safe. Once near, a final fine adjustment will position the robot directly in front of the hatch/sensor area for inspection.",
      "action": [
        "Turn right slightly",
        "Go straight toward the green submersible",
        "Approach until aligned with the front hatch",
        "Hover steadily in front of the hatch and sensor array for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0051_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange and black netted basket on the seabed and inspect its contents.",
    "output": {
      "reasoning": "The orange and black netted basket is clearly visible on the seabed in the foreground, slightly to the right of center. Sunlight from above illuminates it, confirming its position and accessibility. The robot appears to be at a moderate distance (~1–2 meters) and slightly elevated relative to the basket. To inspect its contents, the robot must first approach it closely—ideally positioning directly above or slightly in front—while maintaining stable orientation. No immediate obstacles block the path. Therefore, the plan is: (1) move forward toward the basket, (2) fine-tune position to hover directly over or near the opening, (3) stabilize for visual inspection.",
      "action": [
        "Go straight toward the orange and black netted basket",
        "Move slightly downward to align with the basket's opening",
        "Hover steadily above the basket for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0024_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green columnar structure and inspect its base for attached marine growth or debris.",
    "output": {
      "reasoning": "The central glowing green columnar structure is clearly visible and dominates the scene. It stands vertically with a flared top and illuminated segments, and its base is partially visible, surrounded by net-like debris and some marine growth (e.g., dark organic masses near the bottom). The robot appears to be positioned slightly to the left and at a moderate distance from the structure. To inspect the base closely, I need to: (1) move forward toward the structure while maintaining center alignment, (2) possibly adjust laterally to avoid nearby net debris on the left, and (3) descend slightly if needed to bring the base into optimal view. Given the current perspective, the most immediate action is to advance straight toward the column’s centerline, then fine-tune position once closer.",
      "action": [
        "Go straight toward the central green column",
        "Slightly adjust right to center on the column",
        "Continue forward until near the base",
        "Hover and tilt downward to inspect the base for marine growth or debris"
      ]
    },
    "images": [
      "./data/images/temp_90_0084_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its carved surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned directly ahead of the robot's current viewpoint. It stands upright on the seabed, with carved surfaces facing multiple directions. To inspect its surface for inscriptions or damage, the robot must first close the distance safely without disturbing sediment (to maintain visibility). Given the clear path forward and moderate distance (~2–3 meters), a controlled forward movement is appropriate. Once near (within ~0.5 m), a slow circular maneuver around the column will allow full visual inspection of all sides. The lighting is sufficient for detail capture, and no immediate obstacles block the approach.",
      "action": [
        "Go straight toward the central stone column",
        "Slow down as you approach within 1 meter",
        "Orbit the column clockwise at close range to inspect all carved surfaces",
        "Pause briefly at each cardinal side to capture high-resolution visual data"
      ]
    },
    "images": [
      "./data/images/temp_90_0019_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large ribbed green structure on the seabed while maintaining a safe distance from the coral formation to its right.",
    "output": {
      "reasoning": "The image shows a large ribbed green structure (likely the target) lying horizontally on the seabed. To its right is a dense coral formation with fan-shaped corals, which must be avoided. The robot appears to be positioned slightly in front and to the left of the green structure, with clear space on the left side of the structure. To approach while maintaining a safe distance from the coral, the robot should first move forward cautiously toward the structure, then adjust laterally to stay left of the structure’s midline—ensuring it remains clear of the coral on the right. Since the structure is elongated and oriented roughly parallel to the camera’s view, a slight leftward bias during forward motion will help preserve clearance.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Continue straight while keeping the green structure centered-left in view",
        "Maintain distance so the coral formation remains clearly visible on the right edge of the frame"
      ]
    },
    "images": [
      "./data/images/temp_90_0032_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like structure and inspect the yellow bands and circular emblem on its hull.",
    "output": {
      "reasoning": "The submarine-like structure is visible diagonally across the frame, oriented from lower-left to upper-right. The yellow bands and circular emblem are on its hull, closer to the midsection (near center-right of image). The robot appears to be positioned slightly behind and to the left of the structure, with seaweed-covered seabed on the left side. To approach for inspection, the robot should first move forward toward the structure while slightly adjusting rightward to align with the hull’s midsection where the yellow bands and emblem are located. Once near, a slight turn may be needed to face the hull directly for clear visual inspection.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight slowly",
        "Stop and hover facing the hull"
      ]
    },
    "images": [
      "./data/images/temp_90_0034_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-banded pipe structure while avoiding the surrounding tangled yellow ropes and netting.",
    "output": {
      "reasoning": "The large green-banded pipe structure is directly ahead and slightly elevated in the image, serving as the primary target. Surrounding it are tangled yellow ropes and netting—especially dense on the left and bottom-left, with some extending toward the right side. The robot appears to be positioned below and slightly in front of the structure. To safely approach without entanglement: (1) First, ascend slightly to clear the lower netting; (2) Then, move forward cautiously while maintaining a central path between the left and right rope clusters; (3) Finally, close in on the pipe structure from its front face, avoiding lateral drift into the nets. No sharp turns are needed since the target is aligned with current heading, but fine lateral correction may be required mid-approach.",
      "action": [
        "Ascend slightly",
        "Go straight slowly",
        "Maintain center alignment relative to the pipe",
        "Approach the green-banded pipe structure head-on"
      ]
    },
    "images": [
      "./data/images/temp_90_0016_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red ribbed object on the seabed while avoiding the surrounding rocks and maintaining a safe distance from the yellow fish swimming nearby.",
    "output": {
      "reasoning": "The red ribbed object is clearly visible in the foreground, centered slightly to the right. Surrounding it are several dark rocks: one large rock to the left, another to the far right, and smaller ones scattered nearby. A yellow fish is swimming on the left side of the frame, near the left rock. To approach the red object safely: (1) First, maintain lateral clearance from the yellow fish by staying to its right; (2) Navigate between the left and right rocks—preferably along a central path where seabed is clearer; (3) Advance gradually while keeping the red object centered in view; (4) Avoid sudden turns near rocks; instead, make small course corrections as needed. The safest initial move is to go straight while slightly angling right to avoid the left rock and the fish.",
      "action": [
        "Go straight while slightly turning right",
        "Maintain center alignment with the red ribbed object",
        "Adjust leftward slightly if nearing the right rock",
        "Pause briefly if the yellow fish approaches within 1 meter",
        "Continue forward slowly until 0.5 meters from the red object"
      ]
    },
    "images": [
      "./data/images/temp_90_0059_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral structure in the center while avoiding the smaller yellow and purple spherical corals on the seabed.",
    "output": {
      "reasoning": "The large green ridged coral structure is centrally located and clearly visible. In the foreground, there are smaller yellow and purple spherical corals on the seabed—specifically, one yellow and one purple sphere to the lower right of the central structure, and another yellow sphere to the lower left. To approach the central green coral while avoiding these obstacles, the robot should first move forward cautiously, then adjust laterally to steer clear of the spheres. Given their positions, a slight rightward correction after initial forward motion will help avoid the lower-right yellow and purple spheres, while maintaining safe distance from the left-side yellow sphere. The path should be straight but offset slightly right to stay clear of all small corals.",
      "action": [
        "Go straight slowly",
        "Turn slightly right",
        "Continue straight toward the green ridged coral"
      ]
    },
    "images": [
      "./data/images/temp_90_0039_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue cylindrical structure with yellow bands and inspect its midsection for attached sensors or biofouling.",
    "output": {
      "reasoning": "The blue cylindrical structure with yellow bands is directly in front of the robot and clearly visible. It stands vertically on the seabed, surrounded by coral formations. To inspect its midsection (approximately halfway up the cylinder), the robot must first approach it closely without colliding—maintaining a safe distance initially, then moving laterally or adjusting depth to align with the midsection. Given the current frontal view, the optimal sequence is: (1) advance straight toward the structure to reduce distance, (2) once near (~1–2 m), fine-tune position to center the midsection in the field of view, possibly with slight upward/downward adjustment if needed (though the structure appears vertically aligned and midsection is at camera height), and (3) hold position for inspection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the blue cylindrical structure",
        "Slow down and stop at ~1.5 meters distance",
        "Adjust position slightly to center the midsection in view",
        "Hold position for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0049_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical pipe-like structure emitting bubbles in the center of the underwater cave, while avoiding the algae-covered rocky walls on both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater cave with algae-covered rocky walls on both left and right sides. In the center, a vertical pipe-like structure is visible, emitting a stream of bubbles upward—this is the target. The robot is positioned at the cave entrance or mid-cave, facing the structure. To approach it safely: (1) Maintain central alignment to avoid contact with either wall; (2) Proceed forward steadily while monitoring lateral clearance; (3) The bubble trail provides a clear visual guide for centerline navigation. No sharp turns are needed—the path is straight but requires precise lateral control.",
      "action": [
        "Go straight while keeping centered between the left and right algae-covered walls",
        "Maintain steady forward motion toward the bubble-emitting vertical pipe",
        "Slightly adjust left/right as needed to preserve equal distance from both walls"
      ]
    },
    "images": [
      "./data/images/temp_90_0013_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the stone pillar structure and inspect its base for signs of marine growth or structural damage.",
    "output": {
      "reasoning": "The stone pillar structure is clearly visible in the center-right of the image, standing vertically on the seabed. Sunlight rays illuminate it from above and left, indicating good visibility. The robot appears to be positioned slightly to the left and at a moderate distance (~3–4 m) from the pillar, with open space in front. To inspect the base for marine growth or damage, I need to: (1) move forward toward the pillar while maintaining alignment, (2) get close enough (~1 m) to resolve fine details at the base, and (3) possibly adjust orientation to ensure full base visibility (e.g., slight yaw if needed). No immediate obstacles block the direct path. Given the current view, a straight approach followed by a minor lateral adjustment (if required upon closer inspection) is optimal.",
      "action": [
        "Go straight toward the stone pillar",
        "Slow down as you near the base",
        "Position yourself ~1 meter from the pillar base",
        "Slightly rotate right to view the full circumference of the base"
      ]
    },
    "images": [
      "./data/images/temp_90_0081_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of green mushroom-shaped corals in the center foreground while avoiding the large blue-striped shell on the left.",
    "output": {
      "reasoning": "The image shows a cluster of green mushroom-shaped corals in the center foreground, which is the target. To the left, there is a large blue-striped shell (likely a giant clam or similar bivalve) that must be avoided. The robot is currently positioned such that the corals are directly ahead and slightly to the right of center, while the shell occupies the left mid-ground. To safely approach the corals without colliding with the shell, the robot should first steer slightly right to clear the shell’s position, then proceed straight toward the coral cluster. No immediate obstacles lie directly in the central path once the left-side shell is cleared.",
      "action": [
        "Turn slightly right",
        "Go straight toward the green mushroom-shaped corals"
      ]
    },
    "images": [
      "./data/images/temp_90_0079_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the coiled yellow rope on the seabed while avoiding the large submerged pipe and netting overhead.",
    "output": {
      "reasoning": "The coiled yellow rope is visible on the seabed in the lower-left foreground. A large submerged pipe runs horizontally across the mid-to-background, and netting is draped overhead (especially above and behind the pipe). To approach the rope safely: (1) The robot must stay low to avoid the overhead netting; (2) It should navigate *under* or *around* the pipe—since the rope lies in front of the pipe from the current view, moving forward while staying left of the pipe’s axis avoids collision; (3) The rope is already relatively close, so a short forward movement with slight left correction will bring it into proximity. No sharp turns are needed; gentle steering suffices.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Maintain low altitude to avoid overhead netting"
      ]
    },
    "images": [
      "./data/images/temp_90_0089_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large greenish sponge-like structure in the center-right while avoiding the coral formations on the left and bottom right.",
    "output": {
      "reasoning": "The large greenish sponge-like structure is prominent in the center-right of the image. To approach it, the robot must navigate from its current position (likely near the lower-left, given perspective) toward the center-right. There are coral formations on the left (a tall, dark, textured structure with green algae) and bottom right (branching yellowish corals), which must be avoided. The safest path appears to be: first move slightly right to clear the left coral, then proceed forward while maintaining a safe distance from the bottom-right coral, and finally adjust heading to center on the target sponge. Given the visual layout, a gentle right turn followed by straight advance, then minor fine-tuning, should suffice.",
      "action": [
        "Turn right slightly",
        "Go straight",
        "Adjust heading slightly right to center on the greenish sponge"
      ]
    },
    "images": [
      "./data/images/temp_90_0002_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front right side for damage or marine growth.",
    "output": {
      "reasoning": "The submerged car is visible in the lower-right portion of the image, oriented diagonally with its front facing roughly toward the upper-left. The front right side (from the car’s perspective) corresponds to the side closest to the viewer and slightly to the right — i.e., the right-front quarter panel and headlight area, which appears partially illuminated and covered in green marine growth. To inspect it closely, I need to: (1) move forward toward the car while maintaining a safe distance initially; (2) adjust laterally to position the robot on the car’s front-right side (i.e., approach from the car’s right-front quadrant); (3) get closer for detailed inspection without colliding. Given the car is stationary and no immediate obstacles block the path, a direct but controlled approach is feasible.",
      "action": [
        "Go straight toward the submerged car",
        "Turn slightly right to align with the car's front right side",
        "Move forward slowly to inspect the front right side closely"
      ]
    },
    "images": [
      "./data/images/temp_90_0074_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the upright turquoise cylindrical structure with circular ports and inspect its base where it connects to the seabed pipe.",
    "output": {
      "reasoning": "The upright turquoise cylindrical structure is clearly visible in the center of the image, extending vertically from a seabed pipe. Its base—where it connects to the pipe—is partially visible but currently at a slight distance and slightly below the robot’s current viewpoint. To inspect the base closely, I need to: (1) move forward toward the structure to reduce distance, (2) descend slightly to align with the connection point (since the base is near the seabed), and (3) possibly adjust laterally if needed for optimal viewing angle. The surrounding area appears clear of immediate obstacles, though a diver and other equipment are present to the left—so maintain safe clearance. The lighting from above creates backscatter, but the target is well-lit and identifiable.",
      "action": [
        "Go straight toward the turquoise cylinder",
        "Descend slightly to align with the base connection",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0037_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged pipe structure and inspect the flexible blue corrugated hose connected to its end.",
    "output": {
      "reasoning": "The image shows a large submerged pipe structure lying horizontally on the seabed, with a flexible blue corrugated hose visibly connected to its right end (from the robot’s current perspective). The robot appears to be positioned slightly above and to the left of the structure, with clear visibility of the hose. To inspect the hose connection, the robot must first approach the structure laterally to align with the hose, then move closer for detailed inspection. Since the hose extends toward the foreground/right, the optimal path is: (1) move forward slightly to reduce distance, (2) adjust rightward to center on the hose connection point, (3) advance slowly to get within inspection range without disturbing sediment or risking collision.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_90_0075_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green structure in the center while avoiding the overhanging rock arch on the left.",
    "output": {
      "reasoning": "The glowing green structure is centrally located and clearly visible. An overhanging rock arch occupies the left side of the frame, extending inward from the left wall — posing a collision risk if the robot moves too far left. To safely approach the central structure, the robot should maintain a right-of-center trajectory. From the current viewpoint, the structure is directly ahead but slightly to the right; thus, initial slight right correction ensures clearance from the left arch, followed by steady forward motion toward the target.",
      "action": [
        "Turn slightly right",
        "Go straight toward the glowing green structure",
        "Maintain slight right bias to avoid the left rock arch"
      ]
    },
    "images": [
      "./data/images/temp_90_0025_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small dark cylindrical object on the seabed near the center of the frame, while avoiding the surrounding coral formations.",
    "output": {
      "reasoning": "From the image, a small dark cylindrical object is visible on the seabed near the center of the frame, slightly below the horizontal midline. It appears to be surrounded by coral formations: a large overhanging structure on the upper left, dense coral clusters on the lower left and right, and a prominent vertical coral formation on the far right. To approach the target safely, the robot should first move forward cautiously while staying centered to avoid lateral corals. As it gets closer, minor adjustments may be needed—slight left or right corrections—to navigate through the narrow gaps between coral structures. The path appears clearest directly ahead toward the center, so initial motion should be straight, then fine-tune as proximity increases.",
      "action": [
        "Go straight slowly",
        "Slightly adjust right to avoid left coral",
        "Continue forward toward the dark cylindrical object",
        "Fine-tune left if needed to clear right-side coral",
        "Stop when close to the object"
      ]
    },
    "images": [
      "./data/images/temp_90_0069_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow ring-shaped object on the seabed to the right of the central pipe structure.",
    "output": {
      "reasoning": "The image shows a central vertical pipe structure. To its right, on the seabed, lies a yellow ring-shaped object (highlighted by a yellow oval in the image). The robot is currently facing the pipe, so to approach the yellow ring, it must first turn right to align its heading toward the ring, then move forward (go straight) to close the distance while maintaining orientation toward the target. No immediate obstacles block the path to the ring.",
      "action": [
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_90_0017_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large central column and inspect its surface for attached organisms or structural damage.",
    "output": {
      "reasoning": "The image shows a large central column dominating the scene, positioned directly ahead of the robot's current viewpoint. It is surrounded by coral formations on the left and smaller rocks to the right and rear. The column’s surface appears textured with possible biofouling (e.g., barnacles or algae) and irregularities that may indicate structural wear. To inspect it thoroughly, the robot must first approach closely while maintaining a safe distance from surrounding obstacles (coral on left, rocks on right). Given the column is centered and relatively unobstructed in front, the optimal path is to advance straight toward its midsection, then potentially circle or hover for detailed observation. For this initial phase, the priority is safe forward movement toward the column.",
      "action": [
        "Go straight toward the central column",
        "Maintain slight clearance from left-side coral formation",
        "Stop at ~2 meters from column base for initial visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_90_0029_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submerged vehicle and inspect its side windows for damage or marine growth.",
    "output": {
      "reasoning": "The green submerged vehicle is clearly visible in the center-left of the image, resting on the seabed at a slight angle. Its side windows are facing roughly toward the camera’s current position, but the robot appears to be positioned slightly ahead and to the left of the vehicle’s midsection. To properly inspect the side windows (which run along the hull), the robot should first approach closer along the vehicle’s longitudinal axis, then align parallel to the side to enable a clear, close-range visual inspection. There are no immediate obstacles blocking the path; the orange vertical structures are behind the vehicle and not in the direct approach corridor. Therefore, the plan is: (1) move forward to reduce distance, (2) adjust laterally to align with the side windows, (3) proceed slowly alongside to inspect each window.",
      "action": [
        "Go straight toward the green vehicle",
        "Move right to align parallel with the side windows",
        "Slowly advance alongside the vehicle to inspect each side window"
      ]
    },
    "images": [
      "./data/images/temp_90_0009_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine and inspect its hull near the attached chain and netting.",
    "output": {
      "reasoning": "The submarine is visible in the center-right of the image, oriented horizontally. A thick chain and netting are attached above and slightly ahead of the submarine’s midsection. The robot is currently positioned below and to the left of the submarine, with coiled ropes in the foreground indicating proximity but requiring careful navigation. To inspect the hull near the chain and netting, the robot must first approach the submarine laterally from the left toward its midsection, then adjust upward and forward to reach the attachment point without entanglement. Given the lighting and depth, maintaining a steady, slow approach is critical for visual clarity and safety.",
      "action": [
        "Move right to align with the submarine's midsection",
        "Advance forward slowly toward the submarine",
        "Ascend slightly to position near the hull at chain attachment level",
        "Hover and rotate gently to inspect the hull near the chain and netting"
      ]
    },
    "images": [
      "./data/images/temp_90_0035_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-green coral formation in the foreground while maintaining a safe distance from the large textured rock structure on the left.",
    "output": {
      "reasoning": "The image shows the robot (submersible) positioned with a large textured rock structure on the left and a yellow-green coral formation in the foreground to the right. The coral is clearly visible and relatively close, but the robot must avoid drifting toward the left-side rock. To safely approach the coral, the robot should first slightly turn right to align its heading toward the coral while keeping clearance from the rock. Then, proceed forward cautiously, monitoring proximity to both the rock (left) and the coral (front/right) to maintain safe distance—especially since the coral is in the foreground and could be reached quickly. No immediate obstacles block the path to the coral once aligned.",
      "action": [
        "Turn slightly right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_90_0057_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater vehicle lying horizontally among the coral formations and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The large cylindrical underwater vehicle is lying horizontally on the seabed, partially surrounded by coral formations. It is positioned centrally in the image, slightly angled toward the right, with visible surface features (e.g., rivets, panels) suggesting potential areas for marine growth or damage inspection. The robot appears to be at a moderate distance (~2–3 m) and slightly above/forward of the vehicle. To approach safely without disturbing coral or risking collision, I will: (1) descend gently to align with the vehicle’s midsection height; (2) move forward slowly along its longitudinal axis; (3) maintain a safe lateral offset (~0.5 m) to avoid contact with coral; (4) once near the midsection, begin a slow lateral sweep to inspect surface details. No immediate obstacles block the direct path, but careful navigation is needed due to nearby coral.",
      "action": [
        "Descend slightly to match the vehicle's vertical level",
        "Move forward slowly toward the midsection of the cylindrical vehicle",
        "Maintain a safe distance (~0.5 m) from surrounding coral",
        "Pause near the midsection and begin a slow left-to-right scan of the surface"
      ]
    },
    "images": [
      "./data/images/8_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange cylindrical pipe lying on the seabed and inspect its open end for blockages or damage.",
    "output": {
      "reasoning": "The image shows a large orange cylindrical pipe lying horizontally on the seabed, with its open end facing roughly toward the camera's current position. The robot appears to be at a moderate distance (~2–3 m) and slightly elevated above the seabed. To inspect the open end for blockages or damage, the robot must: (1) approach straight toward the pipe’s open end while maintaining alignment, (2) reduce distance carefully to avoid disturbing sediment or colliding, and (3) position itself directly in front of the opening at close range (~0.5–1 m) for clear visual inspection. No immediate obstacles block the direct path; minor seabed irregularities are present but navigable. The pipe’s orientation suggests turning is unnecessary—only forward motion with slight depth adjustment is needed.",
      "action": [
        "Go straight toward the open end of the orange pipe",
        "Descend slightly to align with the pipe's centerline",
        "Continue forward slowly until ~0.5 meters from the pipe opening",
        "Hover steadily and orient camera directly into the pipe opening for inspection"
      ]
    },
    "images": [
      "./data/images/6_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the larger submerged vehicle with the glowing blue roof and inspect its front section for damage or marine growth.",
    "output": {
      "reasoning": "The image shows two submerged vehicles: a larger one on the left with a distinct glowing blue roof, and a smaller, more damaged one on the right. The target is the larger vehicle with the blue roof. From the robot’s current perspective (likely near center or slightly right), the larger vehicle is positioned to the left and slightly forward. To approach it, the robot should first move leftward to align laterally, then advance straight toward its front section. Once near, it should position itself directly in front for inspection—ensuring stable orientation and sufficient proximity to observe damage or marine growth on the front hull.",
      "action": [
        "Turn left",
        "Go straight toward the larger vehicle",
        "Stop in front of the vehicle",
        "Hover and inspect the front section"
      ]
    },
    "images": [
      "./data/images/10_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient stone column and inspect the glowing blue orb embedded at its top.",
    "output": {
      "reasoning": "The image shows a single ancient stone column centered in the frame, with a glowing blue orb clearly visible at its top. The robot appears to be positioned directly in front of the column at a moderate distance, with clear visibility and no obstacles between it and the target. To inspect the orb closely, the robot should first move forward toward the base of the column, then possibly adjust vertically or tilt upward to focus on the orb at the top. Since the orb is at the top and the column is tall, after reaching proximity, a slight upward orientation adjustment may be needed for optimal inspection.",
      "action": [
        "Go straight toward the stone column",
        "Stop near the base of the column",
        "Tilt camera upward to focus on the glowing blue orb"
      ]
    },
    "images": [
      "./data/images/7_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green core while maintaining a safe distance from the surrounding spiky green and yellow structures.",
    "output": {
      "reasoning": "The image shows a symmetrical, radial structure with a bright glowing green core at the center. Surrounding it are concentric layers of spiky green and yellow structures that appear hazardous. The robot is currently positioned directly facing the core (central perspective), suggesting forward motion is possible—but must be carefully controlled to avoid contact with the spikes. Since the spikes protrude radially outward, maintaining alignment with the central axis minimizes risk. A safe approach strategy is: (1) begin by moving straight toward the core while monitoring proximity to inner spike tips, (2) stop before entering the immediate spike zone (i.e., when the core fills ~30–40% of the frame), ensuring a buffer distance. No lateral correction is needed due to symmetry and centered framing.",
      "action": [
        "Go straight slowly",
        "Stop when the green core occupies about one-third of the image height"
      ]
    },
    "images": [
      "./data/images/4_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure on the left with the circular openings and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows an underwater scene with several green cylindrical structures featuring circular openings. The largest such structure is prominently positioned on the left side of the frame, partially illuminated by sun rays from above. It has a textured surface (possibly encrusting organisms) and visible circular apertures. To approach it for inspection, the robot must first navigate toward the left while maintaining a safe distance to avoid collision, then move closer to inspect its surface. Given the current perspective, the robot appears to be slightly in front and to the right of the target structure. Therefore, the plan is: (1) turn left to align with the structure, (2) advance straight toward it, (3) close in for detailed surface inspection (maintaining stable hover or slow forward motion).",
      "action": [
        "Turn left",
        "Go straight toward the large green cylindrical structure",
        "Approach closely and hover to inspect the surface"
      ]
    },
    "images": [
      "./data/images/1_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green mechanical structure on the seabed and inspect its central cylindrical component.",
    "output": {
      "reasoning": "The image shows a large green mechanical structure on the seabed, centrally located and clearly visible. It has a prominent central cylindrical component with radial supports and circular ports—this is the inspection target. The robot (submersible) is currently positioned above and slightly to the left of the structure, at a moderate distance. To approach and inspect the central cylinder, I need to: (1) descend vertically toward the seabed to reach the structure’s level; (2) move forward (straight) to close the horizontal distance; (3) fine-tune position to center on the cylindrical core; (4) possibly rotate slightly to align the camera for optimal inspection view. No immediate obstacles block the direct path, though caution is needed near the surrounding spiky organisms and debris. Lighting from above helps visibility.",
      "action": [
        "Descend vertically",
        "Go straight toward the green structure",
        "Move closer to the central cylindrical component",
        "Adjust position to center on the cylinder",
        "Rotate slightly to face the cylinder directly"
      ]
    },
    "images": [
      "./data/images/1_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submerged vehicle and inspect its side windows for damage or marine growth.",
    "output": {
      "reasoning": "The green submerged vehicle is clearly visible in the center-left of the image, lying horizontally on the seabed. Its side windows are facing upward and slightly toward the camera. The robot appears to be positioned at a moderate distance (~2–3 m) and slightly ahead of the vehicle’s midsection. To inspect the side windows, the robot must first approach closer (to ~1 m) for high-resolution visual inspection, then align parallel to the vehicle’s longitudinal axis to view all side windows systematically. There are no immediate obstacles blocking the path; orange buoys are anchored above but not interfering with horizontal movement near the seabed. Recommended sequence: (1) move forward to reduce distance, (2) adjust lateral position to align with the vehicle’s side, (3) proceed slowly alongside to inspect each window.",
      "action": [
        "Go straight toward the green vehicle",
        "Move slightly left to align with the vehicle's side",
        "Proceed slowly alongside the vehicle to inspect side windows"
      ]
    },
    "images": [
      "./data/images/6_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central vortex-like structure while maintaining a safe distance from its textured ridges.",
    "output": {
      "reasoning": "The image shows a central vortex-like structure with concentric, textured ridges radiating outward. The robot appears to be positioned directly in front of the vortex, aligned with its axis. To approach the center while maintaining a safe distance from the ridges, the robot should advance slowly along the central axis—where ridges are furthest apart and least likely to cause collision. The bright light source at the far right suggests a directional reference, but the primary goal is axial approach. Since the structure is symmetric and the current view is centered, no lateral correction is needed initially. A controlled forward motion will bring the robot closer while preserving clearance from the ridged periphery.",
      "action": [
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/4_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge located on the seabed near the cluster of dark rocks in the foreground.",
    "output": {
      "reasoning": "The yellow sponge is not clearly visible in the current frame, but the instruction specifies it is on the seabed near a cluster of dark rocks in the foreground. In the image, there is a prominent cluster of dark rocks at the bottom right and center foreground. The seabed is visible and relatively flat there. To approach the sponge, I need to first move toward that rock cluster. Since the robot appears to be positioned slightly left and above the target area (based on perspective), the most direct path is to go straight ahead while slightly adjusting right to align with the rock cluster. Once closer, fine positioning can occur—but for now, the atomic step is to advance toward the foreground rocks.",
      "action": [
        "Go straight",
        "Slightly turn right"
      ]
    },
    "images": [
      "./data/images/3_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its upper ornamental structure for signs of marine growth or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned slightly to the right of center and relatively close to the robot’s current position. The upper ornamental structure (capitals with volutes) is illuminated by overhead light, making it inspectable. To inspect for marine growth or damage, the robot must first approach the column closely enough for high-resolution observation—ideally within 1–2 meters—and then orient its camera upward to focus on the upper ornamental section. Given the open floor space and absence of immediate obstacles, a direct forward movement followed by a slight upward tilt is sufficient. No turning is needed since the column is already in the forward field of view.",
      "action": [
        "Go straight toward the central stone column",
        "Stop approximately 1.5 meters from the base",
        "Tilt camera upward to focus on the upper ornamental structure"
      ]
    },
    "images": [
      "./data/images/2_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-tipped coral formation in the foreground while maintaining clearance from the large dark structure above.",
    "output": {
      "reasoning": "The yellow-tipped coral formation is visible in the foreground, lower-right portion of the image (appearing as pale, branching corals with yellow tips). A large dark structure (likely a rock or overhang) dominates the upper part of the scene, casting shadows and posing a collision risk if the robot ascends. The robot (submarine-like vehicle) is currently positioned mid-water, slightly left of center, oriented horizontally. To approach the coral safely: (1) First, maintain current depth or slightly descend to increase vertical clearance from the overhead structure; (2) Then, move forward while gently steering right toward the coral cluster; (3) Avoid upward pitch to prevent encroaching on the dark structure. The coral is close enough that only short, controlled movements are needed.",
      "action": [
        "Descend slightly",
        "Go straight",
        "Turn right gently",
        "Approach the yellow-tipped coral"
      ]
    },
    "images": [
      "./data/images/8_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small green spherical organism floating near the center and inspect it closely.",
    "output": {
      "reasoning": "The small green spherical organism is visible near the center of the image, slightly above the seabed and suspended in the water column. It appears to be tethered by a thin filament. The robot is currently positioned below and somewhat behind it (from the camera perspective), with several large green cup-shaped organisms in the foreground and midground. To approach it closely for inspection, the robot should first move forward toward the center while maintaining depth to avoid colliding with foreground structures. Then, a slight upward adjustment may be needed to align with the organism’s vertical position. Finally, a slow advance to close proximity will enable detailed inspection.",
      "action": [
        "Go straight toward the center of the image",
        "Slightly ascend to match the organism's depth",
        "Slowly approach the green spherical organism"
      ]
    },
    "images": [
      "./data/images/5_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the underwater cave opening toward the sunlit area while avoiding the large rocks on the seabed.",
    "output": {
      "reasoning": "The image shows an underwater cave with a visible sunlit exit ahead, framed by the cave arch. Large rocks are scattered on the seabed, particularly in the lower half of the frame—some near the center and right. The robot is positioned inside the cave, facing the opening. To safely navigate toward the sunlit area: (1) First, maintain forward motion while staying slightly elevated to avoid seabed rocks; (2) Observe that the clearest path appears to be slightly left of center, where fewer rocks obstruct the way and light rays indicate open water; (3) Adjust heading gently left to steer clear of the central and right-side rock clusters; (4) Continue forward toward the brightest region (sunlit exit), monitoring for new obstacles as depth changes.",
      "action": [
        "Go straight while maintaining slight altitude",
        "Gently turn left to avoid central seabed rocks",
        "Proceed forward toward the sunlit opening",
        "Monitor for obstacles and adjust heading as needed"
      ]
    },
    "images": [
      "./data/images/3_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater passage between the left rock wall and the central coral-covered formation, heading toward the sunlit opening ahead.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage bounded on the left by a dark rock wall and on the right by a large, irregular coral-covered formation. Sunlight streams in from an opening ahead, indicating the goal direction. To safely navigate this constrained corridor, the robot must maintain central alignment—avoiding collision with either side—while progressing forward toward the light. Given the perspective, the passage appears to widen slightly ahead, so a steady forward motion with minor lateral corrections is appropriate. No immediate obstacles block the path, but precise heading control is critical due to the tight clearance.",
      "action": [
        "Go straight slowly while maintaining center alignment between left rock wall and right coral formation",
        "Slightly adjust left if nearing right coral, or right if nearing left rock wall",
        "Continue forward toward the sunlit opening ahead"
      ]
    },
    "images": [
      "./data/images/9_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate upward toward the illuminated surface opening while avoiding the overhead mesh structure.",
    "output": {
      "reasoning": "The robot is currently oriented upward, with a bright illuminated surface opening visible ahead and slightly above. An overhead mesh structure looms directly above the robot, partially obstructing the direct vertical path. To avoid collision, the robot must ascend while laterally shifting—preferably to the right (from its perspective), where the mesh appears less dense and more open space is visible. The yellow lights on the robot’s sides suggest maneuverability; turning slightly right before ascending ensures clearance from the mesh. Once clear of the mesh, continue straight upward toward the light source.",
      "action": [
        "Turn slightly right",
        "Ascend steadily",
        "Maintain slight rightward bias until clear of mesh",
        "Proceed straight upward toward illuminated opening"
      ]
    },
    "images": [
      "./data/images/10_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the textured blue coral-like structure resting on the circular rock base and inspect its surface details.",
    "output": {
      "reasoning": "The target is a textured blue coral-like structure sitting on a circular rock base, clearly visible in the center of the image. The robot appears to be positioned slightly in front and possibly elevated relative to the target. To inspect surface details, the robot must first approach closely without disturbing the structure, then position itself for optimal viewing—likely at a slight angle or directly in front, maintaining stable distance. Given the lighting from above-left, approaching from the front-right may minimize shadow interference. No immediate obstacles block the path. Step-by-step: (1) Move forward toward the structure to reduce distance; (2) Fine-tune position to center the structure in view; (3) Slightly descend or adjust pitch to get closer for surface inspection while avoiding contact.",
      "action": [
        "Go straight toward the blue coral-like structure",
        "Slow down and center the structure in the field of view",
        "Gently descend and move closer for detailed surface inspection"
      ]
    },
    "images": [
      "./data/images/7_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical pipe-like structure emitting bubbles in the center of the underwater cave while avoiding the surrounding rock formations.",
    "output": {
      "reasoning": "The image shows an underwater cave with a central vertical pipe-like structure emitting a stream of bubbles, clearly visible along the light ray from above. The structure is flanked by irregular rock formations on both left and right sides, forming a narrow passage. To approach it safely while avoiding collisions, the robot should first align itself centrally in the passage, then proceed forward cautiously. Since the pipe is directly ahead and centered in the field of view, no lateral correction is needed initially. However, due to potential drift or uneven terrain, small adjustments may be required mid-approach. The safest sequence is: (1) maintain centerline heading, (2) advance steadily toward the bubble column, (3) fine-tune position if rocks encroach laterally.",
      "action": [
        "Go straight",
        "Maintain center alignment",
        "Proceed forward toward bubble stream",
        "Slightly adjust left or right if nearing rock walls"
      ]
    },
    "images": [
      "./data/images/9_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green sponge structure on the right while avoiding the dark overhanging coral formation above.",
    "output": {
      "reasoning": "From the image, the large green sponge structure is located on the right side of the scene, appearing as a textured, porous green mass near the bottom. Above it and slightly to the left is a dark, overhanging coral formation that extends downward — this poses a collision risk if the robot ascends or moves too directly upward/rightward. The robot’s current position appears to be slightly left and below the target sponge, with open water in front and to the right at a lower depth. To safely approach: (1) First, move forward slightly to gain horizontal proximity while staying below the overhang; (2) Then, gently turn right to align toward the sponge; (3) Finally, advance carefully toward the sponge while maintaining depth to avoid the overhead coral. No upward motion is needed—staying low ensures clearance.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight toward the green sponge"
      ]
    },
    "images": [
      "./data/images/5_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green SUV and inspect its front right wheel for damage or entanglement.",
    "output": {
      "reasoning": "The green SUV is visible in the center-left of the image, lying on the seabed at a slight angle. Its front right wheel is partially visible and appears accessible from the current viewpoint. To inspect it closely, I need to approach the vehicle from the front-right side. First, I should move forward toward the SUV while maintaining a safe distance to avoid disturbing sediment. Then, I will reposition to the front-right quadrant of the vehicle to get a clear view of the target wheel. Finally, I will move closer for detailed inspection.",
      "action": [
        "Go straight toward the green SUV",
        "Turn right to position yourself at the front-right side of the SUV",
        "Move forward slowly to approach the front right wheel",
        "Hover and inspect the front right wheel for damage or entanglement"
      ]
    },
    "images": [
      "./data/images/2_2.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate between the two large stone columns toward the sunlit opening in the background.",
    "output": {
      "reasoning": "The image shows two large stone columns framing a central passage, with a sunlit opening visible in the background through the water. The robot is positioned in front of the columns, facing them. To navigate toward the sunlit opening, it must proceed straight through the gap between the two columns. There are no immediate obstacles in the passage, and the lighting suggests the opening is directly ahead. Therefore, the plan is: (1) align with the central gap, (2) move forward steadily while maintaining centerline, (3) continue until nearing the opening.",
      "action": [
        "Go straight toward the gap between the two stone columns",
        "Maintain center alignment between the columns",
        "Continue forward toward the sunlit opening"
      ]
    },
    "images": [
      "./data/images/temp_307_0116_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large moss-covered stone pillar on the left and inspect its surface for embedded artifacts or markings.",
    "output": {
      "reasoning": "The image shows a large, moss-covered stone pillar on the left side of the frame, illuminated by light rays from above. It is the most prominent structure and matches the description in the instruction. The robot appears to be positioned slightly in front and to the right of the pillar, with clear space to maneuver toward it. To approach the pillar for inspection, the robot should first move leftward to align with its base, then advance straight toward it while maintaining proximity for surface inspection. No immediate obstacles block the path to the pillar’s left side.",
      "action": [
        "Turn left",
        "Move forward toward the moss-covered stone pillar",
        "Get closer to the pillar's surface for detailed inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0302_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical pipe structure with attached sensors and inspect its lower junction with the green grid platform.",
    "output": {
      "reasoning": "The image shows a vertical pipe structure with attached sensors positioned centrally above a green grid platform. The lower junction—where the pipe meets the platform—is visible but slightly distant and angled. To inspect it closely, the robot must first approach the structure while maintaining alignment with the junction. Given the current perspective, moving straight forward will reduce distance to the pipe; then a slight downward tilt or descent may be needed to focus on the lower junction. No immediate obstacles block the direct path, so a straightforward approach is safe.",
      "action": [
        "Go straight toward the vertical pipe",
        "Descend slightly to align with the lower junction",
        "Hover and inspect the junction between pipe and green grid platform"
      ]
    },
    "images": [
      "./data/images/temp_307_0100_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green segmented pipe lying on the seabed and inspect its surface for anomalies.",
    "output": {
      "reasoning": "The large green segmented pipe lies horizontally on the seabed, occupying the lower-right to center portion of the image. The robot is currently positioned above and slightly ahead of the pipe, facing it. To approach and inspect its surface, I need to: (1) descend vertically to reach seabed level near the pipe; (2) move forward slowly toward the nearest segment (front tip of pipe); (3) once close, perform a lateral sweep (e.g., move right or left along the pipe) to inspect surface anomalies. Given the pipe’s orientation (pointing left-to-right), approaching from the front and then moving right along its length is optimal for full visual coverage. No immediate obstacles block the path.",
      "action": [
        "Descend to seabed level",
        "Move forward toward the front tip of the green pipe",
        "Move right along the pipe while maintaining close proximity",
        "Inspect surface for anomalies during lateral movement"
      ]
    },
    "images": [
      "./data/images/temp_307_0045_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central gray rock with yellow bands and inspect its top surface for embedded sensors or markings.",
    "output": {
      "reasoning": "The image shows multiple rocks with yellow bands; the central gray rock with yellow bands is clearly visible in the foreground, slightly left of center, and appears to be the target. It has a distinct textured top surface with circular features that may be sensors or markings. To inspect its top surface, I need to first approach it closely (maintaining safe distance to avoid disturbance), then position directly above it for optimal overhead view. The seabed is relatively flat and clear around it, allowing straight-line approach. Since the robot is likely facing the scene from a slight angle, the first step is to move forward toward the rock, then fine-tune position to center it in the field of view before ascending slightly for top-down inspection.",
      "action": [
        "Go straight toward the central gray rock with yellow bands",
        "Stop when ~1 meter away",
        "Adjust position to center the rock in the camera view",
        "Slowly ascend to hover directly above the rock",
        "Inspect the top surface for embedded sensors or markings"
      ]
    },
    "images": [
      "./data/images/temp_307_0080_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine from below while maintaining a safe distance and aligning with its port side for inspection.",
    "output": {
      "reasoning": "The green submarine is positioned horizontally in the mid-water column, slightly above the seabed. From the current viewpoint (below and slightly behind), the robot needs to: (1) ascend gently to approach from below without colliding with the seabed or coral; (2) move forward while maintaining vertical clearance beneath the submarine; (3) adjust lateral position to align with the port (left) side of the submarine as viewed from its bow direction — since the submarine’s bow points rightward in the image, its port side is the left-facing hull; (4) hold position at a safe distance for inspection. The lighting suggests upward visibility is good, supporting controlled ascent and alignment.",
      "action": [
        "Ascend slowly to position directly below the submarine",
        "Move forward cautiously while staying beneath the submarine",
        "Drift left to align with the submarine's port side",
        "Maintain steady depth and distance for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0168_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge cluster on the seabed while maintaining a safe distance from the large dark coral structure above.",
    "output": {
      "reasoning": "From the image, the yellow sponge cluster is visible on the seabed in the lower-right foreground. A large dark coral structure looms above and slightly behind the robot’s current position (center-top of frame), posing a potential collision risk if the robot ascends or drifts upward. The robot is currently oriented horizontally, facing roughly forward-left. To approach the sponge cluster safely: (1) First, maintain depth to avoid the overhead coral—so no upward movement; (2) The sponge cluster lies to the right and slightly ahead; thus, a gentle right turn is needed to align heading toward it; (3) Then proceed forward at low altitude (just above seabed) to close distance while keeping clear of the coral overhead. No sharp maneuvers are required—the path appears open laterally and downward.",
      "action": [
        "Turn right slightly",
        "Go straight slowly while maintaining current depth"
      ]
    },
    "images": [
      "./data/images/temp_307_0011_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine positioned at the center of the cave opening, navigating carefully around the surrounding rocks and sediment.",
    "output": {
      "reasoning": "The submarine is clearly visible at the center of the cave opening, illuminated by light rays from above. The robot is currently outside the cave, facing the entrance. There are rocks and sediment on both left and right sides near the entrance, requiring careful lateral positioning before advancing. To safely approach: (1) First, align centrally with the submarine by moving slightly right to avoid the larger rock mass on the left foreground; (2) Then proceed straight forward through the cleared central corridor; (3) Finally, slow down and fine-tune position as it nears the submarine to avoid collision with sediment or hull features.",
      "action": [
        "Move slightly right to center alignment with the submarine",
        "Go straight toward the cave opening",
        "Advance slowly toward the submarine while maintaining central trajectory",
        "Hover gently in front of the submarine"
      ]
    },
    "images": [
      "./data/images/temp_307_0056_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater structure with green lighting and inspect its base where it connects to the seabed covered in green tubular organisms.",
    "output": {
      "reasoning": "The image shows a tall cylindrical underwater structure illuminated with green lights, positioned centrally. At its base, the seabed is densely covered with green tubular organisms (likely sponges or tunicates). The robot appears to be at a moderate distance and slightly below the structure’s midsection, facing it. To inspect the base where the structure meets the seabed, the robot must: (1) move forward toward the structure to reduce distance, (2) descend vertically to reach the seabed level, and (3) position itself close and stable for inspection—ideally slightly offset to avoid occlusion by the structure’s shadow or protrusions. Given the current perspective, a straight approach followed by a controlled descent is optimal; no immediate obstacles block the path.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Descend slowly to align with the seabed level",
        "Move slightly forward and adjust position to center the base in view",
        "Hold position for inspection of the connection zone"
      ]
    },
    "images": [
      "./data/images/temp_307_0243_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange underwater robot and inspect its front sensors while avoiding the coral formations on the left.",
    "output": {
      "reasoning": "The orange underwater robot is directly ahead and slightly above the current viewpoint, with visible front sensors (two circular elements). Coral formations occupy the left side of the scene, requiring the robot to maintain a rightward bias during approach. To safely inspect the front sensors, I need to: (1) first steer right to clear the coral, (2) then advance straight toward the robot while maintaining safe lateral distance, and (3) finally position closely in front for sensor inspection without colliding. The robot appears stationary, so a controlled, steady approach is feasible.",
      "action": [
        "Turn right slightly",
        "Go straight toward the orange robot",
        "Move forward slowly to position in front of the robot",
        "Hover and inspect the front sensors"
      ]
    },
    "images": [
      "./data/images/temp_307_0102_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its carved details while avoiding the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible in the image, standing upright near the center of the frame. Scattered debris (flat circular objects and broken blocks) lies on the seabed around its base, especially to the left and right front. To approach safely while avoiding debris, I should first align the robot directly toward the pillar from a safe distance, then advance cautiously along a central path where the seabed appears clearest (midway between left and right debris clusters). Once near, I’ll position for close inspection—slightly offset to the front-right to get optimal lighting and view of carved details without disturbing sediment. The debris does not block the direct frontal approach, but lateral deviations are needed during final approach to avoid contact.",
      "action": [
        "Turn slightly left to center the pillar in the field of view",
        "Go straight slowly toward the pillar, maintaining central alignment",
        "Adjust right by 5 degrees to avoid foreground circular debris",
        "Continue forward at reduced speed until 2 meters from base",
        "Hover and rotate slowly clockwise to inspect carved details on front-right face"
      ]
    },
    "images": [
      "./data/images/temp_307_0285_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater device on the seabed while avoiding the red pipe in the foreground.",
    "output": {
      "reasoning": "The green underwater device is visible on the seabed, positioned slightly to the left and behind the red pipe in the foreground. The red pipe spans horizontally across the lower part of the image and blocks a direct forward path. To avoid collision, I must first move laterally away from the pipe—specifically, to the left (since the green device is left-of-center relative to the pipe’s obstruction), then proceed forward toward the device once clear. The device appears stationary and stable on the seabed, so a controlled approach is feasible after clearing the obstacle.",
      "action": [
        "Turn left",
        "Move forward slowly",
        "Adjust right slightly to align with green device",
        "Approach green underwater device"
      ]
    },
    "images": [
      "./data/images/temp_307_0153_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large yellowish pipeline lying horizontally above the coral reef, maintaining a safe distance to inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The large yellowish pipeline is visible diagonally across the upper right portion of the image, lying horizontally above a coral reef. The robot appears to be positioned below and slightly to the left of the pipeline, with clear water ahead. To approach it safely while maintaining inspection distance, I should first move forward toward the pipeline’s midsection (which is most accessible), then adjust laterally to align parallel to it at a safe standoff distance (~1–2 m). Since the pipeline slopes slightly downward from left to right in the frame, approaching from the lower-left ensures minimal risk of collision with reef structures below. No immediate obstacles block the path forward.",
      "action": [
        "Go straight slowly",
        "Slightly ascend to match pipeline elevation",
        "Drift right to align parallel to pipeline at ~1.5 m distance",
        "Maintain steady forward motion along pipeline for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0205_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater device with coiled orange cables and inspect its connection point for entanglement or damage.",
    "output": {
      "reasoning": "The green cylindrical device with coiled orange cables is clearly visible in the image, positioned slightly to the right and ahead of the robot’s current viewpoint. Sunlight penetrates from above, illuminating the scene and revealing the connection point near the left end cap of the cylinder (closest to the robot). The cables appear tangled around the midsection, but the critical inspection zone is the connection interface—likely where the green cap meets the cylinder body. To inspect for entanglement or damage, I must first approach closely without disturbing the cables, then orient the camera to focus on the connection point. Given the current distance (~1.5–2 m estimated), a controlled forward movement is needed, followed by a slight leftward adjustment to center the connection point in view, and finally a minor pitch-up or hover to stabilize for visual inspection.",
      "action": [
        "Go straight slowly",
        "Turn slightly left",
        "Hover and adjust position to center the green device's left-end connection point in view",
        "Inspect the connection point for entanglement or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0174_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-green submersible vehicle ahead while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The yellow-green submersible is directly ahead, centered in the corridor formed by rock formations on both sides. To approach it safely, I must maintain central alignment to avoid collisions with the left and right rocks. The current view shows sufficient clearance, so the first step is to move forward steadily while monitoring proximity to the rocks. As the vehicle gets closer, minor lateral corrections may be needed, but initially, a straight advance is optimal. No immediate obstacles block the direct path.",
      "action": [
        "Go straight",
        "Maintain center alignment",
        "Monitor distance to left and right rock formations"
      ]
    },
    "images": [
      "./data/images/temp_307_0227_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue and orange underwater robot directly ahead while maintaining a safe distance from the surrounding rocky seabed and coral formations.",
    "output": {
      "reasoning": "The target—a blue and orange underwater robot—is directly ahead in the center of the frame, clearly visible under sunbeams. The seabed is uneven with rocky outcrops and coral formations on both left and right sides, requiring lateral clearance. To approach safely: (1) Maintain forward motion while keeping central alignment to avoid drifting toward obstacles; (2) Slightly elevate or hold depth to prevent contact with seabed features; (3) Monitor distance—stop before entering close proximity (<1 m) unless instructed otherwise. Since the robot is stationary and centered, a straight-forward approach with minor stabilization is sufficient.",
      "action": [
        "Go straight slowly",
        "Maintain central heading",
        "Hold depth slightly above seabed",
        "Stop at safe distance (~2 meters)"
      ]
    },
    "images": [
      "./data/images/temp_307_0221_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green vertical structure while avoiding the nearby net and cables on the seafloor.",
    "output": {
      "reasoning": "The glowing green vertical structure is the primary target, centrally located and highly visible. To its left (from the robot’s perspective) lies a large net on the seafloor, and thin cables extend across the bottom near the base of the structure—especially visible to the right and front. To safely approach, the robot should first move forward cautiously while staying slightly to the right of center to avoid the net on the left. Then, as it nears the base, it must adjust laterally to steer clear of the cables extending from the right side and front. The final step is to position directly in front of the structure at a safe distance for observation or interaction.",
      "action": [
        "Go straight while slightly veering right to avoid the net on the left",
        "Continue forward, maintaining clearance from the cables on the seafloor ahead and to the right",
        "Adjust left slightly to center in front of the glowing green structure",
        "Stop at a safe distance in front of the structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0289_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe in the foreground while maintaining a safe distance from the overhead net structure.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe in the foreground, extending diagonally from bottom-left toward upper-right. An overhead net structure looms above and slightly ahead of the robot’s current position. To approach the pipe safely, the robot must avoid ascending too close to the net (which is directly above the pipe’s midsection). The pipe is accessible from its lower-left side, where there is clearer space. Therefore, the plan is: (1) move forward cautiously while staying low to avoid the net; (2) slightly adjust leftward to align with the pipe’s near end; (3) continue forward along the pipe’s side at a safe lateral distance (e.g., ~0.5–1 m) to maintain clearance from both the net above and potential contact with the pipe.",
      "action": [
        "Go straight slowly while keeping depth shallow",
        "Slightly turn left to align with the pipe's near end",
        "Proceed forward alongside the pipe, maintaining lateral clearance"
      ]
    },
    "images": [
      "./data/images/temp_307_0066_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral formation from the front while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The image shows a large, prominent green coral formation centered in the frame, directly ahead of the robot's current position. The coral is well-lit and occupies the mid-to-near field, indicating it is relatively close. To approach it from the front while maintaining a safe distance, the robot should proceed straight but with controlled forward motion—avoiding sudden acceleration or lateral drift. No immediate obstacles block the frontal path, but the seabed texture and nearby smaller corals suggest cautious navigation is needed. A single 'Go straight' step is appropriate for initial approach; subsequent steps (not included here) would involve fine-tuning distance based on real-time feedback.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0040_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central vertical structure wrapped in netting and inspect its upper spherical node.",
    "output": {
      "reasoning": "The image shows a tall vertical structure wrapped in netting, with a clear upper spherical node visible at the top. The robot appears to be positioned below and slightly to the side of the structure, looking upward. Sunlight rays indicate the camera is oriented upward, suggesting the robot is already near the base or mid-section. To approach the central vertical structure and inspect its upper spherical node, the robot should first move closer horizontally toward the centerline of the structure (since it's currently off-center), then ascend vertically while maintaining alignment with the central axis to reach the spherical node. Given the perspective, a slight forward-and-up trajectory is optimal to avoid netting entanglement and maintain visual contact.",
      "action": [
        "Move forward toward the central vertical structure",
        "Ascend vertically while centering on the structure",
        "Approach the upper spherical node for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0278_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green structure and inspect its base where the tentacle-like arms meet the seabed.",
    "output": {
      "reasoning": "The central glowing green structure is clearly visible and dominates the scene. It has a vertical main body with multiple tentacle-like arms extending outward and downward, anchoring into the seabed. The base—where these arms meet the seabed—is illuminated and accessible from the front. From the current viewpoint, the robot is positioned slightly in front and above the structure, at moderate distance. To inspect the base closely, I need to: (1) move forward toward the structure while maintaining alignment with its center; (2) descend slightly to bring the camera level with the junction of arms and seabed; (3) possibly adjust laterally if needed for optimal view, but initial approach should be straight ahead to avoid disturbing sediment or missing the target. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the central green structure",
        "Descend slightly to align camera with the base of the tentacle arms",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0192_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible in the image, positioned slightly ahead and to the right of the robot’s current viewpoint. It stands upright on a tiled seabed, with sunlight filtering from above creating shadows and highlights that reveal surface texture. Marine growth (e.g., algae, barnacles) appears present near its base and along vertical seams; structural damage such as cracks or erosion may be discernible upon closer inspection. To inspect effectively, the robot must first approach the pillar while maintaining a stable orientation, then circle or hover near its mid-section for detailed visual assessment. Given the presence of other pillars and debris nearby, careful navigation is required to avoid collisions. The optimal path is: (1) advance straight toward the pillar’s base, (2) adjust laterally to center on the pillar, (3) ascend slightly to inspect upper-mid surface, (4) maintain close proximity (~0.5–1 m) while slowly rotating around it for 360° coverage.",
      "action": [
        "Go straight toward the central stone pillar",
        "Adjust left slightly to center on the pillar",
        "Move closer until ~1 meter away",
        "Ascend gently to align camera with mid-height of pillar",
        "Rotate slowly clockwise around the pillar for full surface inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0136_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green SUV and inspect its front right wheel and undercarriage for structural damage.",
    "output": {
      "reasoning": "The green SUV is visible in the center-left of the image, resting on the seabed. Its front right wheel and undercarriage are partially visible from the current angle (robot appears to be slightly behind and to the left of the vehicle). To inspect the front right wheel and undercarriage for structural damage, I need to: (1) approach the vehicle safely—moving forward and slightly right to align with its front-right quadrant; (2) get closer to the front right wheel area; (3) position the robot low and near the undercarriage for detailed visual inspection. Obstacles include scattered debris on the seabed and a column to the right—must avoid collision. Lighting is dim but sufficient for visual inspection once close enough.",
      "action": [
        "Move forward and slightly right toward the green SUV",
        "Approach the front right wheel area while maintaining safe distance from seabed debris",
        "Descend slightly and position robot parallel to undercarriage near front right wheel",
        "Inspect front right wheel and adjacent undercarriage for cracks, deformation, or detachment"
      ]
    },
    "images": [
      "./data/images/temp_307_0006_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green spherical object with circular indentations in the center of the sandy floor.",
    "output": {
      "reasoning": "The green spherical object with circular indentations is clearly visible in the center of the sandy floor, slightly ahead and to the left of the robot’s current position. The path appears unobstructed—no red obstacles or significant terrain irregularities block direct access. To approach it efficiently: first, align the robot toward the object by turning slightly left; then proceed straight forward until close enough for interaction (e.g., grasping or inspection). Given the perspective, a small left turn (~15–20°) followed by a short forward movement should center the target in the field of view and reduce distance safely.",
      "action": [
        "Turn left slightly",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0028_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible device suspended among the vertical pipes and inspect its lower orange component.",
    "output": {
      "reasoning": "The green cylindrical submersible device is visible in the center-right of the image, suspended among vertical pipes. It has a distinct lower orange component (likely a sensor or connector) hanging below it. The robot appears to be positioned slightly below and to the left of the device, with rocky seabed and algae-covered structures in the foreground. To approach and inspect the lower orange component, the robot must: (1) move forward and slightly right to align with the device’s central axis, (2) ascend gently to reach the device’s mid-height, then (3) move closer while maintaining stable orientation to inspect the orange part from a frontal/slight-bottom angle. Obstacles (pipes and rocks) are present but there is a clear path from current position toward the device.",
      "action": [
        "Move forward and slightly right",
        "Ascend gently",
        "Approach the green cylindrical device",
        "Position for close inspection of the lower orange component"
      ]
    },
    "images": [
      "./data/images/temp_307_0085_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater instrument suspended above the coral reef and inspect its lower sensor array.",
    "output": {
      "reasoning": "The red cylindrical instrument is directly in front of the robot, suspended vertically above the reef, with its lower sensor array (dark circular housing) visible at the bottom end. The robot is currently positioned at a moderate distance and slightly below the instrument’s midsection. To inspect the lower sensor array, I need to: (1) move forward to reduce distance, (2) adjust vertical position upward slightly to align with the lower array (since the array is at the bottom of the cylinder and the robot appears slightly below center), and (3) possibly fine-tune lateral position to center on the array. Given the clear frontal view and minimal lateral offset, a straight-forward approach followed by a slight upward correction should suffice. No obstacles block the direct path.",
      "action": [
        "Go straight",
        "Move up slightly",
        "Center on the lower sensor array"
      ]
    },
    "images": [
      "./data/images/temp_307_0078_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue corrugated pipe on the seabed and inspect its surface for damage or marine growth, while avoiding the floating net above it.",
    "output": {
      "reasoning": "The image shows a large blue corrugated pipe lying horizontally on the seabed. A floating net is suspended above it, partially draped over the pipe’s left section. To inspect the pipe’s surface while avoiding the net, the robot must approach from a safe angle—likely from the front or side where the net is not directly overhead. The pipe extends diagonally from lower-left to upper-right in the frame; the clearest access appears from the front (toward the camera’s current position), then moving laterally along its length. Since the net hangs loosely and casts shadows, maintaining a slight upward clearance (but below the net) or staying close to the seabed (where the net doesn’t touch) is safer. Step-by-step: (1) Advance straight toward the pipe’s near end (front), staying low to avoid net contact; (2) Once adjacent, move parallel to the pipe (rightward, following its orientation) to inspect surface; (3) Continue inspection along the visible segment, adjusting depth if needed to keep net above.",
      "action": [
        "Go straight toward the near end of the blue pipe",
        "Move right parallel to the pipe while maintaining close proximity to seabed",
        "Continue inspecting the pipe surface along its length"
      ]
    },
    "images": [
      "./data/images/temp_307_0065_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central bioluminescent coral structure and inspect its glowing polyps for signs of activity or damage.",
    "output": {
      "reasoning": "The central bioluminescent coral structure is clearly visible and dominates the image, positioned directly ahead of the robot’s current viewpoint. Its glowing polyps are active (emitting light), suggesting biological activity, but close inspection is needed to assess for subtle signs of damage (e.g., discoloration, retraction, or irregular luminescence). The surrounding environment appears stable with no immediate obstacles blocking a direct approach. To inspect effectively, the robot should first move forward to reduce distance, then stabilize and possibly adjust orientation slightly to center the structure in view for detailed observation.",
      "action": [
        "Go straight toward the central bioluminescent coral",
        "Slow down and hover at ~0.5 m distance",
        "Adjust pitch slightly upward to align camera with polyp layer",
        "Inspect glowing polyps for uniformity, brightness, and structural integrity"
      ]
    },
    "images": [
      "./data/images/temp_307_0092_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the bright central light source while maintaining a safe distance from the textured, undulating walls of the tunnel.",
    "output": {
      "reasoning": "The image shows a tunnel-like structure with a bright central light source directly ahead, indicating the target direction. The walls are textured and undulating (rippled green-blue patterns), suggesting potential collision risk if the robot drifts laterally. To maintain safety, the robot must stay centered while advancing. Since the light is aligned with the current heading, no initial turning is needed—only forward motion with minor lateral corrections if drift is detected. Given the symmetry and clear central axis, the safest strategy is to proceed straight while monitoring wall proximity.",
      "action": [
        "Go straight",
        "Maintain center alignment with the light source",
        "Adjust slightly left or right only if wall proximity decreases"
      ]
    },
    "images": [
      "./data/images/temp_307_0109_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the left while avoiding the large dark rock formation on the right.",
    "output": {
      "reasoning": "The yellow sponge is visible on the left side of the image, near the bottom-left corner. A large dark rock formation occupies the right side, extending vertically and partially blocking the right path. To approach the sponge while avoiding the rock, the robot should first steer left to increase lateral clearance from the rock, then proceed forward toward the sponge. Since the sponge appears relatively close and directly accessible once aligned, a two-step plan suffices: (1) turn left to orient toward the sponge and away from the rock, (2) go straight to reach it. No upward/downward adjustment is needed—the sponge is at similar depth as the robot’s current position.",
      "action": [
        "Turn left",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0077_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the textured green column in the center while avoiding the overgrown structure to the left.",
    "output": {
      "reasoning": "The image shows two prominent vertical structures: on the left, a heavily overgrown, irregular column with dense marine growth; in the center, a smoother, textured green column with horizontal ridges—this matches the target 'textured green column in the center'. The robot appears to be positioned slightly left of center, facing forward. To approach the central column while avoiding the overgrown structure on the left, the robot should first shift rightward to create clearance, then proceed straight toward the central column. A slight right turn followed by forward motion ensures safe navigation without colliding with the left obstacle.",
      "action": [
        "Turn right slightly",
        "Go straight toward the central green column"
      ]
    },
    "images": [
      "./data/images/temp_307_0063_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged green pipe and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The large submerged green pipe is clearly visible in the center-right of the image, lying horizontally on the seabed. The robot appears to be positioned slightly to the left and front of the pipe’s near end (open circular opening). To approach and inspect its surface, the robot should first move forward toward the pipe’s open end to get within close inspection range (~1–2 m), then proceed along its length while maintaining a parallel offset to visually scan for marine growth or damage. Since the pipe extends diagonally away from the camera, a slight rightward correction may be needed to align with its axis before moving alongside.",
      "action": [
        "Go straight toward the open end of the green pipe",
        "Turn slightly right to align parallel to the pipe",
        "Move alongside the pipe from near end to far end, maintaining ~0.5 m distance",
        "Pause briefly at intervals to inspect surface for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0199_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central dark columnar structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The central dark columnar structure is clearly visible and dominates the scene, positioned directly ahead of the robot. It appears to be within reachable distance, with no immediate obstacles blocking a direct approach. To inspect its surface for marine growth or damage, I need to first move closer to reduce distance, then position the robot parallel to the structure for optimal visual inspection. Given the lighting and visibility, approaching straight ahead is safe and efficient. After closing in, a slight lateral adjustment may be needed to align for detailed observation.",
      "action": [
        "Go straight toward the central columnar structure",
        "Stop at approximately 1 meter distance",
        "Adjust position to align parallel to the column's surface",
        "Inspect the surface visually for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0030_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small cylindrical device with red tether floating mid-water and inspect its status.",
    "output": {
      "reasoning": "The image shows a small cylindrical device floating mid-water, with a visible red tether attached—this matches the target described. The robot (likely the large fish-shaped AUV in the upper part of the frame) is currently positioned above and slightly behind the target. To approach and inspect, the robot should first descend slightly to align vertically with the device, then move forward (straight) toward it, and finally stabilize at close range for inspection. No immediate obstacles block the direct path. The lighting from above suggests good visibility along the descent and approach trajectory.",
      "action": [
        "Descend slightly",
        "Go straight toward the cylindrical device",
        "Hover in front of the device for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0274_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large brain coral in the center while maintaining a safe distance from the surrounding rocky outcrops and smaller coral formations.",
    "output": {
      "reasoning": "The large brain coral is centrally located and clearly visible. Surrounding it are smaller coral formations and rocky outcrops—especially to the left, right, and bottom of the frame. To approach safely, I must avoid lateral drift toward obstacles. The coral appears directly ahead at moderate distance, so a controlled forward advance with slight central alignment is appropriate. No immediate collision risk is present in the direct forward path, but I should maintain steady heading and avoid sharp turns. Given the depth and lighting, small adjustments may be needed mid-approach, but initial steps can be straightforward.",
      "action": [
        "Go straight slowly",
        "Maintain central heading toward the brain coral",
        "Slightly adjust left if nearing right-side outcrop",
        "Slightly adjust right if nearing left-side outcrop",
        "Stop at ~1 meter distance from the brain coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0176_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submersible vehicle positioned near the center-left of the frame, maintaining a safe distance while observing its surroundings.",
    "output": {
      "reasoning": "The green submersible vehicle is clearly visible in the center-left of the frame, suspended mid-water near a rocky outcrop. It is oriented horizontally, facing roughly toward the right. The robot appears to be positioned slightly behind and below it, with open water ahead and some coral/rock formations nearby but not directly obstructing a direct approach from the current position. To approach safely while observing its surroundings, I should first move forward cautiously to reduce distance without getting too close, then adjust laterally if needed to maintain optimal observation angle (e.g., slightly to the right to avoid occlusion by foreground rocks). Given the lighting and visibility, a slow, straight advance is safest initially.",
      "action": [
        "Go straight slowly",
        "Maintain slight rightward offset to keep vehicle in view",
        "Pause and observe surroundings"
      ]
    },
    "images": [
      "./data/images/temp_307_0187_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green pipeline along the seabed while maintaining a safe distance from the adjacent yellow-banded cable.",
    "output": {
      "reasoning": "The image shows a green pipeline running along the seabed on the left side of the frame, and a yellow-banded cable (likely a power or communication cable) running parallel to it on the right, elevated slightly on a ladder-like support structure. The robot is currently positioned such that both are visible in the forward view, with the green pipeline closer to the seabed and the yellow-banded cable to the right and slightly above. To follow the green pipeline while maintaining a safe distance from the yellow-banded cable, the robot should stay centered relative to the green pipeline—i.e., keep it on the left but not too close—and avoid drifting right toward the yellow cable. Given the current alignment, the immediate action is to proceed straight while subtly adjusting leftward to maintain optimal lateral offset (~0.5–1 m) from the green pipeline’s edge, ensuring the yellow cable remains clearly to the right without encroachment.",
      "action": [
        "Go straight",
        "Slightly adjust left to keep green pipeline on left side at safe distance",
        "Maintain heading parallel to green pipeline"
      ]
    },
    "images": [
      "./data/images/temp_307_0090_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ornate, tower-like structure in the center while navigating between the large coral formations on the left and right.",
    "output": {
      "reasoning": "The ornate, tower-like structure is centrally located in the image, illuminated by sunbeams from above. Large coral formations flank it: a prominent dark coral mass on the left (foreground/mid-left) and a textured, rounded coral formation on the right (mid-right). The robot appears to be positioned slightly behind and below the structure, with a clear navigable corridor between the two corals. To approach the structure safely, the robot should first align itself centrally—i.e., move forward while maintaining equal clearance from left and right corals. Minor course corrections may be needed: since the left coral appears closer and more protruding, a slight rightward bias might help avoid collision, but the primary action is forward motion toward the center. No immediate turns are required; the path is relatively straight ahead through the gap.",
      "action": [
        "Go straight",
        "Maintain central alignment between left and right coral formations",
        "Proceed toward the tower-like structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0283_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green cylindrical structure with circular ports and inspect its front opening for signs of marine growth or damage.",
    "output": {
      "reasoning": "The image shows a submerged green cylindrical structure with circular ports lying horizontally on the seabed, oriented roughly left-to-right from the robot’s current perspective. Its front opening (largest circular port) faces toward the right side of the frame and is partially illuminated by sun rays. The robot appears to be positioned slightly left and forward of the structure, at a moderate distance (~3–5 m). To inspect the front opening closely, the robot must: (1) move forward to reduce distance, (2) adjust lateral position to align directly with the front opening (which is on the right end of the cylinder), and (3) approach carefully to get a clear view without disturbing sediment. No immediate obstacles block the path, though coral is present on the left—so lateral movement should favor the right side to avoid contact.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Go straight",
        "Move closer to the front opening",
        "Hover and inspect the front opening"
      ]
    },
    "images": [
      "./data/images/temp_307_0269_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe structure on the left and inspect the circular end cap marked with the number '1'.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe structure on the left side of the frame, with multiple circular end caps visible. The end cap marked with the number '1' is clearly visible in the lower-left foreground, slightly angled but facing the camera. The robot is currently positioned at a moderate distance and slightly above the seabed. To approach and inspect the target end cap, the robot should first move forward toward the structure while maintaining alignment with the left-side pipe. As it gets closer, a slight downward and leftward adjustment may be needed to center the '1'-marked cap in the field of view for inspection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the left-side cylindrical pipe",
        "Slightly turn left to align with the end cap marked '1'",
        "Move forward slowly to get closer to the end cap",
        "Hover and stabilize in front of the end cap marked '1' for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0070_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central green pipeline structure and inspect the red indicator light near its midsection.",
    "output": {
      "reasoning": "The image shows a large central green pipeline structure extending vertically, with a visible red indicator light near its midsection (slightly below the center of the structure in the frame). The robot is currently positioned below and slightly in front of the structure, looking upward. To approach and inspect the red light, the robot must first move upward along the structure while maintaining alignment to avoid drifting laterally. Once near the midsection, a slight forward movement will bring the red light into close inspection range. No immediate obstacles block the path; however, due to the upward perspective and lighting from above, careful vertical control is needed to avoid overshooting.",
      "action": [
        "Move upward slowly while keeping the green pipeline centered in view",
        "Adjust position slightly forward when the red indicator light becomes clearly visible",
        "Hover steadily at the red indicator light for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0237_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the prominent white branching coral in the center-right while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The prominent white branching coral is clearly visible in the center-right of the image, illuminated by sun rays and standing out against darker rock formations and other corals. To approach it safely, I must avoid the surrounding rocks: there is a large dark rock formation to the left, and textured rocky structures below and to the right of the target coral. The robot appears to be positioned slightly left and somewhat distant from the coral. A safe approach path would involve moving forward while gently steering right—keeping enough lateral clearance from the left-side rock—and maintaining altitude to avoid bottom obstacles. Since the coral is at mid-depth and well-lit, a straight-forward + slight right correction should bring the robot into proximity without collision.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight",
        "Maintain steady depth and distance"
      ]
    },
    "images": [
      "./data/images/temp_307_0299_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green mechanical structure on the seabed and inspect its central cylindrical component.",
    "output": {
      "reasoning": "The large green mechanical structure is clearly visible in the center-right of the image, resting on the seabed. Its central cylindrical component (the vertical, ribbed cylinder with circular ports) is the inspection target. The robot is currently positioned at a moderate distance and slightly elevated (as seen from the downward-looking perspective). To approach and inspect the central cylinder, the robot should first descend gently to seabed level, then move forward toward the structure, and finally position itself directly in front of or slightly below the central cylinder for close inspection. No immediate obstacles block the direct path, though care should be taken around nearby sea urchins and coral-like formations.",
      "action": [
        "Descend slowly to seabed level",
        "Move forward toward the green mechanical structure",
        "Approach the central cylindrical component head-on",
        "Hover steadily in front of the central cylinder for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0003_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible and dominates the scene, positioned directly ahead of the robot's current viewpoint. Sunlight rays illuminate it from above, enhancing visibility. To inspect its surface for inscriptions or damage, I need to approach it closely while maintaining a stable orientation. Given its central position and lack of immediate obstacles in the forward path, the optimal plan is: (1) move straight toward the base of the column to reduce distance, (2) circle slightly around it (e.g., clockwise) to view multiple sides, and (3) get closer for detailed inspection—ideally positioning the camera parallel to the surface at mid-height where inscriptions are likely. No red obstacles are present; the surrounding smaller rocks are distant and non-blocking.",
      "action": [
        "Go straight toward the central stone column",
        "Circle clockwise around the column at close range",
        "Move closer to the column surface for detailed inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0292_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular metal structure on the left while avoiding the coral clusters on the seabed.",
    "output": {
      "reasoning": "The large circular metal structure is prominently visible on the left side of the image, extending vertically and partially embedded in the seabed. Coral clusters are scattered on the seabed in front and slightly to the right of the robot’s current position. To approach the structure while avoiding corals, the robot should first steer left to align with the structure’s axis, then proceed forward cautiously, maintaining clearance from the seabed corals by staying slightly elevated or hugging the left wall of the structure where corals are less dense. The path should avoid the central seabed region where coral density appears highest.",
      "action": [
        "Turn left slightly",
        "Move forward slowly while keeping the metal structure on the left",
        "Maintain altitude to avoid seabed coral clusters",
        "Continue approaching until close to the structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0048_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large dark rock formation on the right while maintaining a safe distance from the overhanging ledge above.",
    "output": {
      "reasoning": "The image shows a large dark rock formation on the right side of the frame, with an overhanging ledge above it casting shadow and potentially posing a collision risk. The robot is currently positioned to the left and slightly below the formation. To approach the rock safely: (1) First, move forward cautiously to close distance while staying clear of the overhead ledge—this requires maintaining depth (not ascending) and aligning laterally toward the rock’s base. (2) Then, gently turn right to align parallel to the rock face, keeping a buffer zone beneath the overhang. (3) Finally, proceed straight alongside the rock at a consistent safe distance (e.g., ~1–2 m), monitoring proximity to both the rock and the ledge. No immediate obstacles block the path, but lighting gradients suggest the ledge extends inward—so avoid upward motion.",
      "action": [
        "Go straight slowly",
        "Turn right slightly",
        "Go straight while maintaining lateral distance from the rock and avoiding ascent"
      ]
    },
    "images": [
      "./data/images/temp_307_0021_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green bioluminescent structure and inspect its base for signs of marine growth or damage.",
    "output": {
      "reasoning": "The glowing green bioluminescent structure is centrally located in the image, standing vertically on the seabed with a distinct base featuring multiple lobes. The robot is currently at a moderate distance, facing the structure head-on. To inspect the base for marine growth or damage, I need to: (1) approach the structure carefully to reduce distance without disturbing sediment; (2) position the robot close enough to resolve fine details at the base (likely requiring ~1–2 m range); (3) possibly adjust orientation to ensure optimal lighting and field-of-view of the base—since the glow originates from the upper part, the base may be shadowed, so a slight downward tilt or lateral shift may help. No immediate obstacles block the direct path. Therefore, the plan is: go straight toward the structure, then fine-tune position near the base, and finally stabilize for inspection.",
      "action": [
        "Go straight toward the glowing green structure",
        "Slow down as you near the base (within 2 meters)",
        "Slightly descend and/or tilt downward to focus on the base",
        "Hover steadily to inspect the base for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0111_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater structure and inspect its base for attachment points or damage.",
    "output": {
      "reasoning": "The red cylindrical underwater structure is clearly visible in the upper center of the image, suspended vertically with its base extending downward toward the seabed. The robot appears to be positioned slightly below and in front of the structure, with coral and marine life surrounding it. To approach the base for inspection, the robot should first move upward (toward the structure) and slightly forward to align with the cylinder’s axis, then descend carefully along its side to reach the base without disturbing sediment or obstacles. Since the base is likely near the seabed (partially obscured by coral), a controlled descent after alignment is necessary. No immediate obstacles block the direct path upward toward the cylinder.",
      "action": [
        "Move upward toward the red cylindrical structure",
        "Align horizontally with the cylinder's midsection",
        "Descend slowly along the cylinder's side toward its base",
        "Hover near the base and inspect for attachment points or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0181_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the weathered stone column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The weathered stone column is clearly visible in the center-right of the image, standing vertically on a submerged platform. It appears to be the primary target. The robot is currently at a moderate distance and slightly to the left-front of the column. To approach and inspect its surface, I need to: (1) move forward toward the column while maintaining alignment, (2) close the distance to allow detailed visual inspection (within ~1–2 meters), and (3) possibly adjust orientation to scan the full circumference or focus on areas with visible marine growth (e.g., green algae patches near base and mid-section). No immediate obstacles block the direct path, though some scattered debris lies to the left—avoidance is not urgent but should be monitored. Given the column’s height and vertical structure, a straight-in approach followed by a slight circling or lateral drift will enable comprehensive inspection.",
      "action": [
        "Go straight toward the stone column",
        "Slow down as you near the base (within 2 meters)",
        "Circle slightly around the column clockwise to inspect all sides",
        "Pause briefly at each quadrant to observe surface details"
      ]
    },
    "images": [
      "./data/images/temp_307_0230_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central bioluminescent structure with the green spiral tendril and pink ring, avoiding the surrounding red-lit nodes and fibrous filaments.",
    "output": {
      "reasoning": "The central bioluminescent structure is clearly visible: a dark spherical core with a pink ring and a prominent green spiral tendril extending upward. Surrounding it are red-lit nodes (two visible on the lower left and right of the core) and numerous fibrous green filaments radiating outward, forming a web-like obstacle field. The robot appears to be positioned slightly below and in front of the structure, with open water above and some filament clearance directly ahead. To approach safely: (1) First, ascend gently to align with the mid-height of the core and avoid entanglement with lower filaments; (2) Then move forward slowly along the central axis—where the green tendril provides a visual corridor—to minimize proximity to red nodes; (3) Finally, fine-tune lateral position to stay centered between the left and right red nodes, maintaining distance from all filaments. No sharp turns are needed since the target is directly ahead and vertically aligned.",
      "action": [
        "Ascend slightly",
        "Move forward slowly along the central axis",
        "Maintain center alignment between left and right red nodes"
      ]
    },
    "images": [
      "./data/images/temp_307_0152_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the arched rock opening toward the sunlit area ahead, avoiding the large boulders on the seabed.",
    "output": {
      "reasoning": "The image shows an arched rock opening directly ahead, with strong sunlit rays visible beyond it—indicating the target destination. The seabed contains several large boulders (especially on the left and right near the arch base) that must be avoided. To safely navigate through the arch: (1) First, align the robot centrally with the arch opening to ensure clearance; (2) Then proceed forward steadily while maintaining a safe distance from the boulders on both sides; (3) As the arch narrows slightly mid-passage, slight lateral adjustments may be needed—based on visual feedback—but the initial path should be straight ahead toward the brightest light. No sharp turns are required yet; the primary risk is collision with nearby boulders, so forward motion with centered alignment is optimal.",
      "action": [
        "Align center with the arched rock opening",
        "Go straight forward slowly",
        "Maintain central trajectory through the arch",
        "Proceed toward the sunlit area beyond"
      ]
    },
    "images": [
      "./data/images/temp_307_0008_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle on the seabed and inspect its front illuminated porthole.",
    "output": {
      "reasoning": "The image shows a submarine-like vehicle resting on the seabed, oriented horizontally with its front (illuminated porthole) facing roughly toward the right side of the frame. The robot appears to be positioned slightly left and behind the vehicle, with rocky obstacles in the immediate foreground. To approach the front illuminated porthole, the robot must first navigate past the left-side rocks, then move forward and slightly right to align with the vehicle’s front. The porthole is clearly visible and glowing, serving as the inspection target. A safe path exists along the open seabed to the right of the nearest rock. Therefore, the plan is: (1) turn right to clear the left obstacle and face the vehicle’s direction, (2) advance straight toward the vehicle while maintaining alignment with the front, (3) fine-tune position to center on the illuminated porthole for inspection.",
      "action": [
        "Turn right",
        "Go straight",
        "Move slightly right and forward to center on the illuminated porthole"
      ]
    },
    "images": [
      "./data/images/temp_307_0083_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral structure with the spiral interior and inspect its central cavity.",
    "output": {
      "reasoning": "The image shows a large green coral structure with a prominent spiral interior and a visible central cavity. The robot is currently positioned to the left and slightly in front of the structure, with the cavity facing roughly toward the robot's right side. To approach and inspect the central cavity, the robot should first move closer to the structure while aligning its heading toward the cavity entrance. Given the current perspective, a slight right turn will better orient the robot toward the cavity, followed by forward motion to get within inspection range. Once near, a minor adjustment (e.g., slight left or right) may be needed to center the cavity in view, but the primary steps are: turn right to face the cavity, then go straight toward it.",
      "action": [
        "Turn right",
        "Go straight",
        "Move closer to the central cavity"
      ]
    },
    "images": [
      "./data/images/temp_307_0257_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall perforated cylindrical structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The tall perforated cylindrical structure is clearly visible in the center-right of the image, standing vertically on the seabed. The robot appears to be positioned slightly left and at a moderate distance (~2–3 m) from it, with clear visibility and no immediate obstacles blocking the path. To approach and inspect its surface, I need to: (1) move forward toward the structure, (2) adjust lateral position to align parallel to its side for optimal inspection, and (3) perform a slow circumferential or close-range pass to examine surface features (e.g., marine growth, cracks, corrosion). Given the lighting and clarity, a direct forward motion followed by a slight right turn to get alongside is safe and efficient.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Turn slightly right to align parallel to the structure's side",
        "Move slowly alongside the structure to inspect its surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0156_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater structure with yellow and blue bands and inspect its top opening.",
    "output": {
      "reasoning": "The cylindrical structure with yellow and blue bands is clearly visible in the center-left of the image, upright on the seabed. Its top opening (with a blue cap and yellow ridged ring) is facing upward and partially illuminated by surface light. The robot appears to be positioned slightly below and to the side of the structure. To approach and inspect the top opening, the robot should first move forward toward the structure while maintaining a safe distance, then adjust laterally if needed to center itself beneath the opening, and finally ascend vertically to get a clear overhead view of the top opening.",
      "action": [
        "Go straight toward the cylindrical structure",
        "Slightly adjust left/right to center under the structure",
        "Ascend vertically to position above the top opening",
        "Hover and inspect the top opening"
      ]
    },
    "images": [
      "./data/images/temp_307_0281_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical vehicle emitting blue light and inspect its front hatch for signs of damage or open access.",
    "output": {
      "reasoning": "The submerged cylindrical vehicle is centered in the image, emitting blue light from its rear (bottom end), and oriented vertically with its front hatch likely at the top (nose) end, facing upward toward the camera's perspective. The robot appears to be positioned slightly below and in front of the vehicle, viewing it through an opening in a rocky structure. To inspect the front hatch, the robot must first close the distance while maintaining a stable approach angle. Since the vehicle is directly ahead but elevated, the optimal path is to move forward and slightly upward (or pitch up) to align with the front hatch. No immediate obstacles block the direct path, though surrounding rocks require careful lateral positioning to avoid collision. Given the monocular view, depth estimation suggests moderate proximity—so a controlled forward advance followed by fine positioning is appropriate.",
      "action": [
        "Go straight slowly",
        "Pitch up slightly",
        "Approach the front hatch of the cylindrical vehicle",
        "Hover and inspect the front hatch for damage or open access"
      ]
    },
    "images": [
      "./data/images/temp_307_0249_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car between the two large columns and inspect its front left wheel area for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a submerged car positioned between two large, column-like structures—one on the left (a multi-columned ruin) and one on the right (a single thick column). The car is angled slightly, with its front facing roughly toward the camera but tilted leftward; the front left wheel area is partially visible but not yet close enough for detailed inspection. To approach safely and directly, the robot should first move forward while maintaining central alignment between the columns to avoid collision. Once closer (~2–3 m), a slight leftward adjustment will position the robot optimally to inspect the front left wheel region. No immediate obstacles block the path, but marine growth on columns suggests caution near surfaces.",
      "action": [
        "Go straight forward between the two columns",
        "Approach the submerged car steadily",
        "Slightly turn left to align with the front left wheel area",
        "Hover and inspect the front left wheel for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0054_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the left while maintaining a safe distance from the smaller glowing corals in the foreground.",
    "output": {
      "reasoning": "The image shows a large textured coral formation on the left side of the frame, which is the target. In the foreground, there are several smaller corals emitting green glow—these are the obstacles to avoid. The robot is currently positioned such that the large coral is to its left and slightly ahead, while the glowing corals lie directly in front and slightly to the right. To approach the large coral safely: (1) First, move forward cautiously but steer slightly left to begin aligning toward the large coral while keeping distance from the foreground glowing corals; (2) Then, continue moving leftward (or turn left gently) to position the robot parallel to the large coral’s side, ensuring the glowing corals remain behind or to the right; (3) Finally, advance straight along the left side of the large coral at a safe lateral offset (e.g., ~1–2 m) to maintain proximity without collision.",
      "action": [
        "Move forward while gently turning left",
        "Continue turning left until aligned with the left side of the large coral",
        "Proceed straight alongside the large coral at a safe distance"
      ]
    },
    "images": [
      "./data/images/temp_307_0271_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing blue-green circular structure on the seabed while avoiding the larger bioluminescent organism above it.",
    "output": {
      "reasoning": "The image shows a glowing blue-green circular structure on the seabed (foreground, lower right), which is the target. Above it, there is a larger bioluminescent organism with multiple glowing green spots and a bulbous, textured body (mid-to-upper center), which must be avoided. The robot appears to be positioned slightly left and possibly elevated relative to the target. To approach the circular structure safely: (1) First, move forward cautiously while maintaining a safe vertical distance below the large organism; (2) Since the target is to the right of current view, a slight rightward correction is needed; (3) Then proceed straight toward the structure, keeping depth shallow enough to stay under the obstacle but high enough to avoid seabed contact. No sharp turns are needed—gentle steering suffices due to clear lateral separation.",
      "action": [
        "Move forward slowly",
        "Turn slightly right",
        "Continue forward toward the glowing blue-green circular structure on the seabed",
        "Maintain depth below the large bioluminescent organism"
      ]
    },
    "images": [
      "./data/images/temp_307_0272_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green dome-shaped device emitting a green light and inspect its top cylindrical component.",
    "output": {
      "reasoning": "The green dome-shaped device emitting a green light is clearly visible in the foreground, slightly to the right of center. Its top cylindrical component (a blue-tinted cap) is upright and accessible. The robot appears to be positioned at a moderate distance (~1.5–2 m) and slightly elevated relative to the device. To inspect the top cylindrical component, the robot must first approach it directly while maintaining stable depth and orientation, then position itself directly above or slightly in front to get a clear view of the top. No immediate obstacles block the direct path; the surrounding rocks are to the left and rear. Therefore, the plan is: (1) move straight toward the device, (2) fine-tune position to center on the device, (3) ascend slightly if needed to align视线 with the top component, (4) hold position for inspection.",
      "action": [
        "Go straight toward the green dome-shaped device",
        "Slow down and center the device in the field of view",
        "Ascend slightly to align with the top cylindrical component",
        "Hold position and inspect the top cylindrical component"
      ]
    },
    "images": [
      "./data/images/temp_307_0263_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange and green submersible device resting on the seabed near the rocky outcrop.",
    "output": {
      "reasoning": "The orange and green submersible device is clearly visible on the seabed, positioned slightly left of center in the image, near a large rocky outcrop to its right. The robot appears to be facing it at a moderate distance (~2–3 m), with clear visibility and no immediate obstacles in the direct path. To approach it safely, the robot should first move straight forward toward the device, then possibly adjust laterally if needed to center itself relative to the target. Since the device is stationary and well-lit, no avoidance maneuvers are required at this stage.",
      "action": [
        "Go straight",
        "Approach the orange and green submersible device"
      ]
    },
    "images": [
      "./data/images/temp_307_0234_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the carved stone column at the center of the cave while avoiding the rocky outcrops on the left and right.",
    "output": {
      "reasoning": "The carved stone column is centrally located and illuminated by light from above, making it the clear target. Rocky outcrops are visible on both left and right sides—left side shows a large irregular formation, right side has a cluster of boulders near the column’s base. To avoid collisions, the robot must maintain a central trajectory. Given the current perspective (robot appears to be at cave entrance or mid-chamber), the safest path is to proceed straight while slightly favoring the centerline, adjusting as needed to keep equal clearance from both sides. No immediate turn is required since the column is directly ahead and unobstructed in the central corridor.",
      "action": [
        "Go straight",
        "Maintain central alignment",
        "Proceed toward the illuminated column"
      ]
    },
    "images": [
      "./data/images/temp_307_0276_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the right while avoiding the large coral formations on the left and center.",
    "output": {
      "reasoning": "The robot is currently oriented forward, with large coral formations occupying the left and central regions of the scene. A yellow sponge is visible on the right side, partially illuminated by light rays. To approach it safely, the robot must first steer right to avoid the corals on the left and center, then proceed forward toward the sponge. Since the sponge appears at a moderate distance and slightly ahead-right, a controlled right turn followed by straight motion should align the robot with the target while maintaining clearance from obstacles.",
      "action": [
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0050_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green pipe while avoiding the surrounding coral formations and small fish.",
    "output": {
      "reasoning": "The illuminated green pipe is vertically oriented and centrally located in the image, extending from near the bottom toward the top. It is surrounded by coral formations on both left and right sides (e.g., large fan-like coral on left, dense branching coral on lower-left, and dark reef structure on right), and numerous small fish are scattered throughout the water column—especially near the pipe’s midsection and around the corals. To approach safely: (1) The robot should first align directly with the pipe’s axis to minimize lateral deviation; (2) Since the pipe is well-lit and prominent, maintaining forward motion while keeping a safe clearance (~0.5–1 m) from visible coral edges is feasible; (3) Small fish are highly mobile and unlikely to obstruct if the robot moves steadily without sudden maneuvers; (4) Given the pipe extends upward, approaching from below (i.e., moving upward along its length) is natural and avoids entanglement with seabed corals. Therefore, the plan is: center on the pipe, move straight ahead (upward in the image frame), and slightly adjust laterally only if needed to maintain clearance—but initial alignment suggests no immediate turn is required.",
      "action": [
        "Center on the illuminated green pipe",
        "Move straight forward (upward) along the pipe's axis",
        "Maintain slight leftward offset to avoid right-side coral",
        "Proceed steadily while monitoring for fish proximity"
      ]
    },
    "images": [
      "./data/images/temp_307_0150_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central green cylindrical structure with multiple circular openings and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The central green cylindrical structure with multiple circular openings is clearly visible in the image, positioned slightly ahead and centered in the field of view. It is surrounded by netting and other debris, but a clear approach path exists from the current robot position. To inspect its surface for marine growth or damage, I need to: (1) move forward toward the structure while maintaining alignment with its center; (2) close in to a safe inspection distance (~1–2 m) without colliding with surrounding nets; (3) possibly adjust orientation to ensure full surface visibility (e.g., slight yaw if needed). Given the current perspective, the most direct action is to advance straight while monitoring proximity to obstacles.",
      "action": [
        "Go straight toward the central green cylindrical structure",
        "Slow down as you near the structure (within 2 meters)",
        "Hover and stabilize at ~1.5 meters distance",
        "Perform slow clockwise rotation to inspect the full surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0231_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object with green circular feature on the seabed and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The red dome-shaped object with a green circular feature is clearly visible on the seabed in the foreground, slightly to the right of center. The robot appears to be positioned at a moderate distance (~1–2 m) and slightly elevated. To inspect its surface for markings or damage, I need to: (1) approach it carefully to reduce distance without disturbing sediment; (2) position the robot directly in front or slightly above for optimal visual coverage; (3) stabilize and possibly adjust orientation to ensure the green feature and surface details are fully in view. No immediate obstacles block the path, but surrounding rocks suggest cautious forward motion.",
      "action": [
        "Go straight slowly toward the red dome-shaped object",
        "Move slightly left to center the object in the field of view",
        "Descend gently to ~0.5 m above the seabed",
        "Hover and stabilize in front of the object",
        "Rotate slightly clockwise to inspect the green circular feature and adjacent surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0060_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater robot with the red fin and inspect its front sensor array.",
    "output": {
      "reasoning": "The green underwater robot with a red fin is visible in the center-left of the image, oriented roughly toward the camera. Its front sensor array (a cylindrical protrusion with a white tip) is facing forward and slightly downward. The robot is positioned on the seabed among rocks, with clear space in front of it. To inspect its front sensor array, I need to approach from the front—i.e., move straight toward it while maintaining a safe distance, then possibly adjust position to align directly with the sensor for close inspection. No immediate obstacles block the direct path; the main challenge is avoiding contact with surrounding rocks. Given the current perspective, the robot appears ~2–3 meters ahead and slightly to the left. So first, I’ll correct slight lateral offset by moving right, then go straight toward it, and finally fine-tune position to face the sensor array head-on.",
      "action": [
        "Move slightly right",
        "Go straight toward the green robot",
        "Stop at ~1 meter distance in front of the robot",
        "Adjust position to center the front sensor array in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0194_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated submarine at the cave entrance while avoiding the rocky outcrops on the seafloor.",
    "output": {
      "reasoning": "The illuminated submarine is visible at the cave entrance, centered in the image and slightly ahead. Rocky outcrops are present on the seafloor—particularly to the left and right of the current path—and also along the cave walls. To safely approach, the robot must maintain a central trajectory, avoiding lateral deviations toward the rocks. The submarine appears at moderate distance; thus, a sequence of forward movements with minor course corrections is needed. Since the cave opening frames the target, staying aligned with the light beam (which originates from the submarine) helps ensure safe passage. No immediate sharp turns are required—just steady forward motion with slight adjustments to stay clear of near-field rocks.",
      "action": [
        "Go straight slowly",
        "Slightly adjust right to avoid left-side rocks",
        "Continue straight toward the submarine's lights",
        "Maintain center alignment with the cave opening",
        "Approach the submarine steadily"
      ]
    },
    "images": [
      "./data/images/temp_307_0108_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submarine from behind while maintaining a safe distance and aligning with its central axis.",
    "output": {
      "reasoning": "The image shows a large submarine oriented vertically, with its stern (rear) facing the camera. A smaller autonomous underwater vehicle (AUV) is positioned in front of and slightly below the submarine, angled toward it. To approach from behind while maintaining a safe distance and aligning with the central axis, the robot must first reposition to the rear hemisphere of the submarine. Since the current view shows the submarine’s bow (front), the robot is currently in front — thus, it needs to maneuver around to the back. Given the submarine’s vertical orientation and the presence of seabed terrain on the right, the safest path is to ascend slightly, then move laterally to the left (port side) and circle behind, avoiding the rock formation on the right. Once behind, fine adjustments are needed to center on the central axis and maintain safe standoff distance. The immediate next step is to turn left and ascend gently to begin repositioning.",
      "action": [
        "Turn left",
        "Ascend slightly",
        "Move backward while maintaining left offset",
        "Re-center behind the submarine",
        "Maintain safe distance along central axis"
      ]
    },
    "images": [
      "./data/images/temp_307_0241_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient stone column and inspect its surface for inscriptions or damage, avoiding the surrounding coral formations.",
    "output": {
      "reasoning": "The image shows a tall ancient stone column centered in the frame, with coral formations on both left and right sides near its base. The robot appears to be positioned slightly in front and below the column, with clear water ahead but potential obstruction from corals if approaching directly laterally. To safely approach while avoiding corals: (1) First, move forward cautiously along the central axis—this path appears clearest based on lighting and absence of nearby coral in the direct frontal view. (2) Once closer (within ~2 m), adjust laterally—slightly to the left or right—to align for inspection, choosing the side with less dense coral (left side has smaller, more spaced corals; right side has denser clusters). (3) Then perform a slow circular or lateral scan at close range to inspect surface details without contact. Given the instruction emphasizes *avoiding* coral, the initial approach must stay centered, then fine-tune position before inspection.",
      "action": [
        "Go straight toward the column center",
        "Slightly turn left to maintain safe distance from right-side coral",
        "Slowly advance until 1 meter from column base",
        "Orbit clockwise around column at 1-meter radius to inspect surface",
        "Pause and zoom in visually on areas with texture anomalies"
      ]
    },
    "images": [
      "./data/images/temp_307_0088_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged concrete structure and inspect its underside for marine growth or damage.",
    "output": {
      "reasoning": "The submerged concrete structure is directly in front of the robot, elevated above the seabed on supports. Its underside is partially visible but not fully accessible from the current position. To inspect the underside for marine growth or damage, the robot must first move closer to reduce distance and improve resolution, then maneuver underneath—likely by moving forward while slightly descending or adjusting pitch. Given the open space beneath and no immediate obstacles blocking access, a safe approach path exists. The plan: (1) advance toward the structure to get within inspection range, (2) position directly beneath the overhang, (3) stabilize and orient camera upward for detailed visual inspection.",
      "action": [
        "Go straight toward the concrete structure",
        "Descend slightly while continuing forward to pass under the structure",
        "Position centrally beneath the structure's underside",
        "Tilt camera upward and hold position for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0183_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure covered in marine growth and inspect its central opening.",
    "output": {
      "reasoning": "The large green cylindrical structure is centrally located in the image, with its central opening clearly visible and facing the camera. The robot appears to be at a moderate distance, slightly below and in front of the structure. To approach and inspect the central opening, the robot should first move forward to reduce distance, then fine-tune position to center the opening in the field of view. No immediate obstacles block the direct path; marine growth is on the structure itself, not obstructing access. Given the lighting and visibility, a straight approach is safe and optimal.",
      "action": [
        "Go straight toward the green cylindrical structure",
        "Slow down as you near the central opening",
        "Position yourself directly in front of the central opening for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0189_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge on the right while avoiding the dark pipe-like structure above and the spiky coral formations below.",
    "output": {
      "reasoning": "From the image, the yellow sponge is visible on the upper right side of the frame, partially illuminated by sun rays. A dark pipe-like structure (likely a cable or tube) runs diagonally across the upper portion of the scene, above the robot’s current position. Spiky coral formations are present along the bottom right and near the center-right, below the robot. The robot is currently oriented roughly forward-left, with its nose pointing toward the lower-left quadrant. To approach the yellow sponge on the right while avoiding both the overhead pipe and the bottom corals, the robot must: (1) first ascend slightly to clear the spiky corals below, (2) then turn right to align toward the sponge, (3) proceed forward cautiously while maintaining vertical clearance from the pipe above and horizontal clearance from corals below. Given the sponge is elevated and to the right, a gentle upward-right trajectory is optimal.",
      "action": [
        "Ascend slightly",
        "Turn right",
        "Go forward while maintaining safe altitude above corals and below pipe"
      ]
    },
    "images": [
      "./data/images/temp_307_0138_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall dark sponge structure rising from the seabed while avoiding the surrounding yellow cup corals and rocky outcrops.",
    "output": {
      "reasoning": "The image shows a tall, dark, vertically oriented sponge structure centered in the frame, rising from the seabed. Surrounding it are yellow cup corals (especially to the right and lower right) and various rocky outcrops (left and background). The robot appears to be positioned slightly below and in front of the sponge, with clear water ahead but obstacles laterally. To approach safely: (1) Maintain forward motion toward the sponge’s base while keeping central alignment; (2) Slightly adjust left or right as needed to avoid encroaching on yellow cup corals—given their concentration on the right, a slight leftward bias may help; (3) Monitor proximity to avoid collision with nearby rocks. Since the sponge is directly ahead and dominant in view, the primary action is controlled forward advance with minor lateral correction.",
      "action": [
        "Go straight toward the dark sponge",
        "Slightly veer left to avoid yellow cup corals on the right",
        "Continue approaching until close to the sponge base"
      ]
    },
    "images": [
      "./data/images/temp_307_0218_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green plate coral in the center while avoiding the surrounding purple branching corals.",
    "output": {
      "reasoning": "The large green plate coral is clearly visible in the center of the image, characterized by its flat, layered, circular structure and bright green color. Surrounding it are several purple branching corals (e.g., upper right and lower left), which must be avoided. The robot appears to be positioned slightly below and in front of the target coral, with open water space directly ahead toward the central plate coral. To approach safely: (1) Maintain forward motion along a central trajectory to minimize proximity to purple corals on either side; (2) Slightly adjust heading to keep equidistant from left and right purple obstacles; (3) As the coral grows larger in view, reduce speed and fine-tune position to center on the plate coral without contacting adjacent structures. No immediate turn is needed—initial movement should be straight with minor lateral correction if drift is detected.",
      "action": [
        "Go straight",
        "Slightly adjust left to maintain clearance from right-side purple coral",
        "Continue straight while centering on green plate coral",
        "Slow down as coral fills more of the field of view"
      ]
    },
    "images": [
      "./data/images/temp_307_0121_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical structure on the left while avoiding the suspended green vegetation and the small debris on the seabed.",
    "output": {
      "reasoning": "From the image, the red cylindrical structure is on the left side of the frame, partially visible and lying on the seabed. Suspended green vegetation hangs vertically near the center, slightly right of the red structure. Small debris (e.g., rectangular object and fragments) is scattered on the seabed in front and to the right of the robot’s current position. To approach the red structure safely: (1) First, move forward cautiously while staying left of the suspended vegetation to avoid entanglement; (2) Then, adjust slightly leftward to align with the red cylinder; (3) Finally, proceed straight toward it, ensuring the path avoids the seabed debris—especially the rectangular object ahead and to the right. The robot should maintain a low altitude to minimize disturbance but stay above the debris field.",
      "action": [
        "Go straight while keeping left of the hanging green vegetation",
        "Slightly turn left to align with the red cylindrical structure",
        "Proceed straight toward the red cylindrical structure, avoiding seabed debris"
      ]
    },
    "images": [
      "./data/images/temp_307_0291_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central cluster of three glowing green sea anemones inside the circular metal frame structure.",
    "output": {
      "reasoning": "The image shows a circular metal frame structure enclosing three glowing green sea anemones arranged centrally within it. The robot is positioned outside the frame, facing the opening. Sunlight penetrates from above, illuminating the scene and highlighting the anemones at the center. To approach the central cluster, the robot must first enter the circular frame through the visible archway (front-facing opening), then proceed straight toward the illuminated anemones, which are clearly centered and slightly forward inside the structure. No immediate obstacles block the direct path; minor adjustment may be needed to stay centered as it advances.",
      "action": [
        "Enter the circular metal frame through the front opening",
        "Go straight toward the glowing green anemones",
        "Maintain center alignment while advancing"
      ]
    },
    "images": [
      "./data/images/temp_307_0232_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The submerged stone column is directly in front of the robot, centered in the field of view, and appears accessible without immediate obstacles. Sunlight rays indicate relatively clear water ahead. To inspect its surface, the robot must first approach closely (within ~1–2 m) to resolve fine details like marine growth or cracks. Given its current distance (estimated >3 m based on scale), a straight-forward advance is appropriate. Once near, a slow circular or lateral movement will allow full surface inspection—but the initial step is to close the distance safely.",
      "action": [
        "Go straight toward the stone column",
        "Slow down as you near the column (within 2 meters)",
        "Orbit slowly around the column at close range to inspect surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0247_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle positioned centrally between the rocky outcrops and inspect its front hatch.",
    "output": {
      "reasoning": "The image shows a submarine-like vehicle centered between two rocky outcrops, oriented vertically with its front (hatch side) facing the camera. The robot is currently at a moderate distance and slightly below the vehicle’s midline. To approach and inspect the front hatch, I need to: (1) move forward toward the vehicle while maintaining alignment with its central axis; (2) adjust vertical position upward slightly to align with the hatch (which appears near the upper front section); (3) stop at close range for inspection. No immediate obstacles block the direct path, but careful forward motion is needed to avoid contact with rocks or the vehicle itself.",
      "action": [
        "Go straight toward the submarine-like vehicle",
        "Ascend slightly to align with the front hatch",
        "Stop at close proximity to inspect the front hatch"
      ]
    },
    "images": [
      "./data/images/temp_307_0148_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and white striped buoy anchored on the seabed, maintaining a safe distance from the surrounding coral and rocks.",
    "output": {
      "reasoning": "The red and white striped buoy is centrally located in the image, anchored on the seabed. Coral formations are visible to the left and right of the buoy, and a large rock is on the right side. To approach safely, the robot should first align directly toward the buoy while maintaining a central path to avoid the nearby coral and rock. Given the clear frontal view and sufficient clearance ahead, the safest initial action is to move straight forward at a moderate pace, monitoring proximity to obstacles. As it gets closer, fine adjustments can be made—but for now, the primary step is forward motion along the open corridor.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0071_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green and blue segmented underwater vehicle resting near the coral formation and inspect its front sensor housing.",
    "output": {
      "reasoning": "The green and blue segmented underwater vehicle is visible in the center-right of the image, resting near coral formations. Its front sensor housing (dark circular feature) is on the leftmost end of the vehicle, facing roughly toward the camera but slightly angled downward. The robot is currently at a moderate distance and slightly above/behind the vehicle. To approach and inspect the front sensor housing, I need to: (1) move forward cautiously to reduce distance while maintaining clear view; (2) adjust heading slightly left to align with the vehicle’s longitudinal axis; (3) fine-tune position to bring the front sensor housing into close inspection range without disturbing sediment or coral. No immediate obstacles block the path, but care must be taken near the coral.",
      "action": [
        "Go straight slowly",
        "Turn slightly left",
        "Approach the front of the vehicle",
        "Hover in front of the sensor housing for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0178_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, segmented green structure in the center while avoiding the surrounding brain coral formations.",
    "output": {
      "reasoning": "The image shows a tall, segmented green structure (likely a coral or artificial pillar) centered in the frame. Surrounding it are brain coral formations—bulbous, grooved structures—on both left and right sides, posing potential collision risks. The robot is currently positioned at a moderate distance directly facing the target. To safely approach, I should first ensure alignment with the central structure, then advance while maintaining clearance from the flanking corals. Since the path appears relatively clear straight ahead (no immediate obstacles in the central corridor), a direct forward movement is feasible, but must be cautious and incremental to avoid drifting into side corals. No need to turn initially—the structure is already centered.",
      "action": [
        "Go straight slowly",
        "Maintain center alignment with the green structure",
        "Monitor left and right for encroaching brain coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0175_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green netted structure on the left and inspect its attachment point to the seabed.",
    "output": {
      "reasoning": "The large green netted structure is clearly visible on the left side of the image, partially anchored to the seabed. Its attachment point appears near the lower-left portion of the frame, where the net converges toward a cylindrical base resting on the seafloor. To inspect the attachment point, I need to: (1) move closer to the structure while maintaining a safe distance to avoid entanglement; (2) reposition to get a clear view of the base and its connection to the seabed; (3) possibly descend slightly to align the camera with the attachment zone. The current perspective shows the structure at an angle, so a slight leftward approach followed by forward motion and minor descent will optimize visibility.",
      "action": [
        "Turn left slightly",
        "Go straight toward the green netted structure",
        "Descend gently to align with the base of the structure",
        "Move closer to inspect the attachment point on the seabed"
      ]
    },
    "images": [
      "./data/images/temp_307_0251_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing coral-like structure mounted on the illuminated base and inspect its surface texture and embedded red lights.",
    "output": {
      "reasoning": "The glowing coral-like structure is centrally located in the image, mounted on a bright cyan illuminated base. It is clearly visible and within reasonable proximity for approach. The robot appears to be facing it directly, with no immediate obstacles in the forward path. To inspect surface texture and embedded red lights, the robot must first move closer (approach), then possibly adjust orientation for optimal viewing—ideally maintaining a stable position at close range (~0.5–1 m) to resolve fine details. Given the lighting and clarity, a straight-forward approach followed by a slight hover/position stabilization is sufficient; no turning is needed initially since the target is centered.",
      "action": [
        "Go straight toward the coral structure",
        "Slow down as you near the base",
        "Hover steadily at ~0.6 meters distance",
        "Tilt camera slightly downward to inspect upper surface texture",
        "Pan slowly around the structure to view embedded red lights"
      ]
    },
    "images": [
      "./data/images/temp_307_0300_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red submersible vehicle ahead while maintaining a safe distance from the large coral formation on the right.",
    "output": {
      "reasoning": "The red submersible vehicle is directly ahead and slightly elevated in the water column. A large coral formation occupies the right side of the scene, posing a collision risk if the robot drifts right. To approach safely, the robot should maintain central or slight-left heading to keep clearance from the coral. Since the target is ahead, the primary motion is forward, but minor leftward correction may be needed initially to center the path relative to the coral. No immediate turn is required—just straight advance with awareness of right-side obstacle.",
      "action": [
        "Go straight",
        "Slightly adjust left to maintain distance from coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0154_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green cylindrical device suspended by cables and inspect its lower attachment points.",
    "output": {
      "reasoning": "The illuminated green cylindrical device is centrally located in the upper portion of the image, suspended by multiple thin cables extending downward. The robot appears to be positioned below and slightly to the left of the device, with a complex cable/net structure in the foreground obstructing direct access. To inspect the lower attachment points (where cables connect to the device’s base), I must: (1) move forward cautiously to close distance while avoiding entanglement with foreground cables; (2) adjust lateral position to center under the device for optimal viewing angle; (3) ascend slightly to bring the lower attachment region into clear view; (4) stabilize and hover to enable detailed inspection. The green illumination helps visibility, but turbidity and cable density require precise, incremental movements.",
      "action": [
        "Go straight slowly",
        "Move right slightly",
        "Ascend gently",
        "Hover and inspect lower attachment points"
      ]
    },
    "images": [
      "./data/images/temp_307_0288_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-and-black striped coral structure with white nodules, maintaining a safe distance to avoid contact while observing its texture and surrounding marine life.",
    "output": {
      "reasoning": "The target is the large green-and-black striped coral structure with prominent white nodules, centrally located in the image and illuminated from above. It is surrounded by other corals (blue branching, yellow, and beige), but no immediate obstacles block a direct approach. To observe texture and surrounding marine life safely, I should first align the robot directly toward the structure, then advance slowly while maintaining a safe standoff distance—ensuring the structure remains centered in view and not too close to cause contact. Given its size and central position, a straight-forward approach with minor fine-tuning is optimal. No turning is needed initially since the structure is already in front.",
      "action": [
        "Go straight slowly",
        "Maintain center alignment with the coral structure",
        "Pause briefly to observe texture and surrounding marine life"
      ]
    },
    "images": [
      "./data/images/temp_307_0207_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange sea anemone in the foreground while maintaining a safe distance from the surrounding coral formations.",
    "output": {
      "reasoning": "The large orange sea anemone is clearly visible in the foreground, centered slightly to the lower-left of the image. The robot (submersible) is positioned above and behind it, with several coral formations (green ridged corals and brown spherical ones) surrounding the anemone—especially to its right and rear. To approach safely, the robot should first descend gently to align horizontally with the anemone’s depth, then move forward while slightly adjusting left to stay centered on the target and avoid the nearby green coral to the right. Maintaining a safe distance means avoiding lateral drift into adjacent corals; thus, small corrective turns are needed before advancing straight.",
      "action": [
        "Descend slightly",
        "Turn left slightly",
        "Go straight slowly",
        "Maintain steady heading toward the orange anemone"
      ]
    },
    "images": [
      "./data/images/temp_307_0164_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the left green columnar structure and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows two prominent green columnar structures; the left one is closer and more clearly visible, with circular openings and surface texture suggesting possible biofouling or damage. The robot is positioned slightly in front and to the right of the left column, with clear water path toward it. To approach and inspect its surface, I need to: (1) move leftward to align with the column’s midline, (2) advance forward to get within close inspection range (~1–2 m), and (3) possibly rotate slightly to scan the full circumference—though initial inspection can start from current frontal angle. No immediate obstacles block the path. Lighting from above creates shadows, so maintaining stable heading is important.",
      "action": [
        "Move left",
        "Go straight",
        "Hover and slowly rotate right to inspect surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0064_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the two ancient stone columns and inspect their structural integrity, focusing on visible cracks and marine growth on the right column.",
    "output": {
      "reasoning": "The image shows two ancient stone columns standing side by side, with the right column visibly covered in marine growth (algae/barnacles) and exhibiting apparent cracks—especially in its middle and lower sections. The robot is currently positioned at a moderate distance, facing both columns head-on. To inspect structural integrity as instructed, I must first approach the pair to get within close inspection range (~1–2 m), then reposition to focus on the right column’s surface details. Since the right column is the priority for crack and biofouling assessment, after initial approach, a slight rightward lateral movement will align the camera parallel to that column for optimal visual access. No immediate obstacles are present; the seabed appears clear.",
      "action": [
        "Go straight toward the two columns",
        "Stop when ~1.5 meters away",
        "Move slightly right to position alongside the right column",
        "Inspect the right column’s surface for cracks and marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0210_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red conical buoy floating above the coral reef and inspect its underside for attached sensors or fouling organisms.",
    "output": {
      "reasoning": "The red conical buoy is visible in the upper center of the image, floating above a coral reef. It is positioned slightly ahead and above the robot’s current depth. To inspect its underside, the robot must first ascend vertically to reach the buoy’s level, then approach horizontally from below to position itself directly beneath it. The coral structures are dense but there appears to be a clear vertical corridor near the center-left; thus, ascending straight while maintaining slight leftward offset avoids immediate collision with nearby coral. Once at buoy height, a short forward movement will bring the robot under the buoy, enabling visual inspection of its base (where sensors or fouling are likely attached).",
      "action": [
        "Ascend vertically while maintaining slight leftward offset to avoid nearby coral",
        "Move forward slowly toward the red conical buoy",
        "Position directly beneath the buoy’s base",
        "Hover and inspect the underside for sensors or fouling organisms"
      ]
    },
    "images": [
      "./data/images/temp_307_0142_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange-banded sea pen on the right while avoiding the spiky sea urchins in the foreground.",
    "output": {
      "reasoning": "The large orange-banded sea pen is clearly visible on the right side of the image, standing vertically and illuminated by sun rays. In the foreground, several spiky sea urchins (with radial spines) are clustered near the seabed, particularly on the left and center-bottom. To approach the sea pen safely, the robot must avoid these urchins. Since the urchins occupy the lower-left and central foreground, the safest path is to move forward while slightly veering right—keeping distance from the urchins—and then proceed directly toward the sea pen once clear of obstacles. The sea pen is relatively close, so only a few controlled steps are needed.",
      "action": [
        "Turn slightly right",
        "Move forward slowly",
        "Approach the orange-banded sea pen"
      ]
    },
    "images": [
      "./data/images/temp_307_0225_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car between the two stone pillars and inspect its front right side for damage or debris.",
    "output": {
      "reasoning": "The green car is positioned centrally between two stone pillars, slightly angled with its front right side facing the viewer. From the current perspective, the robot appears to be at a moderate distance in front of the car, with clear visibility of the target area. To inspect the front right side, the robot must first approach the car directly, then maneuver to position itself closer to that specific quadrant—ideally from the front-right diagonal to get an optimal inspection angle without occlusion. No immediate obstacles block the path, though scattered rocks on the seabed require cautious forward motion. The light beam from above illuminates the scene well, aiding navigation.",
      "action": [
        "Go straight toward the green car",
        "Turn slightly right to align with the front right side of the car",
        "Move forward slowly to get closer to the front right wheel and bumper area",
        "Hover and inspect the front right side for damage or debris"
      ]
    },
    "images": [
      "./data/images/temp_307_0073_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red dome-shaped object on the seabed while avoiding the large rocks on either side.",
    "output": {
      "reasoning": "The red dome-shaped object is clearly visible on the seabed, centered in the lower part of the image. Two large rocks flank it: one on the left (larger, textured) and one on the right (partially visible, also large). The robot appears to be positioned slightly above and behind the target, with a clear forward path toward the red dome if it stays centered between the two rocks. To avoid collision, the robot should maintain a central trajectory—slightly adjusting left or right only as needed to preserve safe clearance. Since the dome is directly ahead and unobstructed in the midline, the optimal plan is: (1) descend gently toward the seabed while aligning with the dome’s center, (2) proceed straight forward, keeping equal distance from both side rocks, (3) stop upon reaching close proximity to the dome. No sharp turns are needed; minor course corrections may be applied during descent if drift occurs.",
      "action": [
        "Descend slowly while centering on the red dome",
        "Go straight forward, maintaining equal distance from left and right rocks",
        "Stop when near the red dome-shaped object"
      ]
    },
    "images": [
      "./data/images/temp_307_0042_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spherical coral structure and inspect its surface texture and openings.",
    "output": {
      "reasoning": "The large spherical coral structure is clearly visible in the center-right of the image, resting on the sandy seabed. It is relatively close, but to properly inspect its surface texture and openings (e.g., the prominent cavity on its left side), the robot needs to approach closer and position itself for optimal viewing. Given the current perspective, moving forward slightly will reduce distance, then a slight leftward adjustment will align the robot with the main opening and textured surface for detailed inspection. No immediate obstacles block the path.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0031_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the large green pipeline along the seabed while maintaining a safe distance from the nearby yellow buoy and structural support.",
    "output": {
      "reasoning": "The image shows a large green pipeline extending diagonally from the lower-left toward the upper-right, lying on the seabed. A yellow buoy is visible near the top-center, slightly above and to the right of the pipeline’s near section. A structural support (likely a platform base) is in the background left. To follow the pipeline while maintaining safe distance: (1) The robot should initially align parallel to the pipeline’s direction—i.e., move forward while keeping the pipeline on its right or left side consistently; (2) The yellow buoy is above and ahead—so as the robot advances, it must avoid drifting upward/right toward the buoy; (3) The structural support is behind and to the left, not an immediate hazard. Given the pipeline curves gently upward/right, the safest strategy is to stay slightly left of the pipeline (keeping it on the robot’s right), moving forward while subtly adjusting rightward to track the curve, but avoiding encroaching on the buoy’s vicinity. No sharp turns needed—just steady forward motion with minor course corrections.",
      "action": [
        "Go straight while keeping the green pipeline on your right",
        "Slightly turn right to follow the pipeline's gentle curve",
        "Maintain distance from the yellow buoy above and ahead",
        "Continue following the pipeline along the seabed"
      ]
    },
    "images": [
      "./data/images/temp_307_0307_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the leaking pipe joint emitting blue-tinted bubbles and inspect the seal for damage.",
    "output": {
      "reasoning": "The image shows a large underwater pipe with a visible joint emitting a plume of blue-tinted bubbles—indicating a leak. The leak is located toward the left-middle of the frame, slightly elevated and upstream along the pipe axis. The robot appears to be positioned downstream (right side of image), facing the pipe. To approach the leaking joint for inspection, the robot should first move forward along the pipe’s axis, then adjust laterally if needed to center on the joint. Given the clear visibility and unobstructed path, a direct forward motion followed by fine positioning is optimal. The red-banded joints are reference points; the leaking one is the second joint from the left, distinguished by the bubble plume.",
      "action": [
        "Go straight toward the pipe",
        "Move closer to the leaking joint emitting blue bubbles",
        "Position directly in front of the joint for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0120_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the right while avoiding the dark rocky debris on the left.",
    "output": {
      "reasoning": "The image shows a large, textured coral formation on the right side of the frame, with distinct ridged patterns and occupying a significant portion of the right half. On the left, there is dark, irregular rocky debris scattered near the seabed. The robot is currently facing forward, with the coral slightly ahead and to the right, and the debris to the left and slightly behind. To approach the coral while avoiding the debris, the robot should first steer right to align toward the coral, then proceed forward cautiously, maintaining distance from the left-side debris. A slight right turn followed by straight movement is sufficient given the current orientation and spacing.",
      "action": [
        "Turn right slightly",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0295_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible lying on the seabed and inspect its side portholes for damage or marine growth.",
    "output": {
      "reasoning": "The green cylindrical submersible is visible on the seabed, oriented horizontally, with its side portholes facing the camera. The robot appears to be positioned slightly in front and above it, with some rocky obstacles nearby but a clear approach path from the current viewpoint. To inspect the side portholes, the robot must first move closer laterally (to align parallel to the submersible’s side) and then proceed along its length at close range. Given the current perspective, the optimal first step is to move forward slightly to reduce distance, then adjust left/right to position alongside the mid-section of the cylinder where portholes are visible. Since the submersible lies horizontally and the portholes are on its side, a 'go straight' followed by 'turn right' or 'left' may be needed depending on relative offset — but from the image, the robot is slightly to the right of the submersible’s centerline, so moving forward and then gently turning left will align it parallel to the side. After alignment, proceed slowly along the side for inspection.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight alongside the submersible",
        "Pause and inspect side portholes"
      ]
    },
    "images": [
      "./data/images/temp_307_0147_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large rusted cylindrical pipe structure and inspect its interior opening for obstructions or marine growth.",
    "output": {
      "reasoning": "The large rusted cylindrical pipe is directly in front of the robot, angled slightly to the right. Its open end faces the camera, and the interior is partially visible but obscured by shadow and distance. To inspect the interior opening for obstructions or marine growth, I need to: (1) move forward cautiously to reduce distance while maintaining alignment with the pipe’s axis; (2) adjust orientation slightly if needed to center the opening in the field of view; (3) get close enough for high-resolution inspection—ideally within 1–2 meters—without colliding with the rim or protruding bolts. The seabed appears stable, and no immediate obstacles block the approach path.",
      "action": [
        "Go straight slowly toward the pipe opening",
        "Slightly adjust left to center the pipe opening in view",
        "Continue forward until ~1.5 meters from the pipe entrance",
        "Hover and stabilize for visual inspection of interior"
      ]
    },
    "images": [
      "./data/images/temp_307_0262_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue pipeline and inspect the yellow-banded joint near the coral formation.",
    "output": {
      "reasoning": "The image shows a large blue pipeline extending diagonally from the lower right toward the upper left, resting on the seabed near coral formations. A yellow-banded joint is visible mid-pipeline, adjacent to a prominent coral cluster (left foreground). The robot appears to be positioned slightly in front and to the left of the pipeline, with clear visibility of the target joint. To approach the yellow-banded joint for inspection, the robot should first move forward toward the pipeline, then adjust laterally (right) to align with the joint, and finally close in for a detailed view without disturbing the coral.",
      "action": [
        "Go straight toward the blue pipeline",
        "Turn slightly right to align with the yellow-banded joint",
        "Move forward slowly to inspect the joint closely"
      ]
    },
    "images": [
      "./data/images/temp_307_0306_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue pickup truck and inspect its front bumper for damage or attached objects.",
    "output": {
      "reasoning": "The blue pickup truck is visible in the lower-left portion of the image, resting on the seabed. It is oriented with its front facing roughly toward the camera (slightly angled right). Sunlight penetrates from above, illuminating the scene and highlighting the truck’s front bumper area. To inspect the front bumper for damage or attached objects, I need to approach it directly while maintaining a stable, close-range viewing position. The path appears clear of immediate obstacles between the robot’s current position (assumed near center-top due to fisheye distortion) and the truck. First, I should descend slightly and move forward toward the truck; then, fine-tune positioning to align with the front bumper at close range for detailed inspection.",
      "action": [
        "Descend 1 meter",
        "Move forward toward the blue pickup truck",
        "Approach until 2 meters from the front bumper",
        "Hover and stabilize facing the front bumper",
        "Inspect the front bumper for damage or attached objects"
      ]
    },
    "images": [
      "./data/images/temp_307_0201_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue cylindrical device with red top and yellow panels, then inspect its front-facing sensors while avoiding the nearby vertical textured structures.",
    "output": {
      "reasoning": "The blue cylindrical device with a red top and yellow panels is centrally located in the image, illuminated by a light source above. It is flanked on both sides by tall vertical textured structures (likely obstacles to avoid). The robot appears to be positioned slightly below and in front of the device. To approach it safely while avoiding the vertical structures, the robot should first move forward cautiously to close distance, then adjust laterally (slightly left or right) only if needed to maintain clearance—however, since the central path appears open and the device is directly ahead, a straight approach is feasible. Once near, the robot must orient to face the front of the device to inspect its sensors; this likely requires a slight upward tilt and frontal alignment. No immediate collision risk is visible in the direct forward path, so the priority is controlled forward motion followed by precise positioning for inspection.",
      "action": [
        "Go straight slowly toward the blue cylindrical device",
        "Maintain central alignment to avoid left and right vertical structures",
        "Stop at ~1 meter from the device",
        "Tilt upward slightly and center camera on front-facing sensors",
        "Hold position for sensor inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0222_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car parked beside the moss-covered concrete pillar and inspect its driver-side door for damage.",
    "output": {
      "reasoning": "The green car is visible to the right of the moss-covered concrete pillar, parked parallel to it. The driver-side door (left side of the car from our perspective, since the car faces roughly toward the camera) is partially visible but not yet close enough for detailed inspection. To approach it safely and align for inspection, I should first move forward toward the car while staying slightly left of the pillar to avoid collision. Once near the car’s midsection, I will reposition laterally to face the driver-side door directly, then move closer for inspection.",
      "action": [
        "Go straight toward the green car, staying left of the concrete pillar",
        "Turn slightly left to align with the driver-side door",
        "Move forward slowly to inspect the driver-side door"
      ]
    },
    "images": [
      "./data/images/temp_307_0082_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-lit underwater vehicle positioned centrally among the coral formations.",
    "output": {
      "reasoning": "The yellow-lit underwater vehicle is clearly visible and centered in the image, surrounded by coral formations on both sides and slightly ahead. Sunlight rays indicate the vehicle is at moderate depth and well-illuminated. The robot appears to be facing it directly, with a clear forward path—no immediate obstacles block the direct approach. To safely approach, the robot should maintain central alignment and advance steadily while monitoring proximity to nearby corals.",
      "action": [
        "Go straight",
        "Maintain center alignment",
        "Approach slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0046_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical underwater vehicle marked with the number 9 and inspect its side panel for damage or debris.",
    "output": {
      "reasoning": "The large green cylindrical underwater vehicle marked with '9' is visible in the center-right of the image, oriented horizontally. The robot appears to be positioned slightly behind and to the left of the vehicle, with clear water ahead. To approach and inspect its side panel (likely the port or starboard side facing the camera), I need to: (1) move forward toward the vehicle while maintaining safe distance, (2) adjust lateral position to align parallel to the side panel, and (3) close in for detailed inspection. Given the vehicle’s orientation and current perspective, moving straight ahead first brings us closer; then a slight right turn positions us alongside the mid-section of the vehicle where the side panel is accessible. No immediate obstacles block the path.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Go straight slowly",
        "Hover parallel to the side panel of the green vehicle"
      ]
    },
    "images": [
      "./data/images/temp_307_0117_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large yellow-green striped coral formation on the left while maintaining a safe distance from the surrounding smaller corals and fish.",
    "output": {
      "reasoning": "The large yellow-green striped coral formation is prominently visible on the left side of the image, with its vertical ridges and bright coloration distinguishing it from surrounding elements. Smaller corals (e.g., purple and green bushy types) and fish are scattered nearby, especially in the mid- and foreground, requiring careful navigation. To approach safely: first, orient toward the left while avoiding immediate foreground obstacles (e.g., the large curved yellow-green structure in the lower-left corner); then proceed forward at a moderate pace, maintaining lateral clearance from smaller corals; finally, adjust heading slightly to center the target formation in view without closing too quickly. The robot should avoid sudden turns or forward surges to prevent disturbing fish or contacting fragile corals.",
      "action": [
        "Turn left gently by ~15 degrees",
        "Move forward slowly for 2 meters",
        "Maintain slight leftward bias to keep target coral centered",
        "Pause briefly to assess proximity to smaller corals",
        "Continue forward at reduced speed until 1.5 meters from target"
      ]
    },
    "images": [
      "./data/images/temp_307_0161_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone structure with columns and inspect the central opening for potential entry or artifacts.",
    "output": {
      "reasoning": "The image shows a submerged stone structure with two prominent columns and a central opening, located ahead and slightly to the right of the robot's current position. A blue vehicle is visible on the left, but it is not relevant to the task. The goal is to approach the structure and inspect the central opening. Given the clear path forward and slight rightward offset of the structure, the optimal plan is: (1) move forward toward the structure while maintaining alignment with the central opening, (2) adjust slightly right if needed to center the opening in view, (3) continue forward until close enough for detailed inspection (e.g., within 2–3 meters), and (4) stabilize and orient camera toward the opening for inspection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the stone structure",
        "Slightly turn right to align with the central opening",
        "Continue forward until near the entrance",
        "Hover and focus camera on the central opening for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0185_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged vertical structure with the cross-shaped top while avoiding the large green kelp fronds in the foreground.",
    "output": {
      "reasoning": "The target is the submerged vertical structure with a distinct cross-shaped top, located centrally in the background. Large green kelp fronds dominate the foreground and left side, obstructing direct forward movement. To avoid them, I must first steer right to clear the dense kelp on the left and center, then proceed straight toward the structure once a clear path is established. The structure appears reachable in ~3–4 meters; after clearing kelp, a slight left correction may be needed to center on it, but initial priority is rightward evasion.",
      "action": [
        "Turn right",
        "Go straight",
        "Go straight",
        "Slightly turn left",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0268_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red spherical object suspended on a vertical pole and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The red spherical object is clearly visible, suspended centrally on a vertical pole, with sunlight rays highlighting its position. The robot (visible as a dark silhouette in the lower-left foreground) is currently at a moderate distance and slightly to the left and below the target. To approach for inspection, the robot should first move forward toward the object while maintaining alignment with the pole. As it gets closer, a slight rightward correction may be needed to center the sphere in the field of view, followed by a slow upward movement to bring the camera face-on to the sphere’s surface for detailed inspection. No immediate obstacles block the direct path, though the large red structure on the right suggests caution to avoid drifting too far right.",
      "action": [
        "Go straight toward the red spherical object",
        "Slightly turn right to center the sphere in view",
        "Move upward slowly to align camera with the sphere's equator",
        "Hold position and inspect the surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0273_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central bioluminescent coral structure and gently inspect its upper frilled layers while avoiding contact with the surrounding yellow-tipped polyps.",
    "output": {
      "reasoning": "The image shows a central bioluminescent coral structure with intricate upper frilled layers (pink-green ridges) and surrounding yellow-tipped polyps arranged radially at the base. The robot is currently positioned at a moderate distance, facing the structure head-on. To safely approach and inspect the upper frills without contacting the yellow-tipped polyps: (1) First, move forward slowly to reduce distance while maintaining central alignment; (2) Then, ascend slightly to position the camera/viewpoint level with the upper frilled layers; (3) Finally, perform a gentle lateral drift (e.g., slight left or right) to get an oblique view of the frills—ensuring the robot’s body remains clear of the protruding yellow polyps below. Given symmetry, a small rightward drift is safe and provides optimal viewing angle of the frills’ texture.",
      "action": [
        "Go straight slowly",
        "Ascend slightly",
        "Drift right gently"
      ]
    },
    "images": [
      "./data/images/temp_307_0091_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue and white mesh-covered vertical pipe structure and inspect its base where it connects to the seafloor.",
    "output": {
      "reasoning": "The blue and white mesh-covered vertical pipe structure is clearly visible in the center-right of the image, standing upright with its base resting on the seafloor (green seabed visible at bottom). The robot appears to be positioned slightly left and front of the structure, at a moderate distance (~2–3 m). To inspect the base where it connects to the seafloor, the robot must: (1) move forward toward the structure, (2) adjust laterally (slightly right) to align with the base’s centerline, and (3) descend gently to get a close-up view of the junction. No immediate obstacles block the path; the horizontal pipe behind is at a safe distance. Lighting from above creates glare but does not obscure the target.",
      "action": [
        "Go straight toward the vertical pipe structure",
        "Turn slightly right to center on the base",
        "Descend slowly to inspect the pipe-seafloor connection"
      ]
    },
    "images": [
      "./data/images/temp_307_0129_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical structure with the vertical pipe and inspect its base near the orange crate-like objects.",
    "output": {
      "reasoning": "The green cylindrical structure with a vertical pipe is clearly visible in the center-right of the image. Orange crate-like objects are positioned to its left and slightly in front (from the robot’s perspective). The robot appears to be at a moderate distance, with a clear path toward the structure. To inspect the base near the orange crates, I need to: (1) move forward toward the green structure, (2) adjust laterally (left) to align with the base region adjacent to the orange crates, and (3) descend slightly if needed to get a closer view of the base—though current depth seems adequate. No immediate obstacles block the path. The lighting is sufficient for visual inspection.",
      "action": [
        "Go straight toward the green cylindrical structure",
        "Turn left slightly to align with the base near the orange crates",
        "Move forward slowly to inspect the base area"
      ]
    },
    "images": [
      "./data/images/temp_307_0159_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral formation while maintaining a safe distance from the yellow fish swimming above it.",
    "output": {
      "reasoning": "The large green ridged coral formation is the primary target, centrally located and prominent in the image. A yellow fish is swimming directly above it, slightly to the right of the coral’s peak. To approach safely, I must avoid ascending toward the fish and instead maintain a lower trajectory—likely slightly below or beside the coral’s midsection. The robot appears to be positioned at a moderate distance in front of the coral. Given the current perspective, the safest approach path is to move forward while slightly angling left (away from the fish’s position) and keeping depth stable or slightly descending to stay beneath the fish’s swim path.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Maintain current depth or descend gently"
      ]
    },
    "images": [
      "./data/images/temp_307_0134_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green orb on the seabed while avoiding the large carved stone structures on either side.",
    "output": {
      "reasoning": "The glowing green orb is visible on the seabed in the lower-right portion of the image, illuminated by a beam of light. Two large carved stone structures flank the central path: one on the left (closer, with facial features) and one on the right (taller, partially in shadow). The orb lies between them but slightly right-of-center. To approach it safely while avoiding collision, the robot should first center itself in the open corridor between the stones, then proceed forward while maintaining clearance—slightly favoring the left side to avoid the right stone’s protrusions, as the orb is closer to the right but still accessible from a central-left trajectory. The seabed appears relatively flat ahead, so no vertical adjustment is needed yet.",
      "action": [
        "Turn slightly left to align with the central passage between the two stone structures",
        "Go straight slowly while keeping equal distance from both stones",
        "Adjust right slightly as the orb comes into closer view to position directly in front of it",
        "Descend gently toward the seabed to reach the orb"
      ]
    },
    "images": [
      "./data/images/temp_307_0275_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large central sponge-like structure and inspect its central cavity for signs of marine life or sediment accumulation.",
    "output": {
      "reasoning": "The image shows a large, central sponge-like structure with a prominent central cavity directly ahead of the robot's current viewpoint. The structure is well-lit and centered in the frame, indicating it is within close proximity and aligned with the robot’s forward direction. To inspect the central cavity for marine life or sediment, the robot must first approach closer to ensure high-resolution observation, then position itself to peer into the cavity—likely requiring a slight upward or forward tilt and stabilization. No immediate obstacles block the direct path, though surrounding rock formations suggest cautious forward motion is needed to avoid collision. Step-by-step: (1) Advance slowly toward the structure to reduce distance; (2) Fine-tune position to center the cavity in view; (3) Hover and stabilize for inspection.",
      "action": [
        "Go straight slowly",
        "Adjust position to center the cavity in view",
        "Hover and stabilize for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0132_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange submersible vehicle resting on the seabed and inspect its front viewport and surrounding structure for damage or marine growth.",
    "output": {
      "reasoning": "The orange submersible vehicle is directly in front of the robot, resting on the seabed. Its front viewport is clearly visible and centered in the frame. To inspect the front viewport and surrounding structure, I need to approach carefully without disturbing sediment. Given its proximity and frontal orientation, the optimal sequence is: (1) move forward slowly to get closer for detailed inspection; (2) once near, perform a slight upward tilt or hover to align the camera with the viewport center; (3) optionally execute a slow clockwise rotation to survey the surrounding hull structure (left side first, then front-right). No obstacles block the direct path, but bubbles above suggest recent movement—so proceed steadily to avoid turbulence.",
      "action": [
        "Go straight slowly",
        "Hover and adjust pitch slightly upward",
        "Rotate clockwise 30 degrees while maintaining position",
        "Inspect front viewport and hull for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0204_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged pickup truck and inspect its front right wheel area for damage or entanglement.",
    "output": {
      "reasoning": "The submerged pickup truck is clearly visible in the center of the image, oriented diagonally with its front facing slightly left and its right side more exposed to the camera. The front right wheel area is partially visible but not yet close enough for detailed inspection—it appears to be near the foreground yellow cylindrical structures (possibly pipes or buoys), which may pose proximity constraints. To inspect the front right wheel, I need to: (1) approach the vehicle from the front-right quadrant to minimize obstruction; (2) maintain safe distance from surrounding objects; (3) position the robot directly in front of and slightly to the right of the truck to get a clear view of the wheel well and tire. Given the current perspective, moving forward and slightly right will align the robot with the target zone.",
      "action": [
        "Move forward slowly",
        "Turn slightly right",
        "Approach the front right wheel area of the truck",
        "Hover steadily in front of the front right wheel for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0058_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-encrusted cylindrical structure on the left and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a large green-encrusted cylindrical structure prominently on the left side of the frame, consistent with the target described. It appears to be a vertical piling or column covered in algae/biofouling, with visible surface irregularities (holes, erosion) suggesting potential marine growth or damage. The robot is currently at a moderate distance and slightly angled away from it. To inspect its surface closely, I need to: (1) reorient toward the structure (it's on the left, so turn left), (2) advance straight toward it while maintaining safe proximity, and (3) position for close visual inspection—likely requiring a slight upward or lateral adjustment to scan the surface systematically. Given the clear visibility and lack of immediate obstacles, a direct approach is feasible.",
      "action": [
        "Turn left",
        "Go straight toward the green cylindrical structure",
        "Move closer until within 1 meter of the structure",
        "Hover and slowly ascend while scanning the surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0303_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted green car and inspect its open driver-side door for potential artifacts or structural damage.",
    "output": {
      "reasoning": "The rusted green car is clearly visible in the center-left of the image, resting on the seabed with its driver-side door open. The robot appears to be positioned slightly in front and to the right of the vehicle. To approach the open driver-side door for inspection, the robot should first move forward toward the car, then adjust leftward to align with the open door. Given the relatively clear path and absence of immediate obstacles near the target area, a direct approach is feasible. After reaching proximity, a slight turn may be needed to orient the camera toward the interior of the door frame for detailed inspection.",
      "action": [
        "Go straight toward the car",
        "Turn left slightly to align with the open driver-side door",
        "Move forward slowly to get closer to the door opening",
        "Hover and inspect the interior edge and frame of the open door"
      ]
    },
    "images": [
      "./data/images/temp_307_0020_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green coral structure from the front-left while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The large green coral structure is centrally located in the image, with its front-left side visible and accessible. The robot appears to be positioned slightly to the right and behind the coral (based on shadow direction and perspective). To approach from the front-left while maintaining a safe distance, I need to: (1) first reposition leftward to align with the front-left quadrant, (2) then advance cautiously toward it without drifting into contact—keeping a buffer of ~0.5–1 m (visually estimated from scale of surrounding seabed texture), and (3) possibly fine-tune orientation to face the target squarely upon arrival. No immediate obstacles block the path; the seabed is clear and flat.",
      "action": [
        "Move left",
        "Go forward slowly",
        "Adjust heading slightly left to face the front-left side of the coral",
        "Maintain steady distance while approaching"
      ]
    },
    "images": [
      "./data/images/temp_307_0098_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-eyed robotic fish in the center while maintaining a safe distance from the surrounding coral formations.",
    "output": {
      "reasoning": "The yellow-eyed robotic fish is clearly centered in the image, slightly above the seabed and illuminated by light rays from above. Coral formations are present on both left and right sides, as well as in the foreground (bottom), indicating constrained lateral maneuverability. To approach safely: (1) First, ensure forward motion is aligned with the fish’s position—no immediate turn needed since it's already centered. (2) Maintain a moderate distance to avoid contact with nearby corals; given the fish is elevated, staying slightly below and centered minimizes collision risk. (3) Proceed slowly and steadily forward while monitoring proximity to corals on either side. No sharp turns required at this stage.",
      "action": [
        "Go straight slowly",
        "Maintain center alignment with the robotic fish",
        "Monitor left and right coral proximity and adjust laterally if needed"
      ]
    },
    "images": [
      "./data/images/temp_307_0055_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall ribbed cylindrical structure in the center while avoiding the surrounding coral formations and scattered debris.",
    "output": {
      "reasoning": "The tall ribbed cylindrical structure is clearly centered in the image, illuminated by overhead light rays. It is surrounded by coral formations (to the right and lower-left) and scattered debris (e.g., disc-shaped object in lower-left foreground). To approach safely: (1) First, move forward cautiously to close distance while maintaining central alignment with the cylinder; (2) Observe that the left side has a large disc-shaped debris and dense coral, while the right side has coral but appears slightly more open near the base—however, since the cylinder is centered and the path directly ahead is relatively clear (sunlight beams suggest open water column), the safest initial maneuver is straight advance; (3) As we get closer, minor lateral adjustments may be needed, but the first atomic step is to go straight toward the center target. No immediate turn is required because the direct path is unobstructed at current range.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0191_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the bright central light source while maintaining alignment with the concentric green structural rings.",
    "output": {
      "reasoning": "The image shows a bright central light source surrounded by concentric green structural rings, forming a tunnel-like structure. The robot (visible on the left) is currently offset to the left and slightly angled relative to the central axis. To approach the light while maintaining alignment with the rings, the robot must first correct its lateral position to center itself within the ring structure, then proceed straight toward the light. Since the rings are symmetric and visually guide the central axis, minor rightward correction followed by forward motion will preserve alignment. No obstacles block the path directly ahead.",
      "action": [
        "Turn slightly right to center within the green rings",
        "Go straight toward the bright central light"
      ]
    },
    "images": [
      "./data/images/temp_307_0149_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green segmented cylindrical object lying on the seabed and inspect its front end for markings or damage.",
    "output": {
      "reasoning": "The large green segmented cylindrical object is clearly visible on the seabed, oriented horizontally with its front end (narrower, capped end) facing toward the right side of the image. The robot appears to be positioned slightly left and rearward relative to the object. To approach and inspect the front end, the robot should first move forward to reduce distance, then adjust laterally (right) to align with the front axis, and finally move closer for detailed inspection. No immediate obstacles block the path; coral formations are present but leave a clear corridor to the object’s front. Lighting from above creates shadows, but the front end is well-illuminated and accessible.",
      "action": [
        "Go straight toward the green cylindrical object",
        "Turn right to align with the front end of the object",
        "Move forward slowly to inspect the front end for markings or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0097_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall, textured vertical rock formation in the center while avoiding the surrounding coral clusters and maintaining a safe distance from the yellow fish on the right.",
    "output": {
      "reasoning": "The target is the tall, textured vertical rock formation centered in the image. Coral clusters are visible to the left and right of the rock, requiring lateral clearance. A yellow fish is present on the right side, near the coral, so the robot should avoid approaching too closely from that direction. The safest approach path is slightly from the left-front quadrant—maintaining distance from both the right-side coral and the yellow fish—while moving straight toward the rock's base. Initial small correction: slight left turn to align away from the right-side hazards, then proceed forward steadily.",
      "action": [
        "Turn slightly left",
        "Go straight toward the central rock formation",
        "Maintain steady forward motion while keeping the yellow fish and right-side coral on the right edge of view"
      ]
    },
    "images": [
      "./data/images/temp_307_0096_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green net structure ahead while avoiding entanglement with the surrounding suspended fishing nets.",
    "output": {
      "reasoning": "The image shows a complex underwater scene dominated by suspended fishing nets. A brightly illuminated green net structure is visible directly ahead, slightly elevated and centered in the field of view. Surrounding it are numerous loose, tangled nets forming potential entanglement hazards—especially on the left and right flanks, and some strands crossing the foreground. To safely approach the target, the robot must maintain a central, slightly elevated trajectory to avoid lower hanging strands, while carefully monitoring lateral clearance. The illumination suggests the green structure is the intended goal. Given the density of nets, small, controlled movements are essential: first stabilize heading toward the center of the green structure, then advance cautiously while adjusting pitch to stay above lower net layers.",
      "action": [
        "Stabilize heading toward the center of the illuminated green net structure",
        "Gently ascend 0.5 meters to clear lower net strands",
        "Advance straight forward slowly (0.3 m/s) for 2 meters",
        "Scan left and right for net proximity; if within 0.8 m, adjust laterally outward by 0.2 m",
        "Continue advancing toward the green structure while maintaining 1.0 m clearance from surrounding nets"
      ]
    },
    "images": [
      "./data/images/temp_307_0252_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the circular metallic object suspended in the water column while avoiding the coral formations on the seabed.",
    "output": {
      "reasoning": "The circular metallic object is suspended in the water column, centered and slightly above mid-frame, illuminated by sun rays. Coral formations are prominent on the seabed (bottom-left and bottom-right), posing a risk if the robot descends too low or drifts laterally toward them. To approach safely: (1) Maintain neutral buoyancy and ascend slightly to clear the coral canopy; (2) Navigate forward while keeping the target centered; (3) Adjust laterally only if needed to stay clear of corals—currently, the central path appears open. The safest initial action is to go straight while maintaining depth just above the highest coral peaks.",
      "action": [
        "Ascend slightly",
        "Go straight toward the circular metallic object",
        "Maintain center alignment and monitor coral proximity"
      ]
    },
    "images": [
      "./data/images/temp_307_0162_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large ribbed coral structure in the center while avoiding the spiky green coral to its left.",
    "output": {
      "reasoning": "The image shows a large ribbed coral structure in the center (target), with a spiky green coral to its left (obstacle). The robot is positioned slightly in front and below the target. To approach the central ribbed coral while avoiding the spiky green coral on the left, the robot should first move slightly right to clear the obstacle’s lateral proximity, then proceed straight toward the center. A small rightward correction ensures safe clearance before advancing.",
      "action": [
        "Turn slightly right",
        "Go straight toward the large ribbed coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0127_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall central column covered in green algae while avoiding the surrounding rocky outcrops.",
    "output": {
      "reasoning": "The image shows a tall central column covered in green algae, clearly the target. It is surrounded by rocky outcrops on the left, right, and rear—especially prominent on the left (large yellow-green rock) and right (dark vertical rock). The robot appears to be positioned slightly in front and possibly offset to the left of the column. To approach safely while avoiding obstacles: (1) First, reposition slightly rightward to clear the left-side rock and align with the column’s centerline; (2) Then proceed straight forward toward the column, maintaining central alignment; (3) As it nears, fine-tune to stay centered and avoid any protrusions on the column base or nearby smaller rocks. No immediate upward/downward motion is needed—the column is fully visible at current depth.",
      "action": [
        "Turn right slightly",
        "Go straight toward the central column",
        "Adjust position to stay centered on the column while approaching"
      ]
    },
    "images": [
      "./data/images/temp_307_0095_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical pipe with green end caps and inspect its surface for damage or biofouling.",
    "output": {
      "reasoning": "The image shows a large cylindrical pipe with green end caps positioned diagonally across the field of view, extending from left-mid to right-rear. The robot appears to be slightly below and to the left of the pipe’s main body. To approach for inspection, the robot should first move forward toward the nearest visible section (left green cap), then adjust laterally to align parallel to the pipe axis, and finally proceed along its length for surface inspection. Given the pipe’s orientation and proximity, the initial action is to go straight toward the left green end cap while maintaining safe distance to avoid collision with surrounding structures (e.g., the vertical pipe junction on the right).",
      "action": [
        "Go straight toward the left green end cap",
        "Adjust right slightly to align parallel to the pipe axis",
        "Move forward slowly along the pipe surface",
        "Inspect the pipe surface for damage or biofouling"
      ]
    },
    "images": [
      "./data/images/temp_307_0130_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of glowing green and orange cup-shaped corals while maintaining a safe distance from the surrounding dark rock formations.",
    "output": {
      "reasoning": "The image shows a dense cluster of glowing green and orange cup-shaped corals centered in the frame, with dark rock formations surrounding them—particularly on the left and right peripheries. The robot appears to be positioned slightly above and in front of the coral cluster, with clear visibility toward the target. To approach safely: (1) First, descend slightly to align horizontally with the coral cluster (they appear slightly below current viewpoint due to lighting and perspective). (2) Then, move forward slowly while monitoring proximity to dark rocks—since rocks are lateral, maintaining central heading should keep safe distance. (3) Stop when within optimal observation range (~1–2 m), avoiding contact with any coral or rock edges. No sharp turns needed; straight-line approach is feasible given open frontal space.",
      "action": [
        "Descend slightly",
        "Move forward slowly",
        "Maintain central heading",
        "Stop at safe observation distance"
      ]
    },
    "images": [
      "./data/images/temp_307_0122_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure on the left with the circular openings and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The image shows an underwater scene with several green cylindrical structures featuring circular openings and textured surfaces. The largest such structure is positioned on the left side of the frame, partially illuminated by sun rays from above. It appears to be the target described in the instruction. To approach it, the robot should first move toward the left while maintaining forward progress, then close in for inspection. Since the structure is already visible and relatively nearby, a short sequence of directional movements suffices: turn left to align, go straight toward it, and then move closer for surface inspection.",
      "action": [
        "Turn left",
        "Go straight",
        "Move closer to the large green cylindrical structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0004_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red ribbed lantern on the seabed while avoiding the nearby striped sea cucumbers.",
    "output": {
      "reasoning": "The red ribbed lantern is the primary target, located centrally on the seabed and well-lit by sun rays. Two striped sea cucumbers are visible: one to the left of the lantern (partially in frame), and another to the right and slightly behind the lantern. To approach safely while avoiding both, I should first move forward cautiously toward the lantern’s general direction, then adjust laterally—preferably to the right side of the lantern (since the right-side cucumber is farther back and more visible, allowing better clearance) or left if the left cucumber is less obstructive. Given the lantern is directly ahead and the left cucumber is closer to the robot’s current position (left edge of image), a safer initial path is to advance straight, then veer slightly right to pass between the lantern and the right cucumber, keeping distance from both. The seabed appears relatively flat and clear in the central corridor, supporting this path.",
      "action": [
        "Go straight slowly",
        "Turn slightly right",
        "Advance toward the red lantern",
        "Maintain distance from striped sea cucumbers on both sides"
      ]
    },
    "images": [
      "./data/images/temp_307_0023_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large purple coral formation on the left while maintaining a safe distance from the submarine above.",
    "output": {
      "reasoning": "The image shows a submarine positioned above and slightly to the right of the robot’s current view. A large purple coral formation is visible on the left side of the frame, with other corals nearby. To approach the purple coral while avoiding the submarine overhead, the robot should first move leftward toward the coral, then adjust forward motion while maintaining lateral clearance from the submarine. Since the submarine is above, vertical avoidance is less critical than horizontal positioning—so we prioritize lateral movement first, then controlled forward advance. The coral is already in view, so no rotation is needed initially; a gentle left turn followed by straight motion will bring the robot closer safely.",
      "action": [
        "Turn left slightly",
        "Go straight slowly",
        "Maintain leftward offset to keep distance from submarine"
      ]
    },
    "images": [
      "./data/images/temp_307_0118_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine-like robot emerging from the cave entrance, keeping a safe distance to avoid collision while observing its orientation and lighting.",
    "output": {
      "reasoning": "The green submarine-like robot is visible emerging from the cave entrance, centered in the frame and slightly to the left of the cave’s opening. It faces forward (toward the camera), with its lights on, illuminating the water ahead. The cave walls form a narrow corridor; to approach safely, I must avoid the rocky edges on both sides. Given its position and orientation, the safest path is to advance slowly along the central axis of the cave opening while maintaining lateral clearance—approximately 1–2 meters—to prevent collision. Since it’s stationary or moving slowly (no motion blur), I can proceed cautiously straight ahead, then adjust laterally if needed based on real-time observation. No immediate obstacles block the direct path, but I should monitor for drift or sudden movement.",
      "action": [
        "Go straight slowly toward the cave entrance",
        "Maintain center alignment with the submarine’s longitudinal axis",
        "Hold distance at ~1.5 meters from the submarine’s hull",
        "Observe its orientation and light pattern continuously"
      ]
    },
    "images": [
      "./data/images/temp_307_0188_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical pipe assembly on the left and inspect its flange connection for signs of corrosion or leakage.",
    "output": {
      "reasoning": "The green cylindrical pipe assembly is clearly visible on the left side of the image, occupying the foreground. Its flange connection (the circular bolted joint with visible bolts and gasket area) is exposed and accessible. The robot is currently positioned at a moderate distance and slightly angled—likely from the front-right relative to the pipe. To inspect the flange for corrosion or leakage, the robot must: (1) move closer to reduce distance for high-resolution visual inspection, (2) adjust orientation to face the flange directly (i.e., align camera normal to the flange plane), and (3) stabilize at optimal inspection distance (~0.5–1 m). Given the current view, the most efficient path is to advance straight while slightly turning left to center the flange in the field of view.",
      "action": [
        "Turn left slightly",
        "Go straight toward the green pipe assembly",
        "Stop when 1 meter away from the flange",
        "Adjust orientation to face the flange directly"
      ]
    },
    "images": [
      "./data/images/temp_307_0033_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar while avoiding the green net-like structures overhead and the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and illuminated in the center of the image, serving as the primary target. Overhead, green net-like structures span across the top of the frame, requiring the robot to maintain a safe vertical clearance—likely by staying centered and not drifting upward. On the seabed, scattered debris (dark rectangular objects) are present near the base of the pillar and to the left/right; these must be avoided laterally during final approach. The robot appears to be positioned directly in front of the pillar at moderate distance. To safely approach: (1) proceed straight toward the pillar while maintaining neutral heading and depth; (2) monitor for lateral drift toward debris; (3) as proximity increases, fine-tune position to avoid seabed obstacles. No immediate turn is needed—the path is clear ahead.",
      "action": [
        "Go straight toward the central stone pillar",
        "Maintain neutral depth to avoid overhead green nets",
        "Slightly adjust left/right as needed to avoid seabed debris during final approach"
      ]
    },
    "images": [
      "./data/images/temp_307_0044_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green brain coral structure from the front while maintaining a safe distance to avoid contact.",
    "output": {
      "reasoning": "The image shows a large green brain coral structure directly in front of the robot, occupying most of the frame. It is well-lit and clearly visible, with its textured surface indicating proximity. Since the instruction specifies approaching *from the front* while maintaining a *safe distance*, the robot should not move forward aggressively. Instead, it should first ensure alignment with the coral’s central axis (i.e., confirm it's facing it squarely), then proceed with small, controlled forward movements while monitoring distance—ideally using visual cues like relative size change or shadow depth. Given the current perspective, the robot appears already aligned frontally; thus, the immediate action is to advance cautiously without lateral deviation.",
      "action": [
        "Maintain current heading (face directly toward coral)",
        "Move forward slowly by 0.5 meters",
        "Pause and assess distance (ensure >0.3 m clearance)",
        "If still safe, move forward another 0.3 meters"
      ]
    },
    "images": [
      "./data/images/temp_307_0258_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine from the left while maintaining a safe distance, and inspect its hull for visible damage or marine growth.",
    "output": {
      "reasoning": "The green submarine is positioned diagonally in the scene, oriented from lower-left to upper-right, with its bow pointing toward the upper right. The robot appears to be currently located slightly ahead and to the right of the submarine’s midsection (based on perspective and shadow). To approach from the left while maintaining a safe distance, the robot should first reposition laterally to the submarine’s left side—i.e., move left relative to current heading. Then, it should advance slowly along the left flank, keeping parallel alignment to inspect the hull. The coral formation on the left foreground suggests potential obstacle; thus, initial movement must avoid collision by adjusting depth or lateral offset if needed. Given lighting from above and clear visibility, a controlled leftward translation followed by forward motion along the hull is feasible.",
      "action": [
        "Move left to position yourself on the submarine's left side",
        "Advance slowly parallel to the submarine's hull",
        "Maintain steady distance (~1–2 m) while scanning the hull surface",
        "Pause briefly at midsection and stern to inspect for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0167_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spotted rock in the foreground and inspect the orange cone and yellow spherical object resting on its surface.",
    "output": {
      "reasoning": "The large spotted rock is clearly visible in the foreground, centered and slightly to the right. On its surface rest two objects: an orange cone (left side of the rock) and a yellow spherical object (right side of the rock). The robot appears to be positioned at a moderate distance (~1–2 m) from the rock, with clear visibility and no immediate obstacles. To inspect both objects closely, the robot should first approach the rock directly, then position itself to view both items—ideally hovering near the center of the rock’s top surface. Since the instruction specifies 'approach' and 'inspect', the sequence should prioritize closing distance, then fine positioning for visual inspection.",
      "action": [
        "Go straight toward the large spotted rock",
        "Slow down as you near the rock surface",
        "Hover steadily above the rock, centered between the orange cone and yellow sphere",
        "Tilt camera downward slightly to focus on both objects"
      ]
    },
    "images": [
      "./data/images/temp_307_0193_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central corroded metallic pipe structure while avoiding the surrounding porous rock formations and green coral-like growths.",
    "output": {
      "reasoning": "The central corroded metallic pipe structure is clearly visible in the image, positioned vertically and centered in the frame. Surrounding it are porous rock formations on the left and right, and green coral-like growths at the bottom-right and lower-left. The robot appears to be slightly below and in front of the pipe, with a clear upward-forward path toward its midsection. To avoid obstacles: (1) the left-side rock is close, so slight rightward correction is needed; (2) the green coral at bottom-right suggests avoiding low-right trajectories; (3) the pipe’s upper section is illuminated, indicating it's accessible from current depth. A safe approach path is: first ascend slightly while moving forward, then gently correct right to stay clear of left rock, maintaining central alignment toward the pipe’s midsection.",
      "action": [
        "Go straight and slightly ascend",
        "Turn right slightly",
        "Continue straight toward the central pipe"
      ]
    },
    "images": [
      "./data/images/temp_307_0284_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green coral-topped structure and inspect its base for attached sensors or biofouling.",
    "output": {
      "reasoning": "The glowing green coral-topped structure is centrally located in the image and clearly visible. It stands vertically on the seabed, with its base near the sediment surface. The robot appears to be positioned slightly in front and possibly a bit to the left of the structure (based on perspective and lighting). To inspect the base for sensors or biofouling, the robot must first close the distance safely—avoiding nearby netting on the right and coiled cable on the left—and then maneuver to a position directly facing or slightly below the base for optimal visual inspection. Given the current view, the most efficient path is to advance straight toward the structure, then fine-tune positioning once closer.",
      "action": [
        "Go straight toward the glowing green structure",
        "Slow down as you approach the base",
        "Hover and adjust position to face the base directly",
        "Inspect the base area for attached sensors or biofouling"
      ]
    },
    "images": [
      "./data/images/temp_307_0112_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical structure with yellow markings and inspect its surface for damage or fouling.",
    "output": {
      "reasoning": "The image shows a large submerged cylindrical structure with visible yellow markings (likely bands or stripes) running along its length. It is positioned diagonally across the frame, extending from the lower-left toward the upper-right, and appears to be suspended or resting near a netted barrier. The robot is currently at a moderate distance and slightly below/forward of the structure. To approach and inspect its surface, I need to: (1) move forward to reduce distance while maintaining safe clearance from the net on the left; (2) adjust heading to align parallel to the cylinder’s axis for optimal inspection; (3) close in laterally to bring the camera within inspection range (~1–2 m); (4) perform a slow lateral pass along the cylinder to visually scan for damage or fouling. Given lighting and turbidity, maintaining steady motion and orientation is critical.",
      "action": [
        "Go straight slowly",
        "Turn slightly right to align with the cylinder's axis",
        "Move forward closer to the cylinder",
        "Drift left gently to position alongside the cylinder",
        "Perform slow lateral inspection pass along the yellow-marked surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0036_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green-camouflaged underwater vehicle and inspect its front sensor array for damage or debris.",
    "output": {
      "reasoning": "The green-camouflaged underwater vehicle is visible in the center-left of the image, resting on a green base. Its front sensor array (a large circular aperture with a lens/light assembly) faces toward the camera. The robot is currently at a moderate distance and slightly to the right of the vehicle’s frontal axis. To inspect the front sensor array closely, I need to: (1) move forward to reduce distance, (2) adjust leftward to align directly with the front sensor, and (3) stop at close range for visual inspection. No immediate obstacles block the path; the rocky terrain is to the right and rear, so a direct approach is safe.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight slowly",
        "Stop"
      ]
    },
    "images": [
      "./data/images/temp_307_0123_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater passage between the left rock wall and the central coral-covered ridge, maintaining clearance from both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage bounded by a dark rock wall on the left and a coral-covered ridge in the center/right. Sunlight penetrates from above, illuminating the sandy floor and highlighting the passage’s corridor-like shape. The robot appears to be positioned at the entrance, facing into the passage. To navigate safely while maintaining clearance from both sides, the robot should first align centrally within the passage, then proceed forward with slight adjustments to avoid drifting toward either wall. Given the passage’s orientation (slightly curving rightward based on the ridge’s shape), a gentle rightward bias may help stay centered without hitting the left wall. No immediate obstacles are visible ahead, so forward motion is safe if controlled.",
      "action": [
        "Turn slightly right to center in the passage",
        "Go straight slowly",
        "Maintain slight rightward correction to avoid left wall",
        "Continue forward while monitoring distance to both walls"
      ]
    },
    "images": [
      "./data/images/temp_307_0014_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of dark, textured coral formations on the right while maintaining a safe distance from the rising bubble plume in the center.",
    "output": {
      "reasoning": "The image shows a rising bubble plume in the center, which must be avoided for safety. On the right side, there is a cluster of dark, textured coral formations—the target. The robot is currently positioned such that the coral cluster is to its right and slightly ahead, while the bubble plume lies directly ahead and slightly left of center. To approach the coral safely: (1) First, move slightly right to lateralize away from the bubble plume’s trajectory; (2) Then proceed forward while maintaining rightward bias to stay clear of the plume; (3) Finally, fine-tune position to get closer to the coral without encroaching on the plume’s proximity zone (estimated ~1–1.5 m radius based on bubble dispersion).",
      "action": [
        "Turn right slightly",
        "Go forward while staying right of the bubble plume",
        "Approach the dark coral cluster on the right cautiously"
      ]
    },
    "images": [
      "./data/images/temp_307_0170_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater structure with three illuminated circular ports, maintaining a safe distance while aligning with its front face for inspection.",
    "output": {
      "reasoning": "The image shows a large cylindrical underwater structure with three illuminated circular ports arranged horizontally on its front face. The robot is currently positioned slightly below and to the left of the structure, with the front face clearly visible and oriented toward the camera. To approach while maintaining a safe distance and aligning with the front face, the robot should first correct its lateral position to center itself relative to the structure’s symmetry axis, then advance straight while monitoring distance to avoid collision. Given the current angle, a slight rightward correction is needed (since the structure appears left-of-center in the frame), followed by forward motion while keeping the three ports symmetrically framed.",
      "action": [
        "Turn slightly right",
        "Go straight slowly",
        "Maintain alignment so the three illuminated ports remain horizontally centered in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0172_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged metallic structure along its length while maintaining a safe distance to inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The image shows a large submerged metallic structure (likely a shipwreck or pipeline) extending diagonally from the lower-left toward the upper-right, with visible rivets, panels, and marine growth. The robot is currently positioned near the mid-section of the structure, slightly to its left side, at a moderate distance. To inspect along its length while maintaining a safe distance, the optimal path is to parallel-track alongside it—first moving forward (along the structure’s orientation) while keeping lateral offset constant. Given the structure’s diagonal alignment, 'forward' for the robot should align with the structure’s longitudinal axis. The current view suggests the structure continues ahead and upward; thus, the first step is to advance while gently correcting heading to stay parallel. No immediate obstacles are present in the immediate vicinity, so safe progression is feasible.",
      "action": [
        "Turn slightly right to align parallel with the structure's longitudinal axis",
        "Go straight forward while maintaining ~1.5 m lateral distance from the structure",
        "Continue forward, monitoring surface features for damage or biofouling"
      ]
    },
    "images": [
      "./data/images/temp_307_0270_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large cylindrical underwater structure lying on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large cylindrical structure is visible lying horizontally on the seabed, slightly angled with its open end facing toward the upper right of the frame. The robot appears to be positioned above and slightly behind it (relative to the cylinder’s axis). To approach for inspection, the robot should first descend toward the seabed to align with the structure’s height, then move forward along its length—ideally from the closed end toward the open end—to ensure full surface coverage. Given the lighting and nearby coral, we must avoid sudden movements; a controlled descent followed by steady forward motion is safest. No immediate obstacles block the direct path to the rear (closed) end of the cylinder.",
      "action": [
        "Descend vertically to reach seabed level",
        "Move forward slowly toward the rear (closed) end of the cylinder",
        "Align parallel to the cylinder’s long axis",
        "Translate laterally left or right to maintain ~1m standoff distance",
        "Proceed forward along the cylinder’s length while inspecting surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0202_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red conical object hanging beneath the cylindrical underwater vehicle and inspect it closely.",
    "output": {
      "reasoning": "The image shows a cylindrical underwater vehicle suspended above the seabed, with a red conical object hanging directly beneath it (attached via a short tether or rod). The robot is currently positioned at a moderate distance and slightly below/forward of the vehicle. To approach and inspect the red conical object closely, the robot must: (1) move upward to align vertically with the object (since it hangs below the vehicle), (2) then move forward toward the vehicle’s underside where the cone is located, and (3) finally fine-tune position to get close without colliding. The seabed structures are to the sides and not obstructing the direct path upward and forward. Given the perspective, the cone is centered in the field of view under the vehicle, so a straight-up + forward trajectory is optimal.",
      "action": [
        "Go up",
        "Move forward toward the cylindrical vehicle",
        "Approach the red conical object hanging beneath the vehicle",
        "Hover closely in front of the red conical object for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0282_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated green coral structure from the right side while maintaining a safe distance from its textured ridges.",
    "output": {
      "reasoning": "The image shows a large, illuminated green coral structure with prominent textured ridges extending outward. The light source is from the upper right, highlighting the right side of the structure and casting shadows on the left. To approach from the right side while avoiding the ridges, I must first position myself to the right of the structure’s main body. Since the robot appears to be currently facing the front-left portion (based on perspective), I need to reorient rightward. Then, I will advance cautiously along the right flank, keeping lateral clearance from the nearest ridge—visible as the outermost undulating edge on the right—to maintain safety. The goal is not to touch or enter the concave interior but to parallel the right-side contour at a safe offset.",
      "action": [
        "Turn right 15 degrees",
        "Move forward slowly for 2 meters",
        "Maintain slight rightward drift to stay aligned with the right flank",
        "Pause and adjust distance to keep ≥0.5m from nearest ridge"
      ]
    },
    "images": [
      "./data/images/temp_307_0128_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate forward through the central illuminated corridor formed by the green and yellow striped tubular structures.",
    "output": {
      "reasoning": "The image shows a symmetrical underwater corridor formed by green-and-yellow striped tubular structures converging toward a bright central light source. The robot is positioned at the entrance, facing directly down the corridor. The central illuminated path is clearly defined and unobstructed. To follow the instruction, the robot should maintain forward motion along this central axis, avoiding deviation toward either side where the tubular structures are closer. No turns or lateral adjustments are needed initially since the corridor is aligned with the current heading.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0287_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and blue patterned object on the seabed while avoiding the yellow-ringed sea anemone above.",
    "output": {
      "reasoning": "The red and blue patterned object lies on the seabed in the lower-right foreground. A yellow-ringed sea anemone (hazardous) is suspended above and to the left, near the center of the image. To approach the target safely, I must avoid ascending or moving left into the anemone’s vicinity. The safest path is to maintain depth near the seabed and move rightward toward the target, possibly with a slight forward advance once aligned. Since the robot appears already oriented toward the target (right side), the first step is to turn slightly right to align directly, then proceed straight along the seabed while keeping distance from the upper-left anemone.",
      "action": [
        "Turn slightly right",
        "Go straight along the seabed",
        "Approach the red and blue patterned object"
      ]
    },
    "images": [
      "./data/images/temp_307_0072_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green pipeline with yellow markings and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The image shows a large green pipeline with yellow markings extending diagonally from the lower right toward the upper left. The robot (foreground submersible) is positioned to the left and slightly below the pipeline, facing roughly toward it. To approach the pipeline for inspection, the robot should first move forward to reduce distance, then adjust laterally (rightward) to align parallel to the pipeline. Once aligned, it can proceed alongside the pipeline while maintaining close proximity for surface inspection. No immediate obstacles block the direct path, though another submersible is above—likely not interfering. The goal is to get within inspection range (~1–2 m) and travel along the pipe.",
      "action": [
        "Go straight forward",
        "Turn right slightly",
        "Approach the green pipeline until ~1 meter away",
        "Move parallel to the pipeline along its length",
        "Inspect surface for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0179_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate between the two large vertical stone pillars toward the blue coral cluster in the mid-ground.",
    "output": {
      "reasoning": "The image shows two large vertical stone pillars framing a central passage. The blue coral cluster is visible in the mid-ground, slightly to the right of center, beyond the gap between the pillars. To navigate toward it, the robot must first align itself with the gap between the two prominent pillars (left and right). Given the current perspective, the robot appears slightly left of center—so a slight rightward correction is needed to enter the passage. Once aligned, proceed straight through the gap, then adjust slightly right to approach the blue coral cluster, which lies off-center to the right in the mid-ground.",
      "action": [
        "Turn slightly right to align with the gap between the two large stone pillars",
        "Go straight through the gap between the pillars",
        "Turn slightly right to face the blue coral cluster",
        "Advance toward the blue coral cluster"
      ]
    },
    "images": [
      "./data/images/temp_307_0293_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow cylindrical object resting on the seabed near the large dark structure and inspect its surface for markings or damage.",
    "output": {
      "reasoning": "The yellow cylindrical object is visible on the seabed, positioned to the right of the large dark structure (likely a wreck or rock formation). It lies slightly ahead and to the right from the robot’s current perspective. To approach it for inspection, the robot should first move forward toward the general area, then adjust laterally to align with the object. Given its proximity to the dark structure, care must be taken to avoid collision—so a slight right turn followed by forward motion is safer than a direct straight path. Once near, a slow final approach and slight orbit/pan will enable surface inspection for markings or damage.",
      "action": [
        "Turn right slightly",
        "Go forward slowly",
        "Adjust position to center the yellow cylinder in view",
        "Move closer until within 1 meter of the object",
        "Orbit slowly around the cylinder while maintaining close distance to inspect surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0124_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the illuminated opening in the cave wall while avoiding the rocky outcrops on the seafloor.",
    "output": {
      "reasoning": "The illuminated opening is visible in the upper-left portion of the cave wall, serving as the target destination. Rocky outcrops are scattered across the seafloor—particularly prominent on the right and lower-right—and must be avoided. The robot is currently positioned inside the cave, facing roughly toward the opening but slightly offset to the right. To safely approach the opening: (1) First, move slightly left to align with the central axis of the opening and clear the nearest right-side rocks; (2) Then proceed straight forward toward the light, maintaining a safe vertical clearance above the seafloor rocks; (3) As the opening nears, make a minor right correction if needed to center the entry path, since the opening appears slightly right-of-center from current alignment due to perspective. No sharp turns are needed—gentle lateral adjustments suffice given the relatively wide passage.",
      "action": [
        "Move left slightly",
        "Go straight toward the illuminated opening",
        "Adjust right slightly as approaching the opening"
      ]
    },
    "images": [
      "./data/images/temp_307_0022_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the bright green sponge in the foreground while avoiding the surrounding coral structures.",
    "output": {
      "reasoning": "The bright green sponge is clearly visible in the foreground, slightly right of center. Surrounding it are dark, irregular coral structures on the left, right, and behind. To approach safely, I must navigate through a relatively open corridor—likely between the left coral mass and the central sponge. The sponge appears reachable by moving forward while maintaining slight rightward bias to avoid the left coral. No immediate obstacles block the direct path, but careful lateral control is needed to prevent contact with nearby corals.",
      "action": [
        "Go straight slowly",
        "Slightly turn right",
        "Continue forward while keeping the sponge centered in view",
        "Stop when within 0.5 meters of the sponge"
      ]
    },
    "images": [
      "./data/images/temp_307_0143_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large yellow sponge on the left while avoiding the black-spotted coral structure in the foreground.",
    "output": {
      "reasoning": "From the image, the large yellow sponge is visible on the left side of the frame, partially in the foreground. A black-spotted coral structure (likely the dark, porous, spotted object near center-bottom) lies directly in front of the robot’s current position and slightly to the right—blocking a direct straight path. To approach the yellow sponge safely, I must first move laterally left to bypass the coral, then proceed forward toward the sponge. The pipe structure overhead is not an immediate obstacle but indicates orientation; the sponge is clearly leftward relative to the robot’s heading. Therefore, the plan is: (1) turn left to align with the sponge while clearing the coral, (2) advance straight once clear of the coral, (3) fine-tune position to approach the sponge closely.",
      "action": [
        "Turn left",
        "Go straight",
        "Turn left slightly",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0206_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green laser beam straight ahead through the tunnel while maintaining a safe distance from the textured walls on both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater tunnel with textured, irregular walls on both left and right sides. A bright green laser beam projects straight ahead along the central axis of the tunnel, serving as a clear visual guidance path. The robot is currently aligned with the laser, which originates near the center of the field of view and extends into the distance. To follow the instruction, the robot must maintain forward motion while ensuring lateral deviation is minimized to avoid contact with the textured walls—suggesting small corrective adjustments may be needed if drift occurs. Since the laser is centered and the tunnel appears straight in the visible segment, the primary action is to proceed straight, with implicit micro-adjustments assumed in 'maintaining safe distance'. No immediate turn or stop is required.",
      "action": [
        "Go straight",
        "Maintain center alignment with the green laser beam",
        "Slightly adjust left/right as needed to keep equal distance from both textured walls"
      ]
    },
    "images": [
      "./data/images/temp_307_0215_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green underwater robot resting on the large moss-covered rock and inspect its status.",
    "output": {
      "reasoning": "The green underwater robot is clearly visible, resting on top of a large moss-covered rock in the center-right of the image. Sunlight streams from above, illuminating the scene and confirming good visibility. The robot is at moderate distance and slightly elevated relative to the current viewpoint (which appears to be lower and leftward). To approach it safely and for inspection, I should first move forward toward the rock, then adjust laterally if needed to align directly with the robot, and finally close in for a detailed status inspection (e.g., observing lights, sensors, or body integrity). No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the large moss-covered rock",
        "Move slightly right to align with the green robot",
        "Approach closely to inspect the green robot's status"
      ]
    },
    "images": [
      "./data/images/temp_307_0264_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submarine-like object resting on the seabed and inspect its hull for attached marine growth or damage.",
    "output": {
      "reasoning": "The green submarine-like object is visible in the mid-to-upper portion of the image, resting on a flat seabed surface. It is oriented diagonally, with its bow pointing toward the upper-left and stern toward the lower-right. The robot appears to be positioned slightly below and in front of the object (based on perspective and lighting). To approach and inspect the hull, I need to: (1) move forward to reduce distance, (2) adjust lateral position to align parallel to the hull (likely shift left to get a side view), and (3) maintain stable altitude near the seabed to enable close inspection. No immediate obstacles block the path; coral formations are present but to the right and bottom—safe to navigate around by staying centered. Since the hull is long and horizontal, a side-parallel approach is optimal for full visual coverage.",
      "action": [
        "Go straight forward slowly",
        "Shift left slightly to align parallel to the submarine's hull",
        "Maintain steady depth and continue approaching until ~1 meter away",
        "Pan camera right to scan the hull from bow to stern"
      ]
    },
    "images": [
      "./data/images/temp_307_0267_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged turquoise car and inspect its front right side for damage or attached marine growth.",
    "output": {
      "reasoning": "The turquoise car is visible to the right of the central stone column, resting on the seabed. Its front right side (i.e., the passenger-side front corner) is partially visible but not yet close enough for detailed inspection. To approach it safely and position for inspection: (1) First, move forward slightly to clear the column’s immediate vicinity; (2) Then turn right to align with the car’s front-right quadrant; (3) Finally, advance slowly toward that specific area to enable close visual inspection of damage or marine growth.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0248_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submersible vehicle and inspect the red netting wrapped around its front section.",
    "output": {
      "reasoning": "The green submersible vehicle is clearly visible in the center-right of the image, with red netting wrapped around its front (nose) section. The robot appears to be positioned slightly below and to the left of the target, with some green netting (possibly debris or part of the environment) in the immediate foreground on the left. To approach and inspect the red netting, the robot should first move forward toward the vehicle while maintaining a safe lateral offset to avoid collision with foreground objects. Once closer, a slight rightward adjustment may be needed to align with the front section. Finally, a small forward movement will bring the robot into optimal inspection range of the red netting.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight slowly",
        "Hover and inspect the red netting"
      ]
    },
    "images": [
      "./data/images/temp_307_0180_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its upper ornamental structure for signs of marine growth or damage.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, positioned slightly to the right of center and at a moderate distance from the robot's current location. The upper ornamental structure (capital with volutes) is illuminated by overhead light, making it suitable for inspection. To approach it safely and get a clear view of the upper section, I should first move forward toward the column, then position the robot closer and slightly below the ornamental part to inspect for marine growth or damage. No immediate obstacles block the direct path, though the environment is dim and turbid—so controlled forward motion is advised.",
      "action": [
        "Go straight toward the central stone column",
        "Move closer until near the base of the column",
        "Ascend slightly and position in front of the upper ornamental structure",
        "Hover and inspect the capital for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0005_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rightmost column to inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "From the image, three columns are visible: left, center, and rightmost. The rightmost column is clearly identifiable by its position on the far right of the frame and its distinct architectural features (e.g., arched openings, textured surface). The robot appears to be positioned slightly left-of-center, facing roughly forward with the columns ahead. To approach the rightmost column, the robot must first reorient toward the right, then move forward while maintaining alignment with that column. Given the underwater environment and potential for drift or current, small corrective adjustments may be needed. No immediate obstacles block the path to the rightmost column, though the seabed has some debris—so a moderate altitude should be maintained. The goal is to get close enough for visual inspection of surface details (marine growth, cracks, erosion), which likely requires being within ~1–2 meters.",
      "action": [
        "Turn right 30 degrees",
        "Go straight 2 meters",
        "Turn right 15 degrees",
        "Go straight 1.5 meters",
        "Hover and inspect the rightmost column surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0135_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the ancient green-encrusted column and inspect its surface for inscriptions or structural damage.",
    "output": {
      "reasoning": "The ancient column is directly in front of the robot, centered in the field of view, and appears to be within close proximity (a few meters away). The seabed is relatively flat with minor undulations and no immediate obstacles between the robot and the column. To inspect its surface for inscriptions or damage, the robot must first approach it closely—ideally within 1–2 meters—while maintaining a stable orientation to allow high-resolution visual inspection. A straight forward movement is optimal; once near, a slight orbit or lateral shift may be needed for full surface coverage, but the initial step is straightforward approach.",
      "action": [
        "Go straight toward the column"
      ]
    },
    "images": [
      "./data/images/temp_307_0266_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the illuminated green pipeline along the seabed while maintaining a safe distance from the rocky left wall.",
    "output": {
      "reasoning": "The image shows a brightly illuminated green pipeline running diagonally from lower-left to upper-right, parallel to the seabed. A rocky wall is visible on the left side of the frame, close to the pipeline. To follow the pipeline while maintaining a safe distance from the left wall, the robot should position itself slightly right of the pipeline’s centerline—ensuring it stays clear of the rocks but still tracks the pipeline visually. The pipeline appears to curve gently upward/rightward, so the robot must adjust heading gradually to stay aligned. Initial action: move forward while slightly correcting right to establish safe lateral offset; then continue forward with minor right adjustments as the pipeline curves.",
      "action": [
        "Go straight while slightly turning right",
        "Maintain forward motion, keeping the green pipeline centered in view and the rocky wall visible but at a safe distance on the left",
        "Gently turn right as the pipeline curves upward-right"
      ]
    },
    "images": [
      "./data/images/temp_307_0216_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green ribbed cylindrical object with the red fin-like attachment and inspect its surface for biofouling or damage.",
    "output": {
      "reasoning": "The green ribbed cylindrical object with a red fin-like attachment is clearly visible on the right side of the image, resting on or near a spotted rock formation. It is relatively close to the robot’s current position (within ~1–2 m), and the path to it appears unobstructed—no immediate obstacles block direct approach from the current heading. To inspect its surface for biofouling or damage, the robot must first move closer to achieve high-resolution visual coverage. Given the object is to the right and slightly forward, the optimal initial action is to advance straight while maintaining orientation toward the object. Once within ~0.5 m, a slight turn may be needed to align parallel to the cylinder for full surface inspection, but the first step is straightforward forward motion.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0086_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green striped buoy with the orange cone on top and inspect its surface for attached marine growth.",
    "output": {
      "reasoning": "The green striped buoy with an orange cone on top is clearly visible in the center-left of the image, resting on the seabed. It is the primary target. The robot (camera platform) is positioned above and slightly behind it, with a clear line of sight. To approach and inspect its surface, I need to: (1) move forward toward the buoy while maintaining depth; (2) position closely alongside or slightly in front to enable detailed visual inspection; (3) possibly adjust orientation (e.g., slight yaw or pitch) to scan the surface for marine growth. No immediate obstacles block the direct path, though the rusted disc and green dome are nearby—so lateral clearance must be maintained. Since the buoy is stationary and prominent, a straightforward approach is feasible.",
      "action": [
        "Go straight toward the green striped buoy",
        "Move closer until within 0.5 meters of the buoy",
        "Hover and slowly circle around the buoy to inspect all sides",
        "Tilt camera downward slightly to examine the lower surface for marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0062_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and positioned directly ahead of the robot’s current viewpoint. Sunlight rays indicate relatively clear water and good visibility. The pillar stands on a square base, with no immediate obstacles in the direct forward path. To inspect its surface for inscriptions or damage, the robot must first close the distance to within visual inspection range (≈1–2 m), then possibly circle or tilt to examine multiple sides. Given the current alignment, the optimal first step is to move straight toward the pillar’s base, maintaining centering to avoid drifting.",
      "action": [
        "Go straight toward the central stone pillar",
        "Slow down as you approach the base (within 2 meters)",
        "Hover and rotate slowly clockwise to inspect the front-right surface",
        "Rotate counterclockwise to inspect the front-left surface",
        "Tilt camera upward slightly to examine upper sections"
      ]
    },
    "images": [
      "./data/images/temp_307_0115_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front grille for marine growth or damage.",
    "output": {
      "reasoning": "The submerged car is clearly visible in the center-right of the image, resting on the seabed. Its front grille faces the camera, slightly angled to the left. The robot appears to be positioned at a moderate distance (~3–5 m) and slightly elevated (sunlight rays indicate surface above). To inspect the front grille closely, the robot must first approach straight toward the car while maintaining alignment with the grille. Given no immediate obstacles in the direct path, a straightforward advance is safe. Once near (~1–2 m), a slight forward lean or descent may be needed to get a close-up view of the grille’s condition (marine growth/damage). No red obstacles are present per instruction constraints.",
      "action": [
        "Go straight toward the car",
        "Slow down as you near the front grille",
        "Hover steadily 1 meter in front of the grille",
        "Tilt camera downward slightly for detailed inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0052_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green cylindrical device suspended above the twisted green cable structure and inspect its base connection.",
    "output": {
      "reasoning": "The glowing green cylindrical device is centrally located and suspended directly above the twisted green cable structure. From the current perspective, the robot appears to be slightly below and at a distance from the device, with clear visibility of both the device and its connection point to the cable below. To inspect the base connection, the robot must first move upward to align horizontally with the base of the cylinder, then close in while maintaining stable orientation to avoid disturbing the delicate cable structure. The path appears unobstructed, so a direct approach is feasible: ascend, then advance forward toward the base, possibly with slight lateral adjustment if needed upon closer inspection.",
      "action": [
        "Ascend vertically",
        "Move forward toward the glowing green cylinder",
        "Adjust position to center on the base connection",
        "Hover and inspect the base connection"
      ]
    },
    "images": [
      "./data/images/temp_307_0151_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical object on the seabed and inspect its top valve assembly.",
    "output": {
      "reasoning": "The image shows a red cylindrical object upright on the seabed, located to the left-center of the frame. The robot appears to be positioned slightly above and to the right of it, with another large metallic structure dominating the right side of the view. To approach the red cylinder and inspect its top valve assembly, the robot must first move laterally leftward to align with the cylinder, then descend gently to get closer to its top (which is likely near seabed level, given its upright posture). Once in position above or beside the top, a slight upward tilt or hover may be needed to clearly view the valve assembly. No immediate obstacles block the path to the red cylinder, though care should be taken to avoid the nearby large structure on the right.",
      "action": [
        "Turn left",
        "Move forward toward the red cylindrical object",
        "Descend slightly to match its height",
        "Hover above the top of the red cylinder",
        "Tilt camera upward to inspect the top valve assembly"
      ]
    },
    "images": [
      "./data/images/temp_307_0219_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow sponge located on the seabed near the cluster of dark rocks in the foreground.",
    "output": {
      "reasoning": "The yellow sponge is not clearly visible in the current frame, but the instruction specifies it lies on the seabed near a cluster of dark rocks in the foreground. In the image, there is a prominent cluster of dark rocks at the bottom right and center foreground. The seabed is visible and relatively flat there. To approach the sponge, I must first navigate toward that rock cluster. Since the robot appears to be positioned slightly left and above the target area (based on perspective and lighting), the logical first step is to move forward and slightly right to align with the rock cluster. Once closer, I can fine-tune positioning to locate and approach the yellow sponge on the seabed.",
      "action": [
        "Move forward",
        "Turn slightly right",
        "Descend gently toward seabed"
      ]
    },
    "images": [
      "./data/images/temp_307_0007_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large segmented pipeline on the seabed and inspect the yellow-banded joint section for signs of corrosion or damage.",
    "output": {
      "reasoning": "The image shows a large segmented pipeline lying on the seabed, extending diagonally from the lower-left toward the upper-right. A prominent yellow-banded joint section is visible near the center-right of the frame — this is the target for inspection. The robot appears to be positioned slightly to the left and in front of the pipeline, with the yellow joint ahead and slightly to the right. To approach it safely and align for close inspection, the robot should first move forward along the pipeline’s axis, then adjust laterally (rightward) to center on the yellow-banded joint. Given the pipeline’s orientation and the robot’s current pose, a sequence of 'Go straight' followed by 'Turn right' and another 'Go straight' will bring the robot into optimal proximity and alignment for visual inspection.",
      "action": [
        "Go straight",
        "Turn right",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0238_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large bioluminescent spherical organism with red-orange polyps and inspect its surface texture while maintaining a safe distance.",
    "output": {
      "reasoning": "The image shows a large bioluminescent spherical organism with red-orange polyps (likely the target) centered in the frame, resting on a purple base. The robot (submersible with red lights) is currently positioned above and slightly behind the organism, at a moderate distance. To approach while maintaining a safe distance, the robot should first descend vertically to align horizontally with the organism’s equator, then move forward slowly toward it—stopping before entering the immediate vicinity of the polyps (to avoid contact or disturbance). Given the organism’s size and prominence, a controlled forward advance after leveling is sufficient; no lateral correction is needed. Inspection of surface texture can occur once stable at ~1–2 m distance.",
      "action": [
        "Descend vertically to align with the organism's midsection",
        "Move forward slowly toward the organism",
        "Stop at a safe distance (~1.5 meters) for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0301_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated yellow net structure while avoiding the overhead cables and the fish swimming nearby.",
    "output": {
      "reasoning": "The illuminated yellow net structure is the primary target, located centrally and slightly below the current viewpoint. Overhead cables (thick black/yellow bundles) are positioned above and to the left—requiring careful vertical clearance. A fish is visible to the left of the net, swimming horizontally; it poses a dynamic obstacle that should be avoided by maintaining lateral distance. The robot appears to be approaching from a slight distance and angle; to safely reach the net, it should first adjust laterally to the right to avoid both the fish and the left-side cables, then proceed forward while maintaining safe altitude under the overhead cables. The net’s illumination helps with visual tracking during approach.",
      "action": [
        "Turn right slightly",
        "Move forward slowly while maintaining depth below overhead cables",
        "Adjust leftward gently if needed to center on net entrance",
        "Continue forward toward the illuminated yellow net structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0015_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green ring-shaped coral on the right while avoiding the overhanging rock formation above.",
    "output": {
      "reasoning": "The glowing green ring-shaped coral is clearly visible on the right side of the image, partially obscured by a large overhanging rock formation above it. To approach it safely, I must avoid collision with the overhead rock. The coral appears at mid-depth and slightly forward-right from the current viewpoint. A safe path would involve: (1) moving slightly right to align toward the coral while staying below the overhang; (2) advancing forward cautiously, maintaining clearance under the rock; (3) fine-tuning position to center on the coral. Since the rock overhang is directly above the coral’s location, I should not ascend—instead, stay at or slightly below current depth and move laterally first.",
      "action": [
        "Turn right slightly",
        "Go straight slowly while maintaining current depth",
        "Adjust rightward gently to center on the green ring coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0250_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical structure while maintaining a safe distance from the suspended cable above it.",
    "output": {
      "reasoning": "The image shows a large green cylindrical structure lying horizontally on the seabed, with a suspended cable running diagonally above it (from upper left to lower right). The robot appears to be approaching from the left side, slightly below the structure. To approach safely while avoiding the cable: (1) First, align the robot parallel to the cylinder’s long axis to maintain predictable clearance; (2) Since the cable is above and slightly ahead, we should move forward cautiously while keeping depth stable—avoid ascending toward the cable; (3) The current view suggests the robot is already at a safe vertical distance below the cable, so the priority is lateral alignment and controlled forward motion. We’ll proceed straight while monitoring relative position to the cable and structure.",
      "action": [
        "Turn slightly right to align parallel with the green cylinder",
        "Go straight slowly while maintaining current depth",
        "Monitor cable position and adjust depth downward if needed"
      ]
    },
    "images": [
      "./data/images/temp_307_0277_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column while avoiding the scattered debris on the seabed.",
    "output": {
      "reasoning": "The central stone column is clearly visible in the image, illuminated by light rays from above and positioned slightly to the right of center. Scattered debris (small rocks and fragments) lies on the seabed, especially in the foreground and left side. To approach the column safely, I should first move forward while staying centered to avoid the left-side debris, then make a slight right correction as I get closer to align with the column’s base. The path appears relatively clear directly ahead toward the column, but minor lateral adjustments are needed to steer around nearby debris.",
      "action": [
        "Go straight",
        "Slightly turn right",
        "Go straight toward the column base"
      ]
    },
    "images": [
      "./data/images/temp_307_0043_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate through the narrow underwater canyon, keeping centered between the two rocky walls while maintaining a safe distance from the coral-covered surfaces.",
    "output": {
      "reasoning": "The robot is currently positioned in a narrow underwater canyon with rocky walls on both left and right sides. The coral-covered surfaces are visible along the walls, especially on the lower portions and near the edges—these must be avoided to prevent damage. The robot’s current heading is aligned with the canyon’s central axis, as indicated by the light rays converging ahead and the symmetric wall geometry. To maintain centerline navigation while avoiding coral, the robot should proceed forward with minor lateral corrections: slight left adjustments if drifting right toward the right wall (which appears closer and more textured with coral), and vice versa. Given the current view, the right wall is slightly nearer and more heavily covered in coral; thus, a small leftward correction is prudent before continuing straight. Subsequent steps will involve monitoring wall proximity and adjusting incrementally.",
      "action": [
        "Slightly turn left",
        "Go straight",
        "Monitor left and right distances; adjust minimally to stay centered",
        "Continue forward while maintaining safe clearance from coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0047_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green car positioned between the two large stone columns and inspect its front bumper for damage.",
    "output": {
      "reasoning": "The green car is clearly visible in the center of the image, positioned between two large stone columns. The robot appears to be at a moderate distance behind and slightly above the car, with a clear forward path toward it. To approach the car and inspect its front bumper, I need to: (1) move straight forward toward the car, (2) possibly adjust laterally to center on the car if needed, and (3) get close enough to inspect the front bumper—likely requiring a slight downward or forward-tilt maneuver once near. Given the open space and lack of immediate obstacles, a direct forward approach is safe and efficient.",
      "action": [
        "Go straight toward the green car",
        "Adjust slightly left or right to center on the car if needed",
        "Move closer until the front bumper is in clear view",
        "Hover and inspect the front bumper for damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0146_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical perforated pipe structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The vertical perforated pipe structure is directly in front of the robot and dominates the central field of view, indicating it is relatively close. The lighting highlights its surface texture and perforations, suggesting good visibility for inspection. To inspect the surface thoroughly, the robot should first approach closer to ensure high-resolution imaging, then perform a slow circumferential movement (e.g., orbit or rotate around it) to cover all sides. Since no immediate obstacles block the direct path, the initial action is to move forward toward the base of the pipe. After reaching an optimal inspection distance (~1–2 m), a controlled right turn (or left, depending on desired inspection direction) can begin the orbital sweep. Given the instruction emphasizes 'approach' first, we prioritize forward motion before rotation.",
      "action": [
        "Go straight toward the pipe structure",
        "Slow down as you near the base",
        "Begin a gentle right turn to orbit the pipe"
      ]
    },
    "images": [
      "./data/images/temp_307_0087_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged turquoise car and inspect the blue conical object mounted on its roof.",
    "output": {
      "reasoning": "The turquoise car is clearly visible in the center of the image, resting on the seabed. A blue conical object (likely a sensor or beacon) is mounted on its roof, emitting bubbles upward. The robot appears to be positioned slightly in front and to the left of the car, with clear visibility and no immediate obstacles blocking the path. To approach and inspect the blue conical object, the robot should first move forward toward the car, then adjust laterally (slightly right) to align directly beneath or beside the roof-mounted object for close inspection. Given the car’s orientation (front facing camera), approaching from the front-left to front-center is safe and optimal.",
      "action": [
        "Go straight toward the turquoise car",
        "Slightly turn right to align with the car's centerline",
        "Move forward slowly to get closer to the car's roof",
        "Hover and inspect the blue conical object on the roof"
      ]
    },
    "images": [
      "./data/images/temp_307_0104_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central concrete pillar structure and inspect its surface for marine growth or damage, focusing on the circular openings at mid-height.",
    "output": {
      "reasoning": "The image shows a central concrete pillar structure with visible circular openings at mid-height, partially covered in marine growth. The robot is currently positioned at a moderate distance, slightly to the left and front of the pillar. To approach and inspect the circular openings, I need to: (1) move forward toward the pillar while maintaining alignment with its center; (2) adjust laterally if needed to center the target openings in the field of view; (3) close in to a suitable inspection distance (~1–2 m) without colliding; (4) possibly rotate slightly to ensure full visibility of all openings around the circumference. No immediate obstacles block the path, but the seabed is uneven—so maintain stable altitude. Given the current perspective, the most direct path is to go straight ahead first, then fine-tune position.",
      "action": [
        "Go straight toward the central concrete pillar",
        "Adjust left/right slightly to center the pillar in view",
        "Continue forward until ~1.5 meters from the pillar",
        "Hover and slowly rotate clockwise to inspect all circular openings at mid-height"
      ]
    },
    "images": [
      "./data/images/temp_307_0105_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the tall textured pillar on the right while maintaining a safe distance from the surrounding rock formations.",
    "output": {
      "reasoning": "The image shows an underwater scene with a prominent tall textured pillar on the right side of the frame. Surrounding it are irregular rock formations—some to the left, some in front, and one large rounded rock near the bottom right. The robot appears to be positioned slightly left of center, facing forward. To approach the pillar safely, I must avoid colliding with nearby rocks: the large foreground rock on the bottom right, the mid-ground rocks near the pillar’s base, and the left-side rock wall. A safe path would involve first moving slightly right (but not too close to the bottom-right rock), then advancing straight toward the pillar while keeping lateral clearance. Given the pillar is clearly visible and relatively distant, a two-step plan suffices: (1) gently reorient rightward to align with the pillar’s axis while staying clear of obstacles, (2) proceed forward at a steady pace, monitoring proximity to surrounding rocks.",
      "action": [
        "Turn slightly right",
        "Go straight toward the tall textured pillar"
      ]
    },
    "images": [
      "./data/images/temp_307_0026_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red cylindrical underwater instrument and inspect its top sensor module while avoiding the nearby rocky outcrop on the right.",
    "output": {
      "reasoning": "The red cylindrical instrument is the primary target, positioned slightly left of center and angled upward. Its top sensor module (white cap with black components) is visible at the upper end. A rocky outcrop occupies the right side of the frame, posing a collision risk if the robot moves directly rightward or drifts. To safely approach: (1) First, move forward cautiously while maintaining left bias to avoid the right-side rocks; (2) Then, adjust heading slightly right to align with the instrument’s axis without encroaching on the rock; (3) Finally, position directly above or slightly in front of the top sensor module for inspection—ensuring sufficient clearance from both the rock and the instrument’s body. The current orientation shows the instrument tilted, so a slight upward pitch may also help stabilize viewing angle during final approach.",
      "action": [
        "Go straight while keeping left of center to avoid the right rocky outcrop",
        "Gently turn right to align with the red instrument's longitudinal axis",
        "Move forward slowly to approach the top sensor module",
        "Hover steadily above the top sensor module for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0254_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated spherical coral in the center while avoiding the large rock formations on the left and right.",
    "output": {
      "reasoning": "The illuminated spherical coral is clearly visible in the center of the image, directly under a light source (likely the robot's own lamp or an external beacon), making it the primary target. Large rock formations flank both the left and right sides of the scene, forming a narrow corridor toward the coral. To avoid collision, the robot must maintain a central trajectory. The coral appears at moderate distance ahead, so initial forward motion is safe if centered. No immediate obstacles block the direct path, but precise lateral positioning is critical. Therefore, the plan is: (1) slightly adjust to center alignment if needed, (2) proceed straight while monitoring proximity to left/right rocks, (3) decelerate as approaching the coral to ensure safe proximity.",
      "action": [
        "Center yourself between the left and right rock formations",
        "Go straight toward the illuminated spherical coral",
        "Slow down as you near the coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0184_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted green station wagon and inspect its front grille for attached marine growth or debris.",
    "output": {
      "reasoning": "The rusted green station wagon is clearly visible in the center-left of the image, oriented diagonally with its front grille facing toward the lower-right. The robot appears to be positioned slightly ahead and to the right of the vehicle (based on perspective). To approach the front grille for inspection, the robot should first move forward to close distance, then adjust leftward to align directly with the grille. Since the grille is the target inspection point, final positioning should be close (~1–2 m) and head-on. No immediate obstacles block the path; the large circular structure above and the engine-like object to the right are at safe distances. Lighting from above illuminates the scene adequately.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight slowly",
        "Stop in front of the front grille"
      ]
    },
    "images": [
      "./data/images/temp_307_0261_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow branching coral on the left while maintaining a safe distance from the school of fish on the right.",
    "output": {
      "reasoning": "The image shows the robot (submarine) currently oriented horizontally, with a prominent yellow branching coral on the left side of the frame and a school of fish on the right. The coral is relatively close and in clear view, while the fish are mid-to-far right, indicating potential collision risk if moving directly rightward or drifting. To approach the coral safely: (1) First, ensure lateral clearance from the fish—since they’re on the right, a slight leftward adjustment is safe and aligns with the target direction. (2) Then proceed forward toward the coral, maintaining a buffer distance to avoid disturbing it or risking entanglement. (3) Given the coral’s position slightly ahead and left, a gentle left turn followed by straight advance is optimal. No sharp turns needed—the coral is already in field of view.",
      "action": [
        "Turn slightly left",
        "Go straight toward the yellow branching coral",
        "Maintain steady distance from the school of fish on the right"
      ]
    },
    "images": [
      "./data/images/temp_307_0177_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green and purple striped cylindrical object on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large green and purple striped cylindrical object lies on the seabed in the lower-left to center portion of the image. The robot is currently positioned above and slightly to the right of the object, with clear visibility and no immediate obstacles blocking the path. To approach and inspect its surface, I need to: (1) descend toward the seabed to reach the object’s depth; (2) move forward (toward the object) while maintaining a safe distance initially; (3) circle or reposition laterally to enable close-up visual inspection of its surface for damage or marine growth (e.g., algae, barnacles). Given the object’s orientation (horizontal, lying on side), a slight leftward movement may help align for optimal viewing angle before closing in.",
      "action": [
        "Descend slowly toward the seabed",
        "Move forward toward the green and purple striped cylinder",
        "Shift left slightly to align with the cylinder’s midsection",
        "Approach closer for detailed surface inspection",
        "Pan camera gently around the cylinder to inspect full circumference"
      ]
    },
    "images": [
      "./data/images/temp_307_0027_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ribbed cylindrical object lying on the seabed and inspect its surface for attached organisms or damage.",
    "output": {
      "reasoning": "The large green ribbed cylindrical object is clearly visible on the seabed, positioned slightly to the right and ahead of the robot’s current viewpoint. Sunlight rays indicate the robot is relatively shallow and oriented roughly toward the object. To approach it for inspection, the robot should first move forward while maintaining alignment with the object. As it gets closer, a slight rightward correction may be needed to center the object in the field of view. Once within ~1–2 meters, it should slow and stabilize to enable detailed surface inspection (e.g., for organisms or damage). No immediate obstacles block the direct path, though surrounding coral structures require cautious navigation to avoid contact.",
      "action": [
        "Go straight",
        "Slightly turn right",
        "Go straight slowly",
        "Hover and stabilize near the object"
      ]
    },
    "images": [
      "./data/images/temp_307_0137_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-lit submersible vehicle while avoiding the foreground pipeline and yellow coral formations.",
    "output": {
      "reasoning": "The large green-lit submersible is visible in the mid-to-upper right of the image, illuminated by a strong green beam pointing downward. A prominent pipeline runs diagonally across the foreground from lower-left to mid-right, and yellow coral formations are clustered on the right side near the bottom. To approach the submersible safely: (1) First, move left to clear the pipeline’s immediate path; (2) Then proceed forward while staying slightly left of center to avoid both the pipeline (now on the right) and the yellow coral (on the far right); (3) Finally, adjust rightward gently as the submersible comes closer to align with it, ensuring clearance from coral. The submersible appears stationary and elevated, so upward pitch may be needed later, but initial steps focus on lateral avoidance and forward progress.",
      "action": [
        "Turn left",
        "Go straight",
        "Turn slightly right",
        "Go straight toward the green-lit submersible"
      ]
    },
    "images": [
      "./data/images/temp_307_0279_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and directly ahead in the current frame. The robot is positioned at a moderate distance, with clear water and no immediate obstacles between it and the pillar. To inspect the surface for marine growth or structural damage, the robot must first approach closely—ideally within 1–2 meters—and then perform a slow circumferential scan to observe all sides. Given the current orientation (facing the pillar head-on), the optimal first step is to move straight forward toward the base of the pillar. Once near, a controlled orbit (e.g., slight right turn while maintaining distance) will allow full visual inspection.",
      "action": [
        "Go straight toward the central stone pillar",
        "Slow down as you near the base (within 1.5 meters)",
        "Orbit clockwise around the pillar at constant distance",
        "Inspect surface details during orbit, focusing on discoloration, barnacles, cracks, or erosion"
      ]
    },
    "images": [
      "./data/images/temp_307_0235_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange buoy with the green handle and inspect its base for attached marine growth.",
    "output": {
      "reasoning": "The orange buoy with a green handle is clearly visible in the foreground, centered and slightly to the right of the robot’s current position. The base of the buoy is partially submerged and rests on the textured green seabed. To inspect its base for marine growth, I need to: (1) move forward toward the buoy while maintaining alignment, (2) get close enough to resolve fine details at the waterline/base interface, and (3) possibly adjust vertically or laterally to center the base in view. Given the proximity (~1–2 m), a short forward motion followed by a slight descent and fine positioning should suffice. No obstacles block the direct path.",
      "action": [
        "Go straight toward the orange buoy",
        "Descend slightly to align camera with buoy base",
        "Hover and stabilize near the buoy base for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0113_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large blue cylindrical pipe lying on the seabed and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The large blue cylindrical pipe is clearly visible lying horizontally on the seabed, occupying the central-left portion of the image. The robot appears to be positioned slightly above and behind it (given the perspective and light rays from surface). To approach and inspect its surface, I need to: (1) move forward toward the pipe while maintaining a safe distance; (2) align laterally to bring the pipe into optimal inspection range; (3) possibly circle or translate alongside it for full surface coverage. Since the pipe is long and oriented left-to-right in the frame, approaching from the current angle likely requires moving forward and slightly left to reach its midsection first. No immediate obstacles block the path, but caution is needed near the seabed structure on the left. Inspection will require close proximity (~0.5–1 m) and stable positioning.",
      "action": [
        "Go straight slowly",
        "Move slightly left to align with the pipe center",
        "Approach closer until ~1 meter from pipe surface",
        "Translate right along the pipe while maintaining distance",
        "Pause and inspect surface for anomalies"
      ]
    },
    "images": [
      "./data/images/temp_307_0140_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine-like vehicle directly ahead while maintaining a safe distance from the surrounding rocky seabed.",
    "output": {
      "reasoning": "The submarine-like vehicle is centered in the image, directly ahead of the robot’s current position. The seabed is rocky on both left and right sides, with visible boulders close to the vehicle’s path. To approach safely, the robot must maintain a central trajectory to avoid lateral obstacles. Given the clear forward visibility and the vehicle’s stable orientation, a controlled forward movement with minor course corrections (if needed) is appropriate. No immediate turns are required since the target is straight ahead and unobstructed in the mid-field. The safe distance can be preserved by advancing at moderate speed while monitoring proximity to seabed features.",
      "action": [
        "Go straight",
        "Maintain central heading",
        "Proceed cautiously toward the submarine"
      ]
    },
    "images": [
      "./data/images/temp_307_0158_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red underwater vehicle and inspect its front-mounted sensor array.",
    "output": {
      "reasoning": "The red underwater vehicle is directly in front of the robot, positioned slightly to the left of center in the image, and appears at a moderate distance (~2–3 meters). Its front-mounted sensor array (a circular white housing with a central lens) is clearly visible. To inspect it closely, I need to approach straight ahead while maintaining alignment with the vehicle’s longitudinal axis. No immediate obstacles block the path; the seabed is clear ahead, and coral is to the right. A gentle forward motion will bring the robot within inspection range (~0.5–1 m), after which a slight pitch-up or hover may be needed for optimal sensor view—but the instruction only requires approaching and inspecting, so initial approach is priority.",
      "action": [
        "Go straight toward the red vehicle",
        "Slow down as you near the vehicle",
        "Hover facing the front-mounted sensor array"
      ]
    },
    "images": [
      "./data/images/temp_307_0094_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular coral structure in the foreground while avoiding the vertical column on the left.",
    "output": {
      "reasoning": "The image shows a large circular coral structure in the foreground (right-lower quadrant), and a prominent vertical column on the left side of the frame. To approach the coral while avoiding the column, the robot should first move rightward to clear the column’s proximity, then proceed forward toward the coral. Since the coral is already relatively close and centered in the lower-right, a slight right turn followed by straight movement should safely align the robot with the target without colliding with the left column.",
      "action": [
        "Turn right slightly",
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0126_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red-and-blue cylindrical underwater structure and inspect its midsection for attached sensors or connectors.",
    "output": {
      "reasoning": "The red-and-blue cylindrical structure is clearly visible in the center of the image, upright and anchored among coral formations. The robot appears to be positioned slightly to the left and at a moderate distance (~2–3 m) from the structure, with a clear forward path. To inspect the midsection (approximately the blue band region), the robot must first close the distance while maintaining stable alignment. Given the structure’s vertical orientation and the presence of coral on both sides, a straight approach is feasible but requires slight rightward correction to center on the cylinder. Once near (~1 m), a slow hover or slight upward/downward adjustment may be needed to align the camera with the midsection. No immediate obstacles block the direct path.",
      "action": [
        "Go straight toward the red-and-blue cylinder",
        "Slightly turn right to center on the cylinder",
        "Continue forward until ~1 meter away",
        "Hover and adjust altitude to align camera with the blue midsection",
        "Inspect the midsection for sensors or connectors"
      ]
    },
    "images": [
      "./data/images/temp_307_0182_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue pickup truck and inspect its front right wheel for entanglement or damage.",
    "output": {
      "reasoning": "The blue pickup truck is visible in the center-right of the image, resting on the seabed between two large columns. Its front right wheel is partially visible but not yet close enough for detailed inspection. To inspect the front right wheel, I need to approach the vehicle from a position that allows clear access—ideally from the front-right quadrant. Given the current perspective (slightly elevated and left-of-center), the optimal path is: (1) move forward toward the truck while slightly adjusting right to align with the front-right side; (2) once near, fine-tune position to bring the front right wheel into close view; (3) stabilize for inspection. No immediate obstacles block this path, though care must be taken around the column on the left and seabed debris.",
      "action": [
        "Move forward and slightly right toward the blue pickup truck",
        "Approach the front-right side of the truck",
        "Position closely in front of the front right wheel",
        "Stabilize and inspect the front right wheel for entanglement or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0053_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green ridged coral in the foreground while maintaining a safe distance from the yellow sponge on the left and the underwater structure in the background.",
    "output": {
      "reasoning": "The green ridged coral is in the foreground, centered-right in the image. The yellow sponge is on the left side, close to the robot’s current position. An underwater structure (likely a platform or observatory) is visible in the background, slightly left of center. To approach the green coral safely: (1) avoid drifting left toward the yellow sponge; (2) maintain forward progress without getting too close to the background structure; (3) steer slightly right to align with the coral while keeping lateral clearance from the sponge. Given the coral is already relatively close and well-lit, a gentle rightward correction followed by straight advance should suffice.",
      "action": [
        "Turn slightly right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0163_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large orange cylindrical pipe lying on the seabed and inspect its open end for blockages or damage.",
    "output": {
      "reasoning": "The large orange cylindrical pipe is clearly visible on the seabed, lying horizontally with its open end facing roughly toward the camera's current position. The robot appears to be at a moderate distance (~2–3 m) and slightly elevated above the seabed. To inspect the open end for blockages or damage, the robot must: (1) descend gently to seabed level to avoid parallax and ensure stable inspection, (2) move forward straight toward the pipe’s open end, (3) stop at close range (~0.5 m) to allow high-resolution visual inspection, and (4) optionally adjust yaw slightly to center the open end in the field of view if needed. No immediate obstacles are present between the robot and the pipe’s open end.",
      "action": [
        "Descend to seabed level",
        "Go straight toward the orange pipe's open end",
        "Stop approximately 0.5 meters from the open end",
        "Adjust yaw slightly to center the pipe's opening in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0010_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green mesh pipeline while maintaining a safe distance from the red-and-black flexible hose on the right.",
    "output": {
      "reasoning": "The image shows an underwater scene with a green mesh pipeline running diagonally from bottom-left toward upper-right, and a red-and-black flexible hose on the right side, curving upward. The robot (submersible) is currently positioned near the center, slightly above the pipeline, with bubbles indicating forward motion. To follow the green mesh pipeline while avoiding the red-and-black hose on the right, the robot must: (1) align itself parallel to the pipeline’s direction; (2) maintain lateral offset to the left of the pipeline (since the hose is on the right, staying left ensures safe distance); (3) proceed forward steadily, adjusting heading as the pipeline curves gently. The pipeline appears to extend forward and slightly upward; thus, a slight rightward heading correction may be needed initially to stay aligned, but overall the trajectory should track the pipeline’s left edge.",
      "action": [
        "Turn slightly right to align with the green mesh pipeline",
        "Go straight while keeping the green mesh pipeline on the left side",
        "Maintain distance from the red-and-black hose on the right",
        "Adjust heading gently right if pipeline curves right"
      ]
    },
    "images": [
      "./data/images/temp_307_0076_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central black pipe while avoiding the surrounding green anemones and maintaining clearance from the right-hand ribbed structure.",
    "output": {
      "reasoning": "The image shows a central black pipe extending vertically upward, flanked by bright green anemones on the left and bottom, and a ribbed structure on the right. The robot is positioned below and slightly in front of the pipe. To approach the central pipe safely: (1) First, maintain distance from the right-hand ribbed structure—so avoid moving right; (2) Green anemones are clustered to the left and near the base—so steer slightly right of center but not too close to the ribbed structure; (3) The pipe is directly ahead and upward, so proceed forward while gently correcting left-to-center to stay aligned with the pipe’s axis without encroaching on anemones. Given the perspective, a slight leftward bias initially may be needed to clear the left-side anemones, then centering as we ascend.",
      "action": [
        "Gently turn left to clear left-side green anemones",
        "Move forward toward the central black pipe",
        "Slightly adjust right to maintain clearance from the right-hand ribbed structure",
        "Continue straight while keeping the pipe centered in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0280_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted pickup truck on the seabed and inspect its front grille for marine growth or damage.",
    "output": {
      "reasoning": "The rusted pickup truck is clearly visible on the seabed, positioned in the lower-left to center of the frame. Its front grille faces roughly toward the camera but slightly angled to the right. To inspect the front grille closely, I need to approach it directly while maintaining a stable, forward-facing orientation. The large cylindrical object above is suspended and not obstructing the direct path to the truck’s front. There are no immediate obstacles between the robot’s current position (assumed near camera viewpoint) and the truck’s front grille. Therefore, the plan is: (1) move forward toward the truck, (2) adjust slight rightward heading if needed to center on the grille, (3) stop at close inspection distance (~1–2 m) and stabilize for visual assessment.",
      "action": [
        "Go straight toward the rusted pickup truck",
        "Slightly turn right to align with the front grille",
        "Stop at ~1.5 meters from the grille and hold position"
      ]
    },
    "images": [
      "./data/images/temp_307_0290_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the stone tower structure from the front while maintaining a safe distance from the red fish swimming to its right.",
    "output": {
      "reasoning": "The stone tower is centered in the image, directly ahead of the robot's current position. A red fish is visible to the right of the tower (from the robot’s perspective), swimming near its base. To approach the tower from the front while avoiding the red fish, the robot should first maintain a central heading toward the tower, then slightly adjust leftward if needed to ensure safe lateral clearance from the fish. Since the fish is to the right, a small left bias during forward motion will increase safety margin without deviating from the frontal approach. The robot should proceed slowly and monitor relative positions.",
      "action": [
        "Go straight toward the stone tower",
        "Slightly turn left to maintain distance from the red fish on the right",
        "Continue straight at a steady pace, keeping the tower centered in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0196_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large rusted pipe structure on the right while avoiding the rocky outcrop in the foreground.",
    "output": {
      "reasoning": "The image shows a large rusted pipe structure on the right side of the frame, partially illuminated by light rays from above. In the foreground, there is a prominent rocky outcrop occupying the lower-left to center area, which must be avoided. The robot appears to be positioned slightly left of center, facing forward. To approach the pipe structure while avoiding the rocks, the robot should first move rightward to clear the rocky outcrop, then proceed forward toward the pipe. A slight right turn followed by straight motion aligns with the spatial layout: the pipe is to the right and somewhat ahead, and the rocks are directly in front-left. No immediate obstacles block the rightward path.",
      "action": [
        "Turn right slightly",
        "Go straight",
        "Continue straight toward the pipe structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0228_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of yellow-tipped coral formations on the left while maintaining a safe distance from the large dark overhang above.",
    "output": {
      "reasoning": "The image shows a vibrant underwater scene with a cluster of yellow-tipped coral formations on the left side of the frame. Above them, there is a large dark overhang (likely rock or reef structure) casting shadows and occupying the upper portion of the image. To approach the coral safely, I must: (1) orient toward the left where the target corals are located, (2) maintain horizontal distance from the overhang—i.e., avoid moving upward or too close vertically beneath it—and (3) proceed gradually to ensure safe clearance. The lighting rays suggest the overhang is overhead but not directly above the corals; the corals lie in the mid-left foreground. Therefore, the safest path is to first turn left to align with the coral cluster, then advance straight while keeping depth constant (to stay clear of the overhang), and optionally make minor right corrections if drifting too close to the overhang.",
      "action": [
        "Turn left",
        "Go straight slowly",
        "Maintain current depth",
        "Slight right adjustment if nearing overhang"
      ]
    },
    "images": [
      "./data/images/temp_307_0298_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged cylindrical vehicle and inspect its front hatch and sensor array for damage or debris.",
    "output": {
      "reasoning": "The submerged cylindrical vehicle is visible in the center-right of the image, lying on the seabed at a slight angle. Its front hatch (rectangular, dark panel with grid-like texture) and sensor array (circular ports and protruding sensors near the nose) are facing approximately toward the camera’s current position but slightly to the left. The robot appears to be positioned somewhat behind and to the left of the vehicle’s midsection, based on perspective and lighting. To inspect the front hatch and sensor array, the robot must first approach the front end—requiring forward movement while adjusting heading to align with the vehicle’s longitudinal axis. Given the vehicle’s orientation (nose pointing ~30° left of camera’s forward direction), the robot should first turn slightly left to face the front, then advance steadily while maintaining alignment. Once within ~2 meters, it can perform a slow close-in inspection pass (e.g., slight upward/downward or lateral micro-adjustments) to visually scan for damage or debris on the hatch and sensors. No immediate obstacles block the path, though seabed rocks are present to the right—so maintain central clearance.",
      "action": [
        "Turn left 15 degrees",
        "Go straight 3 meters",
        "Turn left 5 degrees",
        "Go straight 2 meters",
        "Hover and slowly ascend 0.5 meters",
        "Drift forward 0.8 meters while keeping front hatch centered in view",
        "Perform slow 10-degree yaw right to scan sensor array"
      ]
    },
    "images": [
      "./data/images/temp_307_0157_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted orange submersible module and inspect its green-rimmed circular opening for signs of damage or obstruction.",
    "output": {
      "reasoning": "The rusted orange submersible module is directly in front of the robot, centered in the frame, with its green-rimmed circular opening clearly visible and facing the camera. The distance appears moderate—close enough to approach without large maneuvers, but not yet at inspection range. To inspect the opening for damage or obstruction, I need to move forward carefully to get a closer view while maintaining stable orientation. No obstacles block the direct path. Therefore, the first step is to advance straight toward the module until near the opening; then a slight adjustment may be needed to center the opening in the field of view for detailed inspection.",
      "action": [
        "Go straight toward the submersible module",
        "Slow down as you near the green-rimmed opening",
        "Position yourself directly in front of the opening for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0061_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red-and-white cylindrical device suspended by a cable and inspect its lower section near the coral formations.",
    "output": {
      "reasoning": "The red-and-white cylindrical device is suspended centrally in the image, hanging from a cable extending upward toward the surface. Coral formations are visible in the foreground and lower-left, indicating the target inspection zone is near the bottom of the device, close to the corals. The robot appears to be positioned slightly below and to the left of the device (based on perspective and lighting). To approach the lower section safely without disturbing the coral or entangling the cable, I should: (1) move forward slightly to close distance while maintaining clearance from corals; (2) adjust rightward to align with the device’s vertical axis; (3) descend gently to reach the lower portion near the coral base; (4) stabilize and orient for inspection. Avoid upward motion to prevent cable contact.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Descend slowly",
        "Hold position near lower section of device"
      ]
    },
    "images": [
      "./data/images/temp_307_0245_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted submarine hull on the right and inspect the circular hatch for signs of damage or marine growth.",
    "output": {
      "reasoning": "The image shows a rusted submarine hull on the right side of the frame, with a visible circular hatch near its conning tower. The robot is currently positioned slightly left and forward of the hull, with a large pipe structure in the mid-left foreground. To approach the hull and inspect the circular hatch, the robot must first move rightward to align with the hull, then advance straight toward the hatch while maintaining a safe distance to avoid collision with the pipe or hull protrusions. Once near, it should position itself directly in front of the hatch for clear visual inspection.",
      "action": [
        "Turn right",
        "Go straight slowly",
        "Move closer to the circular hatch",
        "Hover in front of the hatch for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0038_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central green-striped cylindrical structure while avoiding the two large textured coral formations on either side.",
    "output": {
      "reasoning": "The image shows a central green-striped cylindrical structure directly ahead, flanked symmetrically by two large textured coral formations (left and right). The robot appears to be positioned on a flat seabed path leading toward the structure. To approach the target while avoiding the corals, the robot should maintain a straight trajectory along the central corridor—no lateral deviation is needed since the path is already clear and centered. The structure is relatively close and well-lit by sun rays, indicating good visibility. Thus, the safest and most direct action is to proceed straight forward.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0190_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green sedan and inspect its front bumper for damage or marine growth.",
    "output": {
      "reasoning": "The green sedan is directly in front of the robot, positioned at a moderate distance (~3–4 meters), with its front bumper clearly visible and facing the camera. The vehicle appears stationary on the seabed, partially covered in marine growth (e.g., algae or barnacles), especially on the hood and bumper. No immediate obstacles block the direct path. To inspect the front bumper closely, I need to: (1) advance straight toward the car to reduce distance, (2) fine-tune position to center the bumper in view, and (3) hold steady for visual inspection. Given the lighting and clarity, a slow approach is safest to avoid disturbing sediment.",
      "action": [
        "Go straight toward the green sedan",
        "Slow down as you near the front bumper",
        "Position yourself 1 meter in front of the bumper, centered",
        "Hold position and inspect the front bumper for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0171_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue underwater vehicle hovering near the rocky reef and inspect its orientation relative to the sunlit water column.",
    "output": {
      "reasoning": "The blue underwater vehicle is visible in the center-left of the image, hovering near a rocky reef. Sunlight penetrates from above, forming a distinct sunlit water column on the right side of the frame. The vehicle is angled slightly upward and toward the light, with its nose pointing roughly toward the upper-right (sunlit zone). To inspect its orientation relative to the sunlit column, I need to: (1) move closer to get a clear view without disturbing it, (2) position myself laterally to observe both the vehicle and the light column in the same field of view, and (3) stabilize for observation. Given current distance (~2–3 m), a straight approach followed by a slight rightward shift will align the vehicle and light column for comparative orientation assessment.",
      "action": [
        "Go straight toward the blue underwater vehicle",
        "Slow down as you near the vehicle",
        "Shift slightly right to align the vehicle and sunlit water column in view",
        "Hold position and observe orientation"
      ]
    },
    "images": [
      "./data/images/temp_307_0169_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured coral formation on the right while maintaining a safe distance from the seabed and avoiding the scattered spherical corals in front.",
    "output": {
      "reasoning": "The robot is currently facing forward, with scattered spherical corals directly in front (near the seabed) and a large textured coral formation visible on the right side of the frame. To approach the large coral on the right while avoiding the spherical corals ahead and maintaining safe altitude above the seabed, I must: (1) first steer right to align toward the target coral, (2) simultaneously ascend slightly to increase clearance from the seabed and spherical corals, and (3) then proceed forward cautiously once clear of obstacles. The initial turn right should be moderate to avoid overshooting; after repositioning, a straight advance with continued altitude monitoring will bring the robot closer to the target formation.",
      "action": [
        "Turn right gently",
        "Ascend slightly",
        "Go straight slowly while monitoring distance to spherical corals and seabed"
      ]
    },
    "images": [
      "./data/images/temp_307_0067_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater structure with the glowing blue ring and inspect its central opening.",
    "output": {
      "reasoning": "The green cylindrical underwater structure with a glowing blue ring is directly in front of the robot, centered in the field of view. Its central opening is visible and appears accessible. To inspect it closely, the robot should first move forward to reduce distance, then fine-tune positioning to center the opening in the camera frame for detailed inspection. No immediate obstacles block the direct path. The current orientation is already aligned with the target, so no turning is needed before advancing.",
      "action": [
        "Go straight",
        "Go straight",
        "Hover and center the central opening in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0244_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large perforated brown sponge in the foreground while avoiding the rocky outcrop to the right.",
    "output": {
      "reasoning": "The large perforated brown sponge is clearly visible in the foreground, centered-left in the image. A rocky outcrop occupies the right side of the frame, partially shadowed and extending vertically — this must be avoided. The robot appears to be positioned slightly behind and above the sponge, with open water/sandy bottom between it and the target. To approach safely: (1) First, steer left to clear the right-side rocky outcrop; (2) Then proceed straight toward the sponge, maintaining a safe lateral offset from the rock; (3) Finally, close in gently on the sponge without veering right into the obstacle. No immediate upward/downward motion is needed—the sponge lies on the seabed at similar depth.",
      "action": [
        "Turn left slightly",
        "Go straight toward the sponge",
        "Approach the sponge carefully"
      ]
    },
    "images": [
      "./data/images/temp_307_0041_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the left fluted column and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows two fluted columns: one prominently on the left (closer, larger in frame) and another on the right (farther, partially visible). The user instruction specifies approaching the *left* fluted column. From the robot’s current perspective, the left column is already in front-left view, but not yet close enough for surface inspection. To approach it safely and align for inspection, the robot should first move forward slightly while maintaining orientation toward the left column, then possibly adjust laterally if needed. Given the clear path and no immediate obstacles in front-left, a direct forward motion toward the left column is appropriate, followed by a slight leftward correction to center on it for inspection.",
      "action": [
        "Move forward",
        "Slightly turn left",
        "Approach the left fluted column closely",
        "Inspect its surface for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0125_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater robot and inspect its front sensor array and blue illuminated indicator light.",
    "output": {
      "reasoning": "The green cylindrical underwater robot is clearly visible in the center-right of the image, angled slightly upward and tilted toward the viewer. Its front sensor array (a rectangular panel with a circular orange element and surrounding markings) and blue illuminated indicator light (a bright blue circular LED on a black band) are both visible from the current perspective. To properly inspect these features, the robot must approach closer while maintaining a stable frontal view—ideally aligning directly in front of the target at a short distance (~1–2 m) to ensure high-resolution observation without occlusion. The seabed terrain appears uneven but navigable; no immediate obstacles block a direct approach path. Therefore, the plan is: (1) move forward steadily toward the robot’s front face, (2) fine-tune lateral position to center the sensor array in the field of view, (3) stop at optimal inspection distance, and (4) hold position for visual inspection.",
      "action": [
        "Go straight toward the green cylindrical robot",
        "Slightly adjust left to center the front sensor array in view",
        "Continue advancing slowly until ~1 meter from the robot",
        "Hold position and stabilize orientation for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0242_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral formation in the center while avoiding the tall green seaweed on the right.",
    "output": {
      "reasoning": "The large green ridged coral formation is centrally located in the image, slightly to the left of center and ahead. Tall green seaweed is visible on the right side of the frame, extending vertically and occupying the right foreground/midground. To approach the coral while avoiding the seaweed, the robot should first steer slightly left to create clearance from the seaweed, then proceed straight toward the central coral formation. No immediate obstacles block the path forward once positioned left of center.",
      "action": [
        "Turn left slightly",
        "Go straight toward the central green ridged coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0001_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow-lit underwater vehicle near the coiled pipes and inspect its connection to the pipeline.",
    "output": {
      "reasoning": "From the image, a yellow-lit underwater vehicle is visible in the upper-right quadrant, positioned near coiled pipes that run diagonally across the scene. The vehicle appears to be connected to a pipeline extending from the left side. To approach it safely and inspect the connection, I need to: (1) navigate toward the vehicle while avoiding the coiled pipes on the left and right; (2) align with the vehicle’s position—likely requiring a slight right turn since it's to the right of center; (3) move forward carefully to get within inspection range without disturbing sediment or obstructing view; (4) stabilize and orient for close visual inspection of the connection point between vehicle and pipeline. The lighting source above may cause glare, so maintaining optimal distance and angle is important.",
      "action": [
        "Turn right slightly",
        "Go straight toward the yellow-lit vehicle",
        "Slow down as you near the vehicle",
        "Position yourself parallel to the vehicle-pipeline connection",
        "Inspect the connection point visually"
      ]
    },
    "images": [
      "./data/images/temp_307_0297_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical underwater vehicle positioned vertically in the center, maintaining a safe distance while aligning with its front-facing light source.",
    "output": {
      "reasoning": "The green cylindrical underwater vehicle is clearly visible in the center of the image, oriented vertically. Its front-facing light source (a bright yellow circular light) is illuminated and serves as the alignment target. The robot appears to be positioned slightly below and in front of the vehicle, with clear open water ahead—no immediate obstacles block the path. To approach while maintaining a safe distance and aligning with the front light, the robot should: (1) move forward cautiously to reduce range without getting too close; (2) adjust horizontally (if needed) to center the light in the field of view; (3) fine-tune pitch/depth to match the light’s vertical position. Since the light is already centered in the image and the vehicle is directly ahead, minimal lateral correction is needed—primarily forward motion with slight depth stabilization.",
      "action": [
        "Go straight slowly",
        "Maintain depth to keep the front light centered vertically",
        "Slightly adjust left/right if the light drifts off-center"
      ]
    },
    "images": [
      "./data/images/temp_307_0018_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater vehicle lying on the seabed near the coral formations and inspect its surface for damage or marine growth.",
    "output": {
      "reasoning": "The cylindrical underwater vehicle is lying horizontally on the seabed, partially surrounded by coral formations. It is positioned in the mid-to-lower center of the image, with clear visibility from the current robot perspective. To approach and inspect its surface, I need to: (1) move forward toward the vehicle while maintaining safe clearance from nearby corals; (2) position the robot parallel to the vehicle’s longitudinal axis for optimal inspection; (3) perform a slow lateral pass along its side to observe surface condition (damage/marine growth). The upper vehicle appears to be hovering or suspended—likely not the target—so focus remains on the seabed-lying cylinder. No immediate obstacles block a direct forward path, but careful navigation near corals is required.",
      "action": [
        "Go straight slowly toward the cylindrical vehicle on the seabed",
        "Maintain slight left offset to avoid coral on the right",
        "Stop when ~1 meter away from the vehicle’s midsection",
        "Turn right slightly to align parallel with the vehicle’s long axis",
        "Move laterally left along the vehicle’s side at constant distance for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0012_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the illuminated coral formation directly ahead while maintaining a safe distance from the surrounding textured coral structures.",
    "output": {
      "reasoning": "The image shows an illuminated coral formation directly ahead, centered in the field of view and highlighted by a downward beam of light (likely from the robot’s own light source). Surrounding it are textured, darker coral structures on both left and right sides, which must be avoided to maintain safety. The robot appears to be positioned at a moderate distance, with clear forward visibility. To approach the target while avoiding obstacles, the plan is: (1) proceed straight cautiously toward the illuminated coral; (2) monitor lateral proximity to textured corals and make minor course corrections if needed; (3) stop at a safe distance before contact. Since the target is directly ahead and no immediate obstruction blocks the path, the initial action is to go straight while maintaining orientation.",
      "action": [
        "Go straight",
        "Maintain heading toward illuminated coral",
        "Adjust slightly left or right only if nearing textured corals"
      ]
    },
    "images": [
      "./data/images/temp_307_0144_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large corroded pipeline and inspect the green biofouling patches along its midsection.",
    "output": {
      "reasoning": "The image shows a large, horizontally oriented corroded pipeline extending from left to right, with visible green biofouling patches along its midsection (center-left portion of the pipe). The robot appears to be positioned slightly above and to the right of the pipeline, at a moderate distance. To approach and inspect the green biofouling patches on the midsection, the robot should first move forward toward the pipeline while maintaining alignment with its axis, then adjust laterally (leftward) to center on the target patches, and finally close in for detailed inspection without colliding. The seabed is relatively clear, so no major obstacles impede direct approach.",
      "action": [
        "Go straight toward the pipeline",
        "Move left to align with the green biofouling patches on the midsection",
        "Slowly advance to inspect the green patches closely"
      ]
    },
    "images": [
      "./data/images/temp_307_0200_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue underwater vehicle and inspect its tether connection near the seabed.",
    "output": {
      "reasoning": "The blue underwater vehicle is visible in the center of the image, upright on the seabed with its tether trailing downward and slightly to the right. The robot appears to be positioned at a moderate distance (~2–3 m) directly in front of it. To inspect the tether connection near the seabed, I need to: (1) move forward to reduce distance safely, (2) adjust laterally (right) to align with the tether’s base, and (3) descend slightly to get a close-up view of the connection point near the seabed. Obstacles (rocks, coral) are present on both sides but leave a clear path forward and slight rightward maneuver.",
      "action": [
        "Go straight",
        "Turn right slightly",
        "Descend slowly",
        "Approach the tether connection point near the seabed"
      ]
    },
    "images": [
      "./data/images/temp_307_0101_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow and black patterned pipe on the right while maintaining a safe distance from the blue-and-yellow autonomous underwater vehicle on the left.",
    "output": {
      "reasoning": "The image shows the robot's current view: a yellow-and-black patterned pipe is on the right side of the frame, extending forward; a blue-and-yellow autonomous underwater vehicle (AUV) is on the left, parallel to the pipe and slightly ahead. To approach the pipe while avoiding the AUV, the robot should first steer gently right to align toward the pipe, then proceed forward while maintaining lateral separation from the AUV. Since the AUV is on the left and the pipe is on the right, a slight rightward heading adjustment followed by straight motion will safely close distance to the pipe without encroaching on the AUV’s space. No immediate obstacle blocks the path, and the seabed provides visual reference for stable navigation.",
      "action": [
        "Turn slightly right",
        "Go straight while keeping the blue-and-yellow AUV visible on the left",
        "Continue approaching the yellow-and-black pipe"
      ]
    },
    "images": [
      "./data/images/temp_307_0068_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical submersible vehicle lying on the seabed and inspect its front hatch and side panels for damage or marine growth.",
    "output": {
      "reasoning": "The green cylindrical submersible is lying on the seabed, angled slightly with its front hatch (with visible portholes and a rectangular panel) facing toward the upper-right of the image. The robot appears to be positioned at a moderate distance (~2–3 m) and slightly to the left-front of the vehicle. To inspect the front hatch and side panels, I need to: (1) approach steadily while maintaining a safe distance to avoid disturbing sediment; (2) align with the front of the vehicle to get a clear view of the hatch; (3) then move laterally along the side to inspect side panels. Given the current perspective, the first step is to go straight toward the vehicle’s front section, then adjust rightward to center on the hatch.",
      "action": [
        "Go straight toward the green submersible",
        "Turn slightly right to align with the front hatch",
        "Move forward slowly to inspect the front hatch",
        "Drift right along the side to inspect side panels"
      ]
    },
    "images": [
      "./data/images/temp_307_0051_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange and black netted basket on the seabed and inspect its contents.",
    "output": {
      "reasoning": "The orange and black netted basket is clearly visible on the seabed in the foreground, slightly to the right of center. Sunlight from above illuminates it, confirming its position and accessibility. The robot appears to be positioned a short distance away and slightly above it. To approach and inspect its contents, the robot should first move forward (downward/forward relative to current orientation) to get closer, then possibly adjust laterally to center on the basket opening, and finally stabilize for inspection. Given the basket lies flat on the seabed and its opening faces upward, a direct forward descent with minor right correction will align the robot for optimal viewing.",
      "action": [
        "Go straight forward",
        "Slightly turn right",
        "Descend gently toward the basket",
        "Hover and inspect the contents"
      ]
    },
    "images": [
      "./data/images/temp_307_0024_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central glowing green columnar structure and inspect its base for attached marine growth or debris.",
    "output": {
      "reasoning": "The image shows a prominent central glowing green columnar structure, illuminated and standing vertically. The robot appears to be positioned slightly to the left and at a moderate distance from the structure, with its base visible and surrounded by net-like debris and some marine growth (e.g., mesh entanglements and possible biofouling near the bottom). To inspect the base closely, the robot must first approach the structure directly while maintaining stable orientation. Given the relatively open space around the column, a straight-forward approach is feasible. Once near the base (within ~1–2 m), a slight downward tilt or hover will allow visual inspection of attached growth/debris. No immediate obstacles block the direct path to the base.",
      "action": [
        "Go straight toward the central green column",
        "Slow down as you near the base of the column",
        "Hover and tilt camera downward to inspect the base for marine growth or debris"
      ]
    },
    "images": [
      "./data/images/temp_307_0084_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green cylindrical object on the seabed while avoiding the large porous rock to its right.",
    "output": {
      "reasoning": "The green cylindrical object lies on the seabed in the lower-left portion of the image. A large porous rock is positioned to its right (from the robot's perspective), occupying the mid-right area. To approach the green object while avoiding the rock, the robot should first move forward cautiously toward the green cylinder, then slightly adjust leftward to maintain clearance from the rock. Since the green object is already relatively close and in clear view, a direct but slightly left-biased approach is safe and efficient.",
      "action": [
        "Go straight",
        "Slightly turn left",
        "Continue approaching the green cylindrical object"
      ]
    },
    "images": [
      "./data/images/temp_307_0233_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green truck and inspect its rear cargo area for attached objects or damage.",
    "output": {
      "reasoning": "The green truck is visible in the mid-left portion of the image, partially obscured by a stone column on the right. It is oriented with its rear facing roughly toward the camera, slightly angled to the left. To inspect the rear cargo area, I need to approach from the front or side and then maneuver behind it. Given the column is to the right of the truck, approaching from the left side avoids obstruction. The seabed is relatively clear, allowing forward movement. Step 1: Move forward and slightly left to align with the truck’s left side. Step 2: Continue forward to reach a position parallel to the truck’s rear. Step 3: Turn right to face the rear cargo area directly. Step 4: Move forward slowly to get close enough for detailed inspection.",
      "action": [
        "Move forward and slightly left",
        "Continue forward to align with the truck's rear",
        "Turn right to face the rear cargo area",
        "Move forward slowly for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0226_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large dark rock on the seafloor while maintaining a safe distance from the floating green AUV above.",
    "output": {
      "reasoning": "The image shows a large dark rock on the seafloor to the right of the current robot position (viewed from the robot's perspective). A green AUV is floating above, slightly left and ahead. To approach the rock safely while avoiding the AUV: (1) First, move forward cautiously to close distance to the rock without ascending toward the AUV’s depth; (2) Then, gently turn right to align with the rock’s position; (3) Finally, continue forward at a low altitude to approach the rock while staying well below the AUV’s vertical position (the AUV is near surface, robot is near seafloor—so horizontal maneuvering suffices for safety). The orange object in foreground appears to be part of the robot or a nearby marker; it does not obstruct the path.",
      "action": [
        "Go straight slowly",
        "Turn right slightly",
        "Go straight toward the large dark rock"
      ]
    },
    "images": [
      "./data/images/temp_307_0253_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured rock on the left while avoiding the overhanging pipe above.",
    "output": {
      "reasoning": "The image shows a large textured rock on the left side of the frame, partially illuminated by sun rays. An overhanging pipe (or cylindrical structure) spans horizontally across the upper portion of the scene, directly above the rock and the robot’s likely path. To approach the rock safely, the robot must avoid ascending too high (to prevent collision with the pipe) and instead maintain a lower trajectory while moving leftward toward the rock. The rock is relatively close and visible, so a controlled lateral movement followed by forward advance should suffice. Since the pipe is overhead, staying near the seabed (where smaller rocks and sediment are visible) provides clearance.",
      "action": [
        "Turn left slightly",
        "Descend gently to stay below the pipe",
        "Move forward toward the large textured rock on the left",
        "Stop when near the rock surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0165_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone pillar and inspect its carved surface for inscriptions or damage.",
    "output": {
      "reasoning": "The central stone pillar is clearly visible and positioned directly in front of the robot’s current viewpoint. It stands upright on the seabed, with its carved surface facing the camera. To inspect the surface for inscriptions or damage, the robot must first close the distance to ensure high-resolution visual capture. Given the clear path ahead and no immediate obstacles, the optimal sequence is: (1) move forward toward the pillar until within close inspection range (~1–2 meters), (2) optionally adjust orientation slightly to align the camera perpendicular to the carved face for optimal lighting and detail, and (3) perform a slow circumferential scan or hover to examine all visible sides if needed. Since the instruction specifies 'approach' and 'inspect its carved surface', the initial priority is forward motion to reach proximity.",
      "action": [
        "Go straight toward the central stone pillar",
        "Slow down and stop at ~1.5 meters from the pillar",
        "Adjust heading to face the carved surface directly",
        "Hover and perform a slow vertical scan of the pillar's front face"
      ]
    },
    "images": [
      "./data/images/temp_307_0019_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The submerged stone column is clearly visible in the center of the image, standing vertically on the seabed. Sunlight rays illuminate it from above, enhancing visibility. The robot appears to be positioned at a moderate distance, slightly in front and possibly to the left of the column (based on shadow orientation and perspective). To approach and inspect its surface, the robot should first move forward toward the base of the column, then circle or reposition laterally to examine different facets—especially areas where marine growth (e.g., algae, barnacles) or cracks might be present. Given the clear water and lack of immediate obstacles, a direct approach followed by a close circumferential inspection is optimal.",
      "action": [
        "Go straight toward the stone column",
        "Slow down and stop ~1 meter from the base",
        "Circle clockwise around the column at close range",
        "Pause briefly at each quadrant to inspect surface details",
        "Tilt camera upward to examine upper sections and cap"
      ]
    },
    "images": [
      "./data/images/temp_307_0145_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large ridged green object on the seabed while maintaining a safe distance from the coral structure to its right.",
    "output": {
      "reasoning": "The image shows a large ridged green object (likely the target) lying horizontally on the seabed, oriented roughly left-to-right. To its right is a dense coral structure with fan-shaped corals—this must be avoided per the instruction. The robot appears to be positioned slightly in front and to the left of the green object, with clear space on its left side and ahead. To approach safely while avoiding the coral on the right, the optimal path is to move forward while gently steering left to maintain clearance from the coral. No immediate obstacle blocks direct forward-left motion. A single coordinated action—'Go straight while slightly turning left'—suffices to begin safe approach; fine adjustments can follow in subsequent steps if needed.",
      "action": [
        "Go straight while slightly turning left"
      ]
    },
    "images": [
      "./data/images/temp_307_0032_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine along its starboard side while maintaining a safe distance from the algae-covered seabed and the hull.",
    "output": {
      "reasoning": "The submarine is oriented diagonally in the frame, with its stern (propeller and rudder) visible on the left and bow extending toward the upper right. The starboard side (right side when facing forward) is the side currently visible — it has yellow markings and is adjacent to the algae-covered seabed on the left of the image (which is port side relative to the sub). To approach along the starboard side while avoiding the algae-covered seabed (on the port side of the sub), I must position myself to the right of the submarine’s longitudinal axis. From the current viewpoint, the robot appears slightly ahead and to the port side of the sub (since the sub’s starboard side is facing us, and we see its full starboard hull). To get into a safe parallel trajectory along the starboard side: (1) First, move backward slightly to increase clearance from the hull and reposition; (2) Then turn right (clockwise) to align parallel to the sub’s starboard flank; (3) Finally, advance slowly while maintaining lateral distance to avoid both the hull and the algae bed (which lies on the opposite side, so staying on the starboard side inherently avoids it, but we must not drift left).",
      "action": [
        "Move backward",
        "Turn right",
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0034_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large spiral-shaped coral formation in the center-left while avoiding the shadowed overhang above.",
    "output": {
      "reasoning": "The large spiral-shaped coral formation is clearly visible in the center-left of the image, illuminated by sun rays. Above it, there is a dark, shadowed overhang extending from the upper right toward the center, which must be avoided. To approach the coral safely, I should first move forward slightly to get closer while staying below the overhang’s underside. Then, gently steer left to align with the coral’s position, ensuring the robot remains in the well-lit, open water column and avoids entering the shadowed region directly above the target. The seabed appears relatively flat and obstacle-free in the path toward the coral.",
      "action": [
        "Go straight slowly",
        "Turn left slightly",
        "Go straight toward the spiral coral",
        "Maintain depth to stay below the overhang"
      ]
    },
    "images": [
      "./data/images/temp_307_0296_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the textured vertical pipe structure while avoiding the surrounding coral formations and sediment on the seabed.",
    "output": {
      "reasoning": "The textured vertical pipe is the central target, positioned slightly right of center in the image. Coral formations are visible on the left and near the base of the pipe, and sediment covers the seabed. To approach safely: (1) Start by moving forward cautiously to close distance while staying centered to avoid left-side coral; (2) Slightly adjust rightward as approaching the mid-section to clear the left coral mass; (3) Maintain altitude to avoid disturbing seabed sediment; (4) Final approach straight toward the pipe’s mid-to-upper section, where it’s most visible and unobstructed. The pipe’s strong vertical alignment and light refraction help guide orientation.",
      "action": [
        "Go straight slowly",
        "Slight right turn to center on pipe",
        "Maintain steady altitude",
        "Approach pipe vertically"
      ]
    },
    "images": [
      "./data/images/temp_307_0166_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater device with glowing red lights and inspect its front face for markings or damage.",
    "output": {
      "reasoning": "The cylindrical underwater device with glowing red lights is clearly visible in the center-left of the image, resting on the seabed. Its front face (the circular end with four red lights and central ports) is oriented toward the camera, indicating the robot is already roughly aligned with the target. However, the current distance appears moderate (~2–3 meters), and to inspect markings or damage, the robot must move closer while maintaining frontal alignment. No immediate obstacles block the direct path. Therefore, the plan is: (1) advance straight toward the device to reduce distance, (2) fine-tune position to center the front face in view, (3) halt at close inspection range (~0.5–1 m) for detailed observation.",
      "action": [
        "Go straight toward the cylindrical device",
        "Adjust position slightly to center the front face in view",
        "Stop at close range for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0211_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue segmented pipe on the right and inspect its connection point with the wall.",
    "output": {
      "reasoning": "The blue segmented pipe is visible on the right side of the image, extending diagonally downward and connecting to a vertical wall structure. The robot is currently facing forward with a black pipe in the center-left foreground; the blue pipe lies to its right and slightly ahead. To approach the connection point, the robot must first reorient toward the blue pipe (turn right), then move forward while maintaining alignment with the pipe’s axis, and finally stop near the wall interface for inspection. The lighting from above suggests good visibility along this path, and no immediate obstacles block the route.",
      "action": [
        "Turn right",
        "Go straight toward the blue segmented pipe",
        "Align with the pipe axis",
        "Approach the wall connection point",
        "Stop and inspect the connection"
      ]
    },
    "images": [
      "./data/images/temp_307_0099_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green-banded pipe structure while avoiding entangled yellow ropes and netting on the seafloor.",
    "output": {
      "reasoning": "The large green-banded pipe structure is directly ahead and slightly elevated in the image, serving as the primary target. Entangled yellow ropes and netting are densely spread across the seafloor in the foreground and left side, posing entanglement risks. To safely approach, I must first ascend slightly to clear the immediate netting, then navigate rightward to avoid the dense rope cluster on the left, and finally proceed forward toward the pipe while maintaining clearance from debris below. The robot’s current orientation faces the pipe, so minor lateral correction is needed before advancing.",
      "action": [
        "Ascend slightly",
        "Turn right 15 degrees",
        "Go straight slowly",
        "Maintain altitude to avoid seafloor debris"
      ]
    },
    "images": [
      "./data/images/temp_307_0016_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central stone column and inspect its base for signs of structural damage or marine growth.",
    "output": {
      "reasoning": "The central stone column is clearly visible and positioned slightly to the right of the robot’s current forward view. Sunlight beams from the upper left suggest the robot is approaching from a diagonal front-left angle. To inspect the base, the robot must first close the distance while maintaining a stable approach trajectory—avoiding the chain-linked structure on the left and debris on the right. The optimal path is to move forward while slightly adjusting rightward to center the column in the field of view, then continue straight until near the base (~1–2 m), where a final slight downward tilt or hover may be needed for inspection (though camera pitch is not an explicit action here; we assume visual inspection occurs upon proximity). No immediate obstacles block the direct path to the column’s base.",
      "action": [
        "Move forward",
        "Slightly turn right",
        "Move forward",
        "Move forward"
      ]
    },
    "images": [
      "./data/images/temp_307_0209_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the orange cylindrical object on the seabed while avoiding the surrounding rocks and maintaining a safe distance from the nearby fish.",
    "output": {
      "reasoning": "The orange cylindrical object is clearly visible on the seabed in the foreground, slightly to the right of center. Surrounding it are several rocks: one large dark rock to the right, a textured rounded rock to the left, and smaller rocks scattered nearby. A fish is swimming in the upper-left midwater region, away from the direct path but requiring caution. To approach safely: (1) Start by moving forward slowly toward the orange object while keeping it centered in view; (2) Slightly adjust leftward to avoid the large rock on the right and maintain clearance from the left rock; (3) Monitor the fish’s position—since it’s not directly in the path and appears distant, no evasive maneuver is needed yet, but maintain awareness. The optimal path is a gentle forward-left arc toward the object, staying ~0.5–1 m away from nearest rocks.",
      "action": [
        "Go straight slowly",
        "Slightly turn left",
        "Continue forward while keeping orange object centered",
        "Maintain distance from left and right rocks",
        "Approach orange cylindrical object steadily"
      ]
    },
    "images": [
      "./data/images/temp_307_0059_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Follow the green pipeline along the seabed while maintaining a safe distance from the coral formations on the left.",
    "output": {
      "reasoning": "The image shows a green pipeline extending diagonally from the upper left toward the lower right, lying along the seabed. Coral formations are clearly visible on the left side of the frame, close to the pipeline. To follow the pipeline while maintaining a safe distance from the corals, the robot should position itself slightly to the right of the pipeline’s centerline. The pipeline curves gently to the right in the distance, so the robot must adjust its heading gradually to stay aligned. Initial action: move forward while slightly veering right to establish a safe lateral offset from the left-side corals. Subsequent actions will involve minor course corrections to track the pipeline’s curvature.",
      "action": [
        "Go straight while slightly turning right",
        "Maintain alignment with the green pipeline",
        "Adjust rightward as the pipeline curves right"
      ]
    },
    "images": [
      "./data/images/temp_307_0160_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red bucket suspended by the net and inspect its contents.",
    "output": {
      "reasoning": "The red bucket is suspended centrally in the image, hanging from a net attached to a vertical structure above. It is clearly visible and positioned slightly ahead and to the left of the robot’s current viewpoint (based on perspective and lighting). To approach it, the robot should first move forward toward the bucket while maintaining alignment. As it gets closer, a slight leftward adjustment may be needed to center the bucket in view. Once near, it should stop and orient itself for inspection—likely by positioning directly in front and possibly tilting the camera downward to peer into the bucket. No immediate obstacles block the direct path, though the blue object on the seabed and the right-side cage should be avoided by staying centered.",
      "action": [
        "Go straight toward the red bucket",
        "Slightly turn left to align with the bucket",
        "Go straight until close to the bucket",
        "Stop and position directly in front of the bucket",
        "Tilt camera downward to inspect contents"
      ]
    },
    "images": [
      "./data/images/temp_307_0173_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green vehicle and inspect its front-right side for structural damage or marine growth.",
    "output": {
      "reasoning": "The submerged green vehicle is visible in the center-right of the image, lying on the seabed at a slight angle. Its front-right side (from the robot’s perspective) is partially exposed and accessible, with visible marine growth and possible structural damage (e.g., dents, missing panels). The robot appears to be positioned slightly behind and to the left of the vehicle. To approach and inspect the front-right side, the robot should: (1) move forward toward the vehicle while maintaining clearance from nearby coral (left) and sediment; (2) adjust rightward to align with the front-right quadrant; (3) close in for detailed inspection without disturbing sediment or contacting obstacles.",
      "action": [
        "Go straight toward the green vehicle",
        "Turn slightly right to align with the front-right side",
        "Move forward slowly to inspect the front-right side closely"
      ]
    },
    "images": [
      "./data/images/temp_307_0223_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red spherical object with circular indentations on the right while avoiding the nearby coral formations.",
    "output": {
      "reasoning": "The red spherical object with circular indentations is visible on the right side of the image, partially surrounded by coral formations (dark, textured structures with ridges and openings). The robot is currently positioned left-center, facing forward. To approach the target while avoiding corals: first, the robot must steer right to align toward the red sphere; second, it should maintain a safe lateral distance from the nearby coral on the right (visible just left of the sphere); third, proceed forward cautiously as the sphere is relatively close but obstructed slightly by foreground coral. No immediate backward or sharp turns are needed—gradual right turn followed by straight advance suffices.",
      "action": [
        "Turn right slightly",
        "Go straight slowly",
        "Maintain slight rightward offset to avoid coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0141_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submarine visible through the cave opening while avoiding the coral formations on the seabed.",
    "output": {
      "reasoning": "The submarine is visible through the cave opening ahead and slightly above the current viewpoint. The seabed in the foreground contains prominent coral formations (especially a large dark one at bottom center and others to the left), which must be avoided. To approach the submarine safely, the robot should first ascend slightly to clear the near-field corals, then proceed forward toward the cave opening while maintaining a central path to avoid lateral corals. Given the lighting and perspective, the submarine lies directly ahead beyond the cave threshold, so a straight-forward trajectory with minor upward adjustment is optimal.",
      "action": [
        "Ascend slightly",
        "Go straight toward the cave opening",
        "Maintain central path to avoid left and right corals"
      ]
    },
    "images": [
      "./data/images/temp_307_0107_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue submarine resting on the rocky seabed and inspect its rear thruster area for damage or debris.",
    "output": {
      "reasoning": "The blue submarine is visible resting on the rocky seabed, oriented diagonally with its rear (thruster end) pointing toward the lower-left of the image. Thrusters are located at the tail end, identifiable by the cylindrical housings and exhaust plume trailing upward from the rear. To inspect the rear thruster area, I must first approach the submarine from a safe angle—ideally from the side or slightly behind—to avoid disturbing sediment or colliding with rocks. Given current perspective, the robot is likely ahead and slightly above the sub. The optimal path is: (1) move forward cautiously to close distance while maintaining clearance from rocks; (2) reposition to the submarine’s starboard (right) side to get lateral access; (3) maneuver aft (toward the tail) along the side to align with the rear thruster zone; (4) stabilize and orient camera toward the thrusters for inspection. No immediate obstacles block this path, but careful navigation near the rocky base is required.",
      "action": [
        "Go straight slowly to approach the submarine",
        "Turn right to position alongside the submarine's starboard side",
        "Move aft (backward relative to submarine orientation) to reach the rear thruster area",
        "Hover and adjust orientation to face the rear thrusters for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0240_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central corroded column and inspect the circular openings along its shaft for signs of marine growth or structural damage.",
    "output": {
      "reasoning": "The central corroded column is clearly visible in the image, positioned directly ahead of the robot's current viewpoint. It features multiple circular openings along its shaft, some of which appear partially obscured by marine growth (e.g., the cluster of greenish-yellow organisms near the top-right opening). To inspect these openings for marine growth or structural damage, the robot must first approach the column to reduce distance and improve visual resolution. Given the column’s central placement and the relatively clear path forward (no immediate obstacles in front), the optimal first step is to move straight toward it. Once closer, fine positioning (e.g., slight lateral adjustments or hovering) would be needed for detailed inspection—but per the instruction, the immediate macro-step is *approach*. The image shows no red obstacles or hazards requiring avoidance in the direct path.",
      "action": [
        "Go straight toward the central corroded column"
      ]
    },
    "images": [
      "./data/images/temp_307_0255_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged coral structure in the center while maintaining a safe distance from the surrounding smaller corals and sponges.",
    "output": {
      "reasoning": "The large green ridged coral structure is clearly centered in the image and dominates the scene. Smaller corals and sponges are scattered around its base and periphery—specifically, a yellow spiky sponge in the lower right, a purple-spotted sponge nearby, and several smaller green corals to the left and front. To approach safely, I must avoid lateral drift toward these obstacles. The robot appears to be positioned slightly in front and below the main structure, with open water above and ahead. A controlled forward advance with slight upward pitch will bring the robot closer while preserving clearance from the surrounding benthic elements. No immediate turn is needed since the target is directly ahead and centered.",
      "action": [
        "Go straight slowly",
        "Maintain slight upward pitch to stay above seabed obstacles",
        "Monitor left and right for proximity to smaller corals and sponges"
      ]
    },
    "images": [
      "./data/images/temp_307_0039_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green car and inspect its open driver-side door for potential artifacts or structural damage.",
    "output": {
      "reasoning": "The green submerged car is clearly visible in the center-left of the image, resting on the seabed. Its driver-side door is open and facing toward the camera's current position, suggesting it is accessible from the front-left approach angle. To inspect the open driver-side door for artifacts or damage, I need to: (1) move forward toward the car while maintaining a safe distance to avoid disturbing sediment; (2) adjust laterally to align with the open door (which appears on the left side of the vehicle from our perspective); (3) close in for a detailed visual inspection. The seabed is relatively clear, and no immediate obstacles block the path. Sunlight from above provides good illumination, aiding visibility.",
      "action": [
        "Go straight toward the green car",
        "Move slightly left to align with the open driver-side door",
        "Approach closer to inspect the interior and door frame for artifacts or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0186_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green giant clam on the seabed while maintaining a safe distance to avoid disturbing it.",
    "output": {
      "reasoning": "The image shows a large green giant clam centered on the seabed, with its mantle fully extended. The robot appears to be positioned slightly above and in front of it (based on perspective and lighting). To approach while maintaining a safe distance, we should avoid sudden movements or descending too close—instead, proceed slowly and horizontally toward it from the current position. Since the clam is directly ahead and occupies most of the frame, a gentle forward motion (without descending) will bring the robot closer while keeping clearance. No immediate obstacles are present in the direct path, but small sea urchins are nearby; thus, maintain slight lateral clearance if needed. The safest initial step is to move straight forward at a moderate pace, monitoring proximity.",
      "action": [
        "Go straight slowly"
      ]
    },
    "images": [
      "./data/images/temp_307_0133_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the blue cylindrical structure with yellow bands and inspect its midsection for attached sensors or biofouling.",
    "output": {
      "reasoning": "The blue cylindrical structure with yellow bands is directly in front of the robot and dominates the central field of view. It appears vertically oriented and stationary, resting on the seabed. The midsection (roughly middle third of the cylinder) is visible but not yet close enough for detailed inspection of sensors or biofouling. To inspect it properly, the robot must first approach closer while maintaining a stable frontal orientation. No immediate obstacles block the direct path forward. Therefore, the plan is: (1) move straight toward the structure to reduce distance, (2) position at optimal inspection range (~1–2 m), (3) stabilize and possibly adjust yaw/pitch slightly to center the midsection in frame for visual inspection.",
      "action": [
        "Go straight toward the blue cylindrical structure",
        "Slow down as you near the structure",
        "Position at ~1.5 meters distance facing the midsection",
        "Stabilize and adjust heading to center the midsection in view"
      ]
    },
    "images": [
      "./data/images/temp_307_0049_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate upward along the central light beam while avoiding the algae-covered rock walls on both sides.",
    "output": {
      "reasoning": "The image shows a narrow underwater passage with algae-covered rock walls on left and right. A strong vertical light beam illuminates the center, suggesting an upward path toward the surface or open water. The robot is positioned near the bottom of this shaft, with bubbles rising along the central axis—indicating upward motion is feasible and aligned with the light. To avoid collisions, the robot must stay centered within the light beam, maintaining equal clearance from both algae-covered walls. Since the passage appears relatively straight vertically, a sequence of controlled upward movements with minor lateral corrections (if needed) will suffice. No immediate turns are required; the priority is steady ascent while monitoring proximity to walls.",
      "action": [
        "Go straight upward along the central light beam",
        "Maintain center alignment to avoid left and right algae-covered walls",
        "Continue ascending steadily"
      ]
    },
    "images": [
      "./data/images/temp_307_0013_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the stone pillar structure and inspect its base for signs of marine growth or structural damage.",
    "output": {
      "reasoning": "The stone pillar structure is clearly visible in the center-right of the image, standing vertically on the seabed. Sunlight rays illuminate it from above and left, indicating good visibility. The base of the pillar is partially visible but not yet close enough for detailed inspection of marine growth or damage. To inspect the base, I need to approach it directly while maintaining a stable orientation. Given its central position and clear path (no immediate obstacles in front), the optimal first step is to move straight toward the pillar. Once closer, I can circle or adjust position to examine the base thoroughly—but the immediate atomic action is forward motion toward the target.",
      "action": [
        "Go straight toward the stone pillar"
      ]
    },
    "images": [
      "./data/images/temp_307_0081_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cluster of green mushroom-shaped corals in the center foreground while avoiding the large blue-striped shell on the left.",
    "output": {
      "reasoning": "The image shows a cluster of green mushroom-shaped corals in the center foreground, which is the target. To the left, there is a large blue-striped shell (likely a giant clam) that must be avoided. The robot is currently positioned such that the corals are directly ahead and slightly to the right of center, while the shell occupies the left midground. To safely approach the corals, the robot should first steer slightly right to clear the shell’s position, then proceed straight toward the coral cluster. No immediate obstacles lie directly in the central path once the left-side shell is bypassed.",
      "action": [
        "Turn slightly right",
        "Go straight toward the green mushroom-shaped corals"
      ]
    },
    "images": [
      "./data/images/temp_307_0079_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the weathered vertical column and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The image shows a weathered vertical column centered in the frame, illuminated by sunlight from above. The robot appears to be positioned at a moderate distance (~2–3 m) directly in front of the column, with clear visibility of its surface. To inspect for marine growth or structural damage, the robot must first approach closer (within ~0.5–1 m) to resolve fine details, then possibly circle or tilt to examine different facets. Given the column is straight ahead and no immediate obstacles block the path, the optimal initial action is to move forward steadily while maintaining alignment with the column’s axis.",
      "action": [
        "Go straight toward the column",
        "Slow down as you near the column (within 1 meter)",
        "Hover and rotate slowly clockwise to inspect the surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0155_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the coiled yellow rope on the seafloor while avoiding the large submerged pipe and netting overhead.",
    "output": {
      "reasoning": "The coiled yellow rope is visible on the seafloor in the lower-left foreground. A large submerged pipe runs diagonally across the midground, and netting hangs overhead (especially above and behind the pipe). To approach the rope safely: (1) The robot must stay low to avoid the overhead netting; (2) It should navigate *around* the pipe—since the rope lies to the left of the pipe’s near end, approaching from the left side avoids collision; (3) The current view suggests the robot is slightly right and rearward of the rope, so initial movement should be leftward and slightly forward, then fine-tune toward the rope while maintaining clearance from the pipe’s left edge. No immediate obstacles block the direct left-forward path to the rope.",
      "action": [
        "Turn left",
        "Go straight slowly",
        "Move forward toward the coiled yellow rope",
        "Maintain altitude low to avoid overhead netting",
        "Adjust left to pass clear of the pipe’s left edge"
      ]
    },
    "images": [
      "./data/images/temp_307_0089_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green sponge-like structure in the center while avoiding the coral formations on the right and left.",
    "output": {
      "reasoning": "The large green sponge-like structure is centrally located and appears to be the main target. Coral formations are visible on both left (tall, dark, textured) and right (branching, yellowish) sides, forming a narrow corridor. The robot is currently positioned slightly below and in front of the sponge, with clear forward path but requiring lateral correction to stay centered and avoid corals. To safely approach: first, adjust heading slightly right to center relative to the sponge (since left coral is closer and more prominent), then proceed straight toward the sponge while maintaining central alignment. No immediate sharp turns needed—small corrections suffice.",
      "action": [
        "Turn slightly right",
        "Go straight toward the green sponge-like structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0002_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged car and inspect its front right side for damage or marine growth.",
    "output": {
      "reasoning": "The submerged car is visible in the lower-right portion of the image, oriented diagonally with its front facing roughly toward the upper-left. The front right side (from the car's perspective) corresponds to the side closest to the camera and slightly to the right — i.e., the visible right fender and headlight area in the image. To inspect it closely, I need to: (1) move forward toward the car while maintaining a safe distance initially; (2) adjust position to align with the front right quadrant; (3) close in for detailed visual inspection. There are no immediate obstacles blocking the path, though stone structures are nearby—so lateral movement must be cautious. Given the current view, the most direct approach is to advance straight, then slightly right to position for inspection.",
      "action": [
        "Go straight toward the submerged car",
        "Slightly turn right to align with the car's front right side",
        "Move closer to inspect the front right fender and headlight area"
      ]
    },
    "images": [
      "./data/images/temp_307_0074_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large textured cylindrical structure on the right while avoiding the foreground sea anemones and the central vertical pipe.",
    "output": {
      "reasoning": "The image shows a large textured cylindrical structure on the right side of the frame, which is the target. In the foreground (bottom-left and center), there are sea anemones with distinct ridged textures that must be avoided. A central vertical pipe runs upward near the center, also to be avoided. The robot appears to be positioned slightly left and front of the target cylinder. To approach safely: first, move rightward to clear the foreground anemones and align laterally with the cylinder; then, proceed forward while maintaining distance from the central pipe; finally, close in on the cylinder’s side, ensuring no collision with nearby obstacles.",
      "action": [
        "Turn right slightly",
        "Move forward cautiously",
        "Adjust left to avoid central pipe",
        "Continue forward toward the cylindrical structure",
        "Approach the side of the large textured cylinder"
      ]
    },
    "images": [
      "./data/images/temp_307_0256_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large teal cylindrical structure with visible portholes and inspect its base where it connects to the seabed pipeline.",
    "output": {
      "reasoning": "The image shows a large teal cylindrical structure with visible portholes, standing vertically and connected at its base to a horizontal pipeline lying on the seabed. The robot appears to be positioned slightly in front and below the structure, with the base and pipeline clearly visible in the lower part of the frame. To inspect the base connection, the robot must move closer while maintaining a stable orientation to avoid disturbing sediment or losing visual clarity. Given the current perspective, the optimal path is to advance straight toward the base, then possibly adjust laterally for a better inspection angle if needed. Since the instruction specifies approaching and inspecting the base connection, the first step is to go straight toward the structure’s lower section.",
      "action": [
        "Go straight toward the base of the teal cylindrical structure"
      ]
    },
    "images": [
      "./data/images/temp_307_0037_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central cylindrical structure and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The image shows a central cylindrical structure (likely a pilings or column) surrounded by other debris and smaller structures on the seabed. Sunlight penetrates from above, illuminating the scene. The robot appears to be positioned slightly left and front of the target structure, at moderate distance (~3–5 m). To approach and inspect its surface, the robot should first move forward toward the structure while maintaining alignment with its center. As it nears (~1–2 m), it should adjust laterally (slightly right) to position itself parallel to the cylinder for optimal visual inspection. Finally, it should circle or hold position to scan the surface for marine growth or damage.",
      "action": [
        "Go straight toward the central cylindrical structure",
        "Adjust slightly right to align parallel to the cylinder",
        "Move closer until ~1 meter away",
        "Hold position and slowly rotate clockwise to inspect the surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0286_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged pipe structure and inspect the flexible blue hose connected to its side for signs of damage or detachment.",
    "output": {
      "reasoning": "The image shows a large submerged pipe structure lying horizontally on the seabed, with a flexible blue hose connected to its side (visible on the right end of the pipe). The robot is currently positioned slightly above and to the left of the pipe, with clear visibility of the hose connection point. To inspect the hose for damage or detachment, the robot must first approach the pipe structure closely, then maneuver to position itself parallel and near the hose connection—ideally at a slight angle to allow visual inspection of the joint and hose surface. Given the current view, the optimal path is: (1) move forward toward the pipe to reduce distance, (2) adjust rightward to align with the hose connection zone, (3) fine-tune position to stabilize for close inspection.",
      "action": [
        "Go straight toward the pipe structure",
        "Turn right slightly to align with the blue hose connection",
        "Move forward slowly to get closer to the hose",
        "Hold position for visual inspection of the hose and its attachment point"
      ]
    },
    "images": [
      "./data/images/temp_307_0075_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Navigate toward the bright light source at the center of the circular tunnel while maintaining a safe distance from the textured inner walls.",
    "output": {
      "reasoning": "The image shows a circular tunnel with a bright light source at the center, indicating the forward direction of travel. The inner walls are highly textured and patterned, suggesting potential hazards or turbulence if too close; therefore, maintaining central alignment is critical for safety. The robot appears to be positioned near the tunnel axis, facing the light. To navigate safely: (1) Keep heading directly toward the light (i.e., straight ahead), as it marks the tunnel’s centerline; (2) Slightly adjust left/right as needed to avoid drifting toward the textured walls—however, since no immediate wall proximity is observed in the current frame, initial action is to proceed straight while monitoring lateral deviation. No turns are required yet; sustained forward motion with minor course correction suffices.",
      "action": [
        "Go straight",
        "Maintain center alignment toward the light",
        "Monitor distance to walls and correct laterally if drifting"
      ]
    },
    "images": [
      "./data/images/temp_307_0110_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green and orange buoy structure and inspect the circular opening near its midsection.",
    "output": {
      "reasoning": "The image shows a green-and-orange buoy structure directly in front of the robot, with a clearly visible circular opening near its midsection (green section). The robot is already aligned with the buoy and at moderate distance. To inspect the circular opening closely, the next steps should be: (1) move forward to reduce distance while maintaining alignment, (2) fine-tune position to center the opening in view, and (3) stabilize for inspection. No obstacles block the direct path.",
      "action": [
        "Go straight toward the buoy",
        "Adjust position to center the circular opening in view",
        "Hold position for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0213_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large brain coral and inspect its central opening for signs of marine life or debris.",
    "output": {
      "reasoning": "The large brain coral is directly in front of the robot, centered in the image, with its central opening clearly visible and facing the camera. The robot is at a moderate distance—close enough to identify features but not yet in inspection range. To inspect the central opening for marine life or debris, the robot should first move forward steadily to reduce distance without disturbing sediment. Once near, a slight upward or lateral adjustment may be needed to align the camera with the opening’s interior, but the initial priority is forward approach. No obstacles block the direct path; a small fish is to the left but poses no interference.",
      "action": [
        "Go straight toward the brain coral",
        "Slow down as you near the central opening",
        "Position camera to center on the opening for close inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0197_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green brain coral in the center while avoiding the surrounding smaller corals and sponges.",
    "output": {
      "reasoning": "The large green brain coral is clearly centered in the image and serves as the primary target. Surrounding it are smaller corals (yellow-brown brain-type on the left) and sponges (e.g., green ring-shaped near bottom right). To approach safely, I must navigate through a relatively clear path—observing that the space directly in front of the large coral (slightly lower-center) appears less cluttered. The robot should first align toward the center, then advance cautiously while monitoring proximity to nearby obstacles. Since the large coral dominates the central field, a straight-forward approach with minor lateral correction is optimal. No immediate collision risk is visible in the direct forward path, but small protrusions near the base suggest slow, controlled movement is needed.",
      "action": [
        "Turn slightly left to center the large green brain coral in the field of view",
        "Move forward slowly",
        "Maintain slight rightward offset to avoid the yellow brain corals on the left",
        "Continue forward until within 1 meter of the large green brain coral"
      ]
    },
    "images": [
      "./data/images/temp_307_0246_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green ancient tower while avoiding the large rock archway on the left.",
    "output": {
      "reasoning": "The glowing green ancient tower is the target, positioned centrally-right in the scene. A large rock archway dominates the left side, forming a potential collision hazard if the robot proceeds straight or drifts left. The open seafloor path to the right of the archway provides a safe corridor toward the tower. To avoid the archway while approaching the tower, the robot should first steer slightly right to clear the archway’s outer edge, then proceed straight toward the tower, maintaining a safe lateral offset from the left obstacle.",
      "action": [
        "Turn right slightly",
        "Go straight toward the glowing green tower",
        "Maintain slight rightward offset to avoid the rock archway"
      ]
    },
    "images": [
      "./data/images/temp_307_0025_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the small blue cylindrical object resting on the sandy seabed near the coral cluster.",
    "output": {
      "reasoning": "From the image, a small blue cylindrical object is visible on the sandy seabed, positioned slightly left of center and in the mid-ground, near a coral cluster to its right. The robot appears to be at a higher elevation and farther back (given perspective and light rays). To approach it safely: first, descend gently toward the seabed to reduce altitude; then move forward while maintaining orientation toward the object; finally, fine-tune lateral position to align directly with the cylinder, avoiding nearby coral.",
      "action": [
        "Descend slowly",
        "Move forward toward the blue cylindrical object",
        "Adjust left/right position to center on the object",
        "Approach closely without disturbing sediment"
      ]
    },
    "images": [
      "./data/images/temp_307_0069_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central blue dome-shaped coral while maintaining a safe distance from the surrounding yellow spiky corals.",
    "output": {
      "reasoning": "The central blue dome-shaped coral is clearly visible in the image, positioned slightly ahead and centered in the field of view. Surrounding it are several yellow spiky corals—especially prominent in the lower-left and left mid-ground—that pose collision risk if approached directly. To maintain a safe distance, I should first align the robot to face the blue coral directly (ensuring no immediate obstruction), then advance cautiously while monitoring lateral proximity to yellow corals. Given the layout, a slight rightward offset may help avoid the densest yellow cluster on the left. The path appears relatively clear forward from current position, so a controlled forward motion with minor heading correction is feasible.",
      "action": [
        "Turn slightly right to center the blue dome-shaped coral in the view",
        "Move forward slowly while keeping the blue coral centered",
        "Maintain at least one robot-width distance from yellow spiky corals on the left"
      ]
    },
    "images": [
      "./data/images/temp_307_0217_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large central coral formation with bulbous, textured polyps and inspect its surface for signs of marine life or anomalies.",
    "output": {
      "reasoning": "The image shows a large central coral formation with distinctive bulbous, textured polyps, clearly the target. It is positioned directly ahead and slightly elevated relative to the robot’s current viewpoint. The surrounding seabed is relatively clear, with smaller coral patches on either side but no immediate obstacles blocking a direct approach. To inspect its surface closely for marine life or anomalies, I need to: (1) move forward steadily toward the base of the formation, (2) adjust altitude/position to bring the camera closer to the polyp surfaces without disturbing sediment, and (3) perform a slow circumferential scan to examine multiple facets. Given the lighting from above, approaching from below-front will maximize visibility of texture details.",
      "action": [
        "Go straight toward the central coral formation",
        "Slowly ascend while maintaining forward motion to reach mid-height of the coral",
        "Hover gently 0.5 meters in front of the lower bulbous section",
        "Perform a slow clockwise rotation to inspect the surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0294_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the vertical green pipe structure while avoiding the surrounding coral formations and the yellow ring-shaped object on the seabed.",
    "output": {
      "reasoning": "The vertical green pipe structure is directly ahead and centrally located in the image, illuminated by overhead light rays. Coral formations are visible on the left and right sides near the seabed, and a yellow ring-shaped object lies on the seabed to the right of the pipe (near the lower-right quadrant). To approach the pipe safely, the robot must maintain a central trajectory, avoiding lateral deviations toward the corals or the yellow ring. Since the pipe is already aligned with the robot’s current heading, the immediate action is to advance straight while monitoring proximity to obstacles. No turning is needed initially; slight course corrections may be required later, but the first step is forward motion along the clear central path.",
      "action": [
        "Go straight"
      ]
    },
    "images": [
      "./data/images/temp_307_0017_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged military vehicle behind the central stone pillar and inspect its front right wheel assembly.",
    "output": {
      "reasoning": "The image shows a submerged military vehicle located behind a central stone pillar. The vehicle is partially visible to the right of the pillar from the current viewpoint. To approach the vehicle, I must first navigate around the pillar—likely to its right side, since the vehicle is positioned on that side and appears accessible without backtracking. Once past the pillar, I need to move closer to the vehicle’s front right wheel assembly, which is visible in the image (front-right corner of the vehicle, near the seabed). The path appears clear of major obstacles; minor seabed rocks are present but navigable at low altitude. Therefore, the plan is: (1) turn right to begin circumventing the pillar, (2) proceed forward while maintaining clearance from the pillar, (3) align with the vehicle’s front right side, and (4) move in for close inspection of the wheel assembly.",
      "action": [
        "Turn right",
        "Go straight to pass the right side of the stone pillar",
        "Approach the military vehicle from its front-right side",
        "Move closer to inspect the front right wheel assembly"
      ]
    },
    "images": [
      "./data/images/temp_307_0106_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Inspect the large submerged pipeline for visible damage or leaks, focusing on the segmented joints and surface corrosion.",
    "output": {
      "reasoning": "The image shows a large submerged pipeline lying horizontally on the seabed, with clearly visible segmented joints (reinforced with yellow bands) and surface texture suggesting possible corrosion. The robot is positioned at one end, looking along the pipeline’s length. To inspect for damage or leaks—especially at joints and corroded surfaces—the robot must move parallel to the pipeline, maintaining close proximity for high-resolution visual inspection. Given the pipeline extends into the distance, a systematic forward motion with slight lateral adjustments will allow full coverage of joints and surface anomalies. No immediate obstacles block the path, so the primary action is to advance steadily while keeping the pipeline centered in view.",
      "action": [
        "Go straight along the pipeline",
        "Maintain close distance to pipeline surface",
        "Pause briefly at each segmented joint for detailed inspection",
        "Scan for discoloration, pitting, or bubbles indicating leaks"
      ]
    },
    "images": [
      "./data/images/temp_307_0260_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the glowing green column and inspect its surface for biological growth or structural damage.",
    "output": {
      "reasoning": "The glowing green column is the central target, positioned slightly left of center in the image and illuminated from above. It stands vertically on the seabed, with visible texture suggesting possible biological growth or damage. The robot is currently at a moderate distance, facing roughly toward the column but not directly aligned. To approach and inspect its surface, I need to: (1) move forward to reduce distance, (2) adjust heading slightly right to center the column in the field of view (since it appears left-of-center), (3) continue approaching until close enough for detailed inspection (within ~1–2 m), and (4) possibly circle or tilt to examine surface features. Given the monocular view and no immediate obstacles, a direct approach with minor lateral correction is safe and efficient.",
      "action": [
        "Go straight",
        "Turn slightly right",
        "Go straight",
        "Go straight",
        "Hover and inspect surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0265_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green ridged shell-like structure on the seabed while avoiding the smaller pink coral to its left.",
    "output": {
      "reasoning": "The image shows a large green ridged shell-like structure on the seabed, positioned centrally-right in the frame. To its left (from the robot's perspective), there is a smaller pink coral with a distinct brain-like texture. The robot is currently at some distance and slightly elevated, facing the scene. To approach the green structure while avoiding the pink coral, the robot should first steer right to clear the coral’s position, then proceed straight toward the green structure. Since the coral is directly to the left of the target, a slight rightward correction before advancing ensures safe clearance.",
      "action": [
        "Turn right slightly",
        "Go straight toward the green ridged shell"
      ]
    },
    "images": [
      "./data/images/temp_307_0214_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green submarine and inspect its front hull for damage or marine growth.",
    "output": {
      "reasoning": "The green submarine is clearly visible in the center-left of the image, resting on the seabed at a moderate distance. Its front hull (nose section with portholes and grilles) faces roughly toward the camera, slightly angled to the right. To inspect the front hull closely, I need to: (1) approach straight ahead while maintaining alignment with the front; (2) possibly adjust laterally if needed to center the hull in view; (3) stop at a close but safe inspection distance (~1–2 m) to allow visual assessment of damage or marine growth. No immediate obstacles block the direct path. Given the current perspective, a slight rightward correction may help align with the centerline of the front hull before closing in.",
      "action": [
        "Go straight toward the green submarine",
        "Slightly turn right to align with the front hull centerline",
        "Continue forward slowly until ~1.5 meters from the front hull",
        "Hover and stabilize for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0304_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central ornate column and inspect its lower perforated section for marine growth or damage.",
    "output": {
      "reasoning": "The image shows an underwater scene with multiple ornate columns; the central column is clearly visible and features a lower perforated section (with circular holes) that is the inspection target. The robot appears to be positioned slightly to the left and at a moderate distance from the central column. To approach it, the robot should first move forward toward the column while maintaining alignment with its center. Once near, it should position itself close to the lower perforated section—ideally at eye level with that region—for detailed visual inspection of marine growth or damage. No immediate obstacles block the direct path, though fish are present but non-obstructive.",
      "action": [
        "Move forward toward the central ornate column",
        "Adjust heading slightly right to center on the column",
        "Continue forward until within 1 meter of the column",
        "Descend gently to align camera with the lower perforated section",
        "Hover and inspect the perforated area for marine growth or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0236_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged green submarine wreckage from the front, maintaining a safe distance to inspect its open hatch and surrounding debris field.",
    "output": {
      "reasoning": "The green submarine wreckage is directly in front of the robot, oriented horizontally with its open hatch facing forward. The robot appears to be at a moderate distance (~3–4 meters) and slightly below the hull. To approach from the front while maintaining a safe distance (e.g., ~1.5–2 m), I should first align directly with the centerline of the wreck’s bow. The debris field lies on the seabed in front and to the left; I must avoid drifting downward or leftward into it. A slow forward advance with minor pitch-up adjustment will keep the robot at neutral buoyancy and centered on the hatch. No immediate obstacles block the frontal approach path.",
      "action": [
        "Align heading directly toward the submarine's bow (center of open hatch)",
        "Gently ascend 0.3 meters to match mid-hull height",
        "Advance slowly forward for 2 meters",
        "Pause and stabilize at ~1.8 meters from the hatch"
      ]
    },
    "images": [
      "./data/images/temp_307_0305_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged stone pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The image shows a tall, submerged stone pillar centered in the frame, with visible marine growth (greenish algae/biofouling) and possible erosion or damage on its surface. The robot is currently at a moderate distance, facing the pillar directly. To inspect the surface closely, the robot must first approach it while maintaining a stable heading, then circle or hover near the pillar to examine different sections (front, sides, top). Given the clear visibility and lack of immediate obstacles, the safest initial action is to move straight toward the base of the pillar, then adjust position for detailed inspection.",
      "action": [
        "Go straight toward the stone pillar",
        "Slow down as you near the base",
        "Circle clockwise around the pillar at close range",
        "Pause briefly at each quadrant to inspect surface details",
        "Tilt camera upward to examine upper sections and cap"
      ]
    },
    "images": [
      "./data/images/temp_307_0195_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and green dome-shaped structure in the foreground while avoiding the overhanging moss-covered ledge above.",
    "output": {
      "reasoning": "The red and green dome-shaped structure is clearly in the foreground, occupying the lower-left to center of the frame. An overhanging moss-covered ledge looms above it (upper-right), posing a collision risk if the robot ascends or moves upward carelessly. To safely approach the dome, the robot should first maintain a low altitude to stay clear of the ledge, then move forward toward the dome’s base. Since the dome is slightly left-of-center and close, a slight rightward correction may help align with its center while avoiding the ledge’s shadowed underside. No immediate obstacles block the direct forward path at current depth.",
      "action": [
        "Go straight slowly",
        "Slightly turn right",
        "Maintain low altitude to avoid overhead ledge",
        "Continue approaching the red-green dome"
      ]
    },
    "images": [
      "./data/images/temp_307_0114_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large vertical pillar covered in marine growth and inspect its surface for attached organisms or structural features.",
    "output": {
      "reasoning": "The image shows a large vertical pillar centrally located, covered in marine growth (e.g., barnacles, algae, possibly sponges), with visible structural irregularities and attached organisms. The robot appears to be at a moderate distance, slightly below and in front of the pillar. To inspect its surface closely, the robot must first approach it directly while maintaining a safe distance to avoid collision. Given the pillar’s vertical orientation and the presence of nearby coral structures on the left and right, a straight-forward approach is safest. Once near, a slow lateral movement (e.g., slight left or right) may be needed for full surface coverage—but the initial macro-instruction only requires *approaching* and *inspecting*, so the first step is to close the distance. Since the pillar dominates the center of the frame, no major reorientation is needed—just forward motion toward it.",
      "action": [
        "Go straight toward the central pillar",
        "Slow down as you near the pillar (within ~1–2 m)",
        "Maintain stable depth and position to begin visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0029_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red buoy on the seabed and inspect its top valve for signs of leakage or damage.",
    "output": {
      "reasoning": "The red buoy is clearly visible on the seabed in the foreground, slightly to the right of center. It is upright and stable, with its top valve (a cylindrical red protrusion) facing upward and accessible. The robot appears to be positioned a short distance away (~1–2 m), with clear water and minimal obstacles between it and the buoy. To inspect the top valve, the robot must first approach directly toward the buoy, then position itself vertically above or slightly in front of the valve for close visual inspection. No red obstacles are present in the immediate path—only rocks to the left and right, which can be avoided by maintaining a central trajectory.",
      "action": [
        "Go straight toward the red buoy",
        "Slow down as you near the buoy",
        "Hover 0.5 meters in front of the buoy's top valve",
        "Tilt camera downward slightly to focus on the valve",
        "Inspect the valve for cracks, corrosion, or fluid traces"
      ]
    },
    "images": [
      "./data/images/temp_307_0203_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green giant clam in the center while maintaining a safe distance to avoid disturbing it, and observe its mantle for signs of activity.",
    "output": {
      "reasoning": "The large green giant clam is clearly centered in the image, resting on the seabed with its mantle partially exposed. It occupies the mid-to-near field, and there are no immediate obstacles between the robot and the clam. To approach safely without disturbing it, I should first move forward slowly toward it while maintaining a lateral offset (e.g., slightly to the left or right) to avoid direct frontal approach, which may trigger a defensive closure. Given its size and position, a gentle forward motion followed by a slight lateral adjustment will allow observation of the mantle from a safe distance (~1–1.5 m). The fish nearby suggest the environment is active but not threatening; thus, slow, steady movement is appropriate. No turning is needed initially since the clam is directly ahead.",
      "action": [
        "Go straight slowly",
        "Slightly adjust left to maintain safe lateral distance",
        "Pause and observe the mantle for movement"
      ]
    },
    "images": [
      "./data/images/temp_307_0198_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Inspect the green segmented pipeline for structural integrity along its visible length.",
    "output": {
      "reasoning": "The green segmented pipeline is prominently visible, extending diagonally from the lower-left toward the upper-right of the frame. It lies on a flat seabed surface, with clear visibility along its length under directional lighting. To inspect its structural integrity along the visible length, the robot must approach parallel to the pipeline and maintain a consistent distance for high-resolution visual assessment. Given its orientation, the optimal path is to move forward while slightly adjusting left or right to stay aligned with the pipeline’s axis. Since the pipeline appears straight and unobstructed in the visible segment, a continuous forward motion with minor lateral correction will suffice for full inspection of the visible portion.",
      "action": [
        "Move forward slowly while keeping the green pipeline centered in the field of view",
        "Slightly adjust left to maintain parallel alignment with the pipeline",
        "Continue forward until the end of the visible pipeline segment is reached"
      ]
    },
    "images": [
      "./data/images/temp_307_0259_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the red and green segmented pipe lying on the seabed, focusing on the glowing yellow end cap for inspection.",
    "output": {
      "reasoning": "The image shows a segmented pipe on the seabed, colored red and green, with a glowing yellow end cap on the left side (near the robot's current position). The robot is positioned slightly above and to the right of the pipe, looking down its length. The glowing yellow end cap is clearly visible and is the inspection target per the instruction. To approach it safely and precisely, the robot should first move forward toward the pipe while maintaining orientation, then adjust laterally if needed to center on the yellow cap. Since the cap is already in view and relatively close, the primary action is to advance straight toward it, possibly with a slight leftward correction to align directly.",
      "action": [
        "Go straight toward the pipe",
        "Slightly turn left to center on the glowing yellow end cap",
        "Approach closely for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0212_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the green submerged vehicle and inspect its right-side windows for damage or marine growth.",
    "output": {
      "reasoning": "The green submerged vehicle is clearly visible in the center-left of the image, oriented diagonally with its front pointing toward the lower-right. Its right side (from the vehicle’s perspective) faces the camera’s current position, but the robot appears to be slightly ahead and to the left of the vehicle’s midsection. To inspect the right-side windows, the robot must first approach closer along a path that maintains visibility of that side—ideally moving parallel to the vehicle’s longitudinal axis from front to rear or vice versa. Given the current angle, the optimal first step is to move forward slightly to align better with the mid-section, then shift right to bring the right-side windows into clear view without occlusion. The orange vertical structures above suggest mooring or reference points; they should be avoided but are not immediate obstacles. No visible damage or marine growth can be assessed yet—close inspection requires proximity (~1–2 m).",
      "action": [
        "Go straight slowly for 2 meters",
        "Drift right 1 meter while maintaining depth",
        "Hover and stabilize facing the right-side windows",
        "Perform slow forward scan along the right-side window row"
      ]
    },
    "images": [
      "./data/images/temp_307_0009_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the yellow and blue submersible vehicle hovering near the orange sea fans and inspect its underside for debris or damage.",
    "output": {
      "reasoning": "The yellow and blue submersible is visible in the mid-left of the image, hovering above the seabed near prominent orange sea fans. It is oriented diagonally, with its underside partially visible but not fully accessible from the current viewpoint. To inspect its underside for debris or damage, I need to: (1) approach it carefully to avoid disturbing sediment or colliding with sea fans; (2) position the robot directly beneath or slightly below and behind the vehicle to get a clear view of the underside; (3) maintain stable hover for inspection. Given its location relative to the sea fans (to their left and slightly forward), the safest approach path is from the front-right quadrant—moving straight toward it while slightly adjusting left to clear the nearest sea fan cluster, then descending gently once close.",
      "action": [
        "Go straight toward the yellow and blue submersible",
        "Slightly turn left to avoid the nearest orange sea fan cluster",
        "Continue approaching until ~1 meter away",
        "Gently descend to position below the submersible",
        "Hover and inspect the underside for debris or damage"
      ]
    },
    "images": [
      "./data/images/temp_307_0229_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large circular coral structure in the center-left and inspect its top surface for signs of marine growth or damage.",
    "output": {
      "reasoning": "The image shows a large circular coral structure located in the center-left of the scene, elevated above the seabed and clearly visible. It has a distinct flat top surface with radial patterns, ideal for inspection. The robot is currently positioned at a moderate distance and slightly below the structure (as light rays come from above and the structure looms overhead). To approach and inspect the top surface, the robot should: (1) move forward toward the structure while maintaining a safe vertical clearance; (2) ascend slightly to align with the top plane; (3) position directly above or just in front of the top surface for optimal visual inspection; (4) hover/stabilize to allow detailed observation of marine growth or damage. No immediate obstacles block the path, though smaller corals are nearby—so lateral precision is needed.",
      "action": [
        "Move forward toward the large circular coral in the center-left",
        "Ascend slightly to align with the top surface height",
        "Position directly above the coral’s top surface",
        "Hover and stabilize for visual inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0131_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the rusted orange cylindrical object in the foreground and inspect its surface for marine growth or damage.",
    "output": {
      "reasoning": "The rusted orange cylindrical object is clearly visible in the foreground on the right side of the image. It is upright and positioned closer to the camera than the larger greenish structure behind it. To approach and inspect its surface, the robot should first move forward toward it while maintaining a safe distance, then circle or reposition to get a close-up view of its surface (especially around the holes and bands) for signs of marine growth (e.g., barnacles, algae) or corrosion/damage. Given its proximity and orientation, a direct forward movement followed by a slight leftward arc will allow optimal inspection without collision.",
      "action": [
        "Go straight toward the orange cylindrical object",
        "Move slightly left to align with its midsection",
        "Hover closely and rotate slowly to inspect surface details"
      ]
    },
    "images": [
      "./data/images/temp_307_0220_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged submarine and inspect its hull near the visible chain and rope attachments.",
    "output": {
      "reasoning": "The submarine is positioned horizontally in the mid-to-upper portion of the image, slightly angled with its bow pointing left. A prominent chain and rope assembly is visible near the submarine’s midsection, hanging vertically from above and draping toward the foreground. The robot appears to be at a distance and slightly below/forward of the submarine. To inspect the hull near the chain and rope attachments, I need to: (1) move forward toward the submarine while maintaining depth to avoid collision; (2) adjust lateral position to align with the chain/rope area (likely near the center of the hull); (3) close in carefully for detailed inspection without disturbing the ropes or sediment. Given the lighting from above, approaching from below may improve visibility of the hull surface. The foreground ropes are not obstructive but should be avoided during approach.",
      "action": [
        "Go straight slowly toward the submarine",
        "Slightly ascend to match the submarine's depth",
        "Drift right to align with the chain and rope attachment point",
        "Move forward gently to inspect the hull near the chains"
      ]
    },
    "images": [
      "./data/images/temp_307_0035_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the cylindrical underwater vehicle positioned near the coral formation and inspect its side-mounted sensor array.",
    "output": {
      "reasoning": "The cylindrical underwater vehicle is visible to the right of the coral formation, oriented horizontally with its side-mounted sensor array facing roughly toward the camera. The robot appears to be positioned slightly left and forward of the vehicle, with clear water between them. To approach and inspect the side-mounted sensor array, the robot should first move laterally right to align with the vehicle’s midsection, then advance straight while maintaining a safe distance to avoid disturbing the coral or the vehicle. A slight downward adjustment may be needed to match the vehicle’s depth, as it appears slightly below the current viewpoint. Finally, a minor yaw adjustment (e.g., slight left turn) will orient the camera directly toward the side sensor array for inspection.",
      "action": [
        "Move right",
        "Go straight",
        "Descend slightly",
        "Turn left slightly"
      ]
    },
    "images": [
      "./data/images/temp_307_0057_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large green cylindrical pipe on the left while avoiding the rising bubbles and staying centered in the tunnel.",
    "output": {
      "reasoning": "The large green cylindrical pipe is clearly visible on the left side of the tunnel. The robot is currently centered in the tunnel, with rising bubbles concentrated near the top-left and above the pipe—likely from equipment or vents near the pipe. To approach the pipe while avoiding bubbles and staying centered, I should: (1) maintain centerline alignment initially to avoid drifting into bubble zones; (2) gradually steer left *after* passing the densest bubble cluster (which appears just ahead and above the pipe); (3) keep forward motion steady to minimize turbulence-induced drift. The pipe’s position suggests a gentle left turn is needed once clear of bubbles, but not too early to avoid collision or entering bubble-rich water.",
      "action": [
        "Go straight while maintaining centerline",
        "Slightly turn left after passing the main bubble cluster",
        "Continue forward toward the green pipe"
      ]
    },
    "images": [
      "./data/images/temp_307_0119_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged blue car and inspect its front grille and headlights for damage or marine growth.",
    "output": {
      "reasoning": "The blue car is visible in the center-right of the image, partially buried in sediment, with its front grille and headlights facing the camera. The robot appears to be positioned slightly left and forward of the car, at a moderate distance (~2–3 m). To inspect the front grille and headlights closely, the robot must first approach straight toward the car’s front, then possibly adjust laterally to center on the grille area. No immediate obstacles block the direct path, though marine growth (e.g., fan-like organisms) is present nearby—caution needed to avoid contact. Since the goal is inspection, final positioning should allow clear, stable view of the front fascia (grille + headlights), ideally within 1 m.",
      "action": [
        "Go straight toward the blue car",
        "Slow down as you near the front bumper",
        "Adjust slightly right to center on the grille",
        "Hover steadily 0.8 meters in front of the grille for inspection"
      ]
    },
    "images": [
      "./data/images/temp_307_0103_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the central red-and-teal subsea tooling assembly and inspect the orange square housing for visible damage or debris.",
    "output": {
      "reasoning": "The image shows a clear frontal view of the central red-and-teal subsea tooling assembly, with the orange square housing prominently visible at the lower front. The robot is already aligned with the assembly and appears to be at moderate distance—close enough to identify major features but not yet at inspection range. To inspect the orange square housing for damage or debris, the robot must approach closer while maintaining stable orientation. No immediate obstacles block the direct path forward. Therefore, the next logical step is to advance straight toward the assembly until the orange housing fills more of the frame (e.g., ~1–2 m away), enabling high-resolution visual inspection.",
      "action": [
        "Go straight toward the central assembly"
      ]
    },
    "images": [
      "./data/images/temp_307_0208_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the purple sponge-like structure on the seabed while avoiding the coral formation to its right.",
    "output": {
      "reasoning": "The purple sponge-like structure is visible on the seabed in the lower-left portion of the image. A coral formation is present to its right (image right), which must be avoided. The robot is currently positioned above and slightly behind the sponge, facing roughly toward it. To approach safely while avoiding the coral, the robot should first move forward slightly to align with the sponge’s left side, then gently steer left to maintain clearance from the coral, and finally descend or move closer to reach the sponge without contact.",
      "action": [
        "Go straight",
        "Turn left slightly",
        "Go straight toward the purple sponge"
      ]
    },
    "images": [
      "./data/images/temp_307_0093_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged concrete pillar and inspect its surface for marine growth or structural damage.",
    "output": {
      "reasoning": "The image shows a tall, submerged concrete pillar centered in the frame, illuminated from above with visible marine growth and surface irregularities. The robot appears to be positioned at a moderate distance directly in front of the pillar, with clear visibility and no immediate obstacles. To inspect the surface thoroughly, the robot should first close the distance to get within optimal inspection range (e.g., 1–2 meters), then perform a slow circumferential movement around the pillar to examine all sides, while maintaining stable altitude and orientation for high-resolution imaging.",
      "action": [
        "Go straight toward the pillar",
        "Slow down and stop at ~1.5 m distance",
        "Circle the pillar clockwise at constant depth",
        "Maintain steady forward motion while tilting camera slightly downward to scan surface details",
        "Pause briefly at each quadrant to capture high-res imagery"
      ]
    },
    "images": [
      "./data/images/temp_307_0239_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the large submerged pipeline and inspect its surface for corrosion or damage along the visible section.",
    "output": {
      "reasoning": "The image shows a large submerged pipeline extending diagonally from the lower-left toward the upper-right, with visible structural supports and surface texture. The robot appears to be positioned slightly in front and to the left of the pipeline’s near end (marked by a blue-and-yellow cap). To approach and inspect the visible section, the robot should first move forward to get closer to the pipeline, then align parallel to it (likely by turning right), and finally proceed alongside it while maintaining a consistent standoff distance for visual inspection. Given the pipeline’s orientation and the robot’s current position, the initial action is to advance straight toward the pipeline’s near end, then adjust heading to run parallel along its length.",
      "action": [
        "Go straight toward the pipeline",
        "Turn right to align parallel with the pipeline",
        "Move forward slowly alongside the pipeline while inspecting its surface"
      ]
    },
    "images": [
      "./data/images/temp_307_0139_1.png"
    ]
  },
  {
    "instruction": "# Role\nYou are an advanced underwater robot vision-language-action (VLA) task planning model. Your core capability is to decompose complex macroscopic tasks into logically rigorous, executable natural language atomic step sequences based on monocular RGB images (visual input) and user natural language instructions.\n\n# Context & Constraints\n1. **Environment**: Underwater environment with potential uneven lighting, turbidity, or dynamic obstacles.\n2. **Input**:\n    - Monocular RGB image from current perspective (analyze object positions, orientations, and obstacles).\n    - User macroscopic instruction (e.g., \"Avoid red obstacles and turn right\" or \"Approach the side rock\").\n3. **Output Requirements**:\n    - Must conform to provided JSON Schema format.\n    - Action field must be a list of strings.\n    - Step descriptions should be concise, natural, and action-oriented (e.g., \"Turn right\", \"Go straight\").\n\n# Workflow\nBefore generating output, perform these internal reasoning steps:\n1. Visual Perception: Identify key targets and obstacles in the image.\n2. Spatial Reasoning: Determine relative positions (left? front? distance?) between robot and target.\n3. Plan Decomposition: Break down path into sequential actions based on spatial relationships.\n\n# Task\nGenerate corresponding reasoning process and action sequence based on provided image and instruction.",
    "input": "Approach the submerged turquoise car and inspect its front bumper for damage or marine growth.",
    "output": {
      "reasoning": "The turquoise car is visible in the center-right of the image, resting on the seabed among coral and rocks. Its front bumper faces roughly toward the camera but is slightly angled to the left. The robot appears to be positioned behind and slightly above the car (based on perspective and lighting). To approach the front bumper for inspection, the robot should first move forward toward the car while maintaining a safe distance from surrounding coral. As it gets closer, it should adjust laterally (slightly right) to align directly with the front bumper, then descend gently to get a close-up view. No immediate obstacles block the direct path, though care must be taken near the coral on the left.",
      "action": [
        "Go straight toward the turquoise car",
        "Slightly turn right to align with the front bumper",
        "Descend gently to get closer to the front bumper",
        "Hover and inspect the front bumper for damage or marine growth"
      ]
    },
    "images": [
      "./data/images/temp_307_0224_1.png"
    ]
  }
]